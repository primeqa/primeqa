{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQuAD 1.1\n",
    "\n",
    "In this notebook, we will see how to fine-tune and evaluate a model on the SQuAD 1.1 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies\n",
    "\n",
    "If not already done, make sure to install PrimeQA with `notebooks` extras before getting started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# If you want CUDA 11 uncomment and run this (for CUDA 10 or CPU you can ignore this line).\n",
    "#! pip install 'torch~=1.11.0' --extra-index-url https://download.pytorch.org/whl/cu113\n",
    "\n",
    "# Uncomment to install OneQA from source (pypi package pending).\n",
    "# The path should be the project root (e.g. '.' below).\n",
    "#! pip install .[notebooks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration\n",
    "\n",
    "We start by setting some parameters to configure the process.  Note that depending on the GPU being used you may need to tune the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This needs to be filled in.\n",
    "output_dir = 'FILL_ME_IN'        # Save the results here.  Will overwrite if directory already exists.\n",
    "\n",
    "# Optional parameters (feel free to leave as default).\n",
    "model_name = 'roberta-base'  # Set this to select the LM.  Since this is a multi-lingual dataset, we use the XLM-Roberta model.\n",
    "cache_dir = None                 # Set this if you have a cache directory for transformers.  Alternatively set the HF_HOME env var.\n",
    "train_batch_size = 8             # Set this to change the number of features per batch during training.\n",
    "eval_batch_size = 8              # Set this to change the number of features per batch during evaluation.\n",
    "gradient_accumulation_steps = 8  # Set this to effectively increase training batch size.\n",
    "max_train_samples = 100          # Set this to use a subset of the training data (or None for all).\n",
    "max_eval_samples = 20            # Set this to use a subset of the evaluation data (or None for all).\n",
    "num_train_epochs = 1             # Set this to change the number of training epochs.\n",
    "fp16 = False                     # Set this to true to enable fp16 (hardware support required).\n",
    "num_examples_to_show = 10        # Set this to change the number of random train examples (and their features) to show."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from transformers.trainer_utils import set_seed\n",
    "\n",
    "seed = 42\n",
    "set_seed(seed)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=train_batch_size,\n",
    "    per_device_eval_batch_size=eval_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    evaluation_strategy='no',\n",
    "    learning_rate=4e-05,\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.1,\n",
    "    save_steps=50000,\n",
    "    fp16=fp16,\n",
    "    seed=seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Model\n",
    "\n",
    "Here we load the model and tokenizer based on the model_name parameter set above.  We use a model with an extractive QA task head which we will later fine-tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModelForDownstreamTasks were not initialized from the model checkpoint at roberta-base and are newly initialized: ['task_heads.qa_head.classifier.out_proj.weight', 'task_heads.qa_head.classifier.out_proj.bias', 'task_heads.qa_head.qa_outputs.weight', 'task_heads.qa_head.classifier.dense.weight', 'task_heads.qa_head.qa_outputs.bias', 'task_heads.qa_head.classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaModelForDownstreamTasks(\n",
      "  (roberta): RobertaModel(\n",
      "    (embeddings): RobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
      "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): RobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): RobertaPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (lm_head): RobertaLMHead(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (decoder): Linear(in_features=768, out_features=50265, bias=True)\n",
      "  )\n",
      "  (task_heads): ModuleDict(\n",
      "    (qa_head): ExtractiveQAHead(\n",
      "      (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
      "      (classifier): RobertaClassificationHead(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (out_proj): Linear(in_features=768, out_features=5, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig, AutoTokenizer\n",
    "from primeqa.mrc.models.heads.extractive import EXTRACTIVE_HEAD\n",
    "from primeqa.mrc.models.task_model import ModelForDownstreamTasks\n",
    "\n",
    "from primeqa.mrc.trainers.mrc import MRCTrainer\n",
    "\n",
    "task_heads = EXTRACTIVE_HEAD\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_name,\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    cache_dir=cache_dir,\n",
    "    use_fast=True,\n",
    "    config=config,\n",
    ")\n",
    "model = ModelForDownstreamTasks.from_config(\n",
    "    config,\n",
    "    model_name,\n",
    "    task_heads=task_heads,\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "model.set_task_head(next(iter(task_heads)))\n",
    "\n",
    "print(model)  # Examine the model structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data\n",
    "\n",
    "Here we load the SQuAD 1.1 dataset using Huggingface's datasets library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset squad (/u/mabornea/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8a64288e98546e4981a945ade67c6ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 100 train examples.\n",
      "Using 20 eval examples.\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "import random\n",
    "\n",
    "raw_datasets = datasets.load_dataset(\n",
    "    'squad',\n",
    "    'plain_text',\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "\n",
    "train_examples = raw_datasets[\"train\"]\n",
    "max_train_samples = max_train_samples\n",
    "if max_train_samples is not None:\n",
    "    # We will select sample from whole data if argument is specified\n",
    "    train_examples = train_examples.select(range(max_train_samples))\n",
    "\n",
    "print(f\"Using {train_examples.num_rows} train examples.\")\n",
    "\n",
    "eval_examples = raw_datasets[\"validation\"]\n",
    "max_eval_samples = max_eval_samples\n",
    "if max_eval_samples is not None:\n",
    "    # We will select sample from whole data if argument is specified\n",
    "    random_idxs = random.sample(range(len(eval_examples)), max_eval_samples)\n",
    "    eval_examples = eval_examples.select(random_idxs)\n",
    "\n",
    "print(f\"Using {eval_examples.num_rows} eval examples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "Here we preprocess the data to create features which can be given to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function SQUADPreprocessor._augment_examples at 0x7fa1be078c20> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "618a132db7374f628ddcc66ebc48a6fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13380b459fd246ba825b601ed0dfa21f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on train dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing produced 100 train features from 100 examples.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4f303f09cf94de6bf0b76819ac642a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80c3e69387f74a978305be5e18039ac5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on eval dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing produced 20 eval features from 20 examples.\n"
     ]
    }
   ],
   "source": [
    "from primeqa.mrc.processors.preprocessors.squad import SQUADPreprocessor\n",
    "\n",
    "preprocessor = SQUADPreprocessor(\n",
    "    stride=128,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Train Feature Creation\n",
    "with training_args.main_process_first(desc=\"train dataset map pre-processing\"):\n",
    "    train_examples, train_dataset = preprocessor.process_train(train_examples)\n",
    "\n",
    "print(f\"Preprocessing produced {train_dataset.num_rows} train features from {train_examples.num_rows} examples.\")\n",
    "\n",
    "# Validation Feature Creation\n",
    "with training_args.main_process_first(desc=\"validation dataset map pre-processing\"):\n",
    "    eval_examples, eval_dataset = preprocessor.process_eval(eval_examples)\n",
    "\n",
    "print(f\"Preprocessing produced {eval_dataset.num_rows} eval features from {eval_examples.num_rows} examples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import ClassLabel, Sequence\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Based on https://github.com/huggingface/notebooks/blob/main/examples/question_answering.ipynb\n",
    "def show_elements(dataset):\n",
    "    df = pd.DataFrame(dataset)\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "        elif isinstance(typ, Sequence) and isinstance(typ.feature, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda x: [typ.feature.names[i] for i in x])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8627eb1981764ccfbcb9c2ec5fec889f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>example_id</th>\n",
       "      <th>target</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>In 1882, Albert Zahm (John Zahm's brother) built an early wind tunnel used to compare lift to drag of aeronautical models. Around 1899, Professor Jerome Green became the first American to send a wireless message. In 1931, Father Julius Nieuwland performed early work on basic reactions that was used to create neoprene. Study of nuclear physics at the university began with the building of a nuclear accelerator in 1936, and continues now partly through a partnership in the Joint Institute for Nu...</td>\n",
       "      <td>Which individual worked on projects at Notre Dame that eventually created neoprene?</td>\n",
       "      <td>5733b1da4776f4190066106b</td>\n",
       "      <td>{'end_positions': [245], 'passage_indices': [0], 'start_positions': [222], 'yes_no_answer': ['NONE']}</td>\n",
       "      <td>UNKNOWN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>In 2014 the Notre Dame student body consisted of 12,179 students, with 8,448 undergraduates, 2,138 graduate and professional and 1,593 professional (Law, M.Div., Business, M.Ed.) students. Around 21–24% of students are children of alumni, and although 37% of students come from the Midwestern United States, the student body represents all 50 states and 100 countries. As of March 2007[update] The Princeton Review ranked the school as the fifth highest 'dream school' for parents to send their ch...</td>\n",
       "      <td>How many teams participate in the Notre Dame Bookstore Basketball tournament?</td>\n",
       "      <td>5733b5df4776f41900661107</td>\n",
       "      <td>{'end_positions': [1454], 'passage_indices': [0], 'start_positions': [1446], 'yes_no_answer': ['NONE']}</td>\n",
       "      <td>UNKNOWN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>The library system of the university is divided between the main library and each of the colleges and schools. The main building is the 14-story Theodore M. Hesburgh Library, completed in 1963, which is the third building to house the main collection of books. The front of the library is adorned with the Word of Life mural designed by artist Millard Sheets. This mural is popularly known as \"Touchdown Jesus\" because of its proximity to Notre Dame Stadium and Jesus' arms appearing to make the s...</td>\n",
       "      <td>What is the name of the main library at Notre Dame?</td>\n",
       "      <td>5733ad384776f41900660fed</td>\n",
       "      <td>{'end_positions': [173], 'passage_indices': [0], 'start_positions': [145], 'yes_no_answer': ['NONE']}</td>\n",
       "      <td>UNKNOWN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary repu...</td>\n",
       "      <td>To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?</td>\n",
       "      <td>5733be284776f41900661182</td>\n",
       "      <td>{'end_positions': [541], 'passage_indices': [0], 'start_positions': [515], 'yes_no_answer': ['NONE']}</td>\n",
       "      <td>UNKNOWN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>In 1919 Father James Burns became president of Notre Dame, and in three years he produced an academic revolution that brought the school up to national standards by adopting the elective system and moving away from the university's traditional scholastic and classical emphasis. By contrast, the Jesuit colleges, bastions of academic conservatism, were reluctant to move to a system of electives. Their graduates were shut out of Harvard Law School for that reason. Notre Dame continued to grow ov...</td>\n",
       "      <td>Those who attended a Jesuit college may have been forbidden from joining which Law School due to the curricula at the Jesuit institution?</td>\n",
       "      <td>57338724d058e614000b5ca0</td>\n",
       "      <td>{'end_positions': [448], 'passage_indices': [0], 'start_positions': [430], 'yes_no_answer': ['NONE']}</td>\n",
       "      <td>UNKNOWN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>All of Notre Dame's undergraduate students are a part of one of the five undergraduate colleges at the school or are in the First Year of Studies program. The First Year of Studies program was established in 1962 to guide incoming freshmen in their first year at the school before they have declared a major. Each student is given an academic advisor from the program who helps them to choose classes that give them exposure to any major in which they are interested. The program also includes a L...</td>\n",
       "      <td>What entity provides help with the management of time for new students at Notre Dame?</td>\n",
       "      <td>5733a70c4776f41900660f64</td>\n",
       "      <td>{'end_positions': [520], 'passage_indices': [0], 'start_positions': [496], 'yes_no_answer': ['NONE']}</td>\n",
       "      <td>UNKNOWN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>This Main Building, and the library collection, was entirely destroyed by a fire in April 1879, and the school closed immediately and students were sent home. The university founder, Fr. Sorin and the president at the time, the Rev. William Corby, immediately planned for the rebuilding of the structure that had housed virtually the entire University. Construction was started on the 17th of May and by the incredible zeal of administrator and workers the building was completed before the fall s...</td>\n",
       "      <td>In what year was the Main Building at Notre Dame razed in a fire?</td>\n",
       "      <td>57338653d058e614000b5c81</td>\n",
       "      <td>{'end_positions': [94], 'passage_indices': [0], 'start_positions': [90], 'yes_no_answer': ['NONE']}</td>\n",
       "      <td>UNKNOWN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>In 1882, Albert Zahm (John Zahm's brother) built an early wind tunnel used to compare lift to drag of aeronautical models. Around 1899, Professor Jerome Green became the first American to send a wireless message. In 1931, Father Julius Nieuwland performed early work on basic reactions that was used to create neoprene. Study of nuclear physics at the university began with the building of a nuclear accelerator in 1936, and continues now partly through a partnership in the Joint Institute for Nu...</td>\n",
       "      <td>In what year did Albert Zahm begin comparing aeronatical models at Notre Dame?</td>\n",
       "      <td>5733b1da4776f41900661068</td>\n",
       "      <td>{'end_positions': [7], 'passage_indices': [0], 'start_positions': [3], 'yes_no_answer': ['NONE']}</td>\n",
       "      <td>UNKNOWN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>Notre Dame is known for its competitive admissions, with the incoming class enrolling in fall 2015 admitting 3,577 from a pool of 18,156 (19.7%). The academic profile of the enrolled class continues to rate among the top 10 to 15 in the nation for national research universities. The university practices a non-restrictive early action policy that allows admitted students to consider admission to Notre Dame as well as any other colleges to which they were accepted. 1,400 of the 3,577 (39.1%) we...</td>\n",
       "      <td>How many miles does the average student at Notre Dame travel to study there?</td>\n",
       "      <td>5733ae924776f41900661017</td>\n",
       "      <td>{'end_positions': [637], 'passage_indices': [0], 'start_positions': [618], 'yes_no_answer': ['NONE']}</td>\n",
       "      <td>UNKNOWN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>University_of_Notre_Dame</td>\n",
       "      <td>The College of Engineering was established in 1920, however, early courses in civil and mechanical engineering were a part of the College of Science since the 1870s. Today the college, housed in the Fitzpatrick, Cushing, and Stinson-Remick Halls of Engineering, includes five departments of study – aerospace and mechanical engineering, chemical and biomolecular engineering, civil engineering and geological sciences, computer science and engineering, and electrical engineering – with eight B.S....</td>\n",
       "      <td>The College of Science began to offer civil engineering courses beginning at what time at Notre Dame?</td>\n",
       "      <td>5733a6424776f41900660f52</td>\n",
       "      <td>{'end_positions': [164], 'passage_indices': [0], 'start_positions': [155], 'yes_no_answer': ['NONE']}</td>\n",
       "      <td>UNKNOWN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def trim_document(example, max_len=500):\n",
    "    example['context'] = example['context'][0]\n",
    "    doc_len = len(example['context'])\n",
    "    if doc_len > max_len:\n",
    "        example['context'] = f\"{example['context'][:max_len - 3]}...\"        \n",
    "    return example\n",
    "\n",
    "random_idxs = random.sample(range(len(train_examples)), num_examples_to_show)\n",
    "random_train_examples = train_examples.select(random_idxs).remove_columns(['passage_candidates'])\n",
    "random_train_examples = random_train_examples.map(trim_document)\n",
    "\n",
    "show_elements(random_train_examples)  # Show random train examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59bf8c41b7bd4c768fedba961f37eb20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "765fbf134d9147f0b4ab5b9cca9c016e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>example_id</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>example_idx</th>\n",
       "      <th>start_positions</th>\n",
       "      <th>end_positions</th>\n",
       "      <th>target_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5733be284776f41900661182</td>\n",
       "      <td>[0, 3972, 2661, 222, 5, 9880, 2708, 2346, 2082, 11, 504, 4432, 11, 226, 2126, 10067, 1470, 116, 2, 2, 37848, 37471, 28108, 6, 5, 334, 34, 10, 4019, 2048, 4, 497, 1517, 5, 4326, 6919, 18, 1637, 31346, 16, 10, 9030, 9577, 9, 5, 9880, 2708, 4, 29261, 11, 760, 9, 5, 4326, 6919, 8, 2114, 24, 6, 16, 10, 7621, 9577, 9, 4845, 19, 3701, 62, 33161, 19, 5, 7875, 22, 39043, 1459, 1614, 1464, 13292, 4977, 845, 4130, 7, 5, 4326, 6919, 16, 5, 26429, 2426, 9, 5, 25095, 6924, 4, 29261, 639, 5, 32394, 2426, 16, ...]</td>\n",
       "      <td>0</td>\n",
       "      <td>135</td>\n",
       "      <td>142</td>\n",
       "      <td>SPAN_ANSWER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5733a6424776f41900660f52</td>\n",
       "      <td>[0, 133, 1821, 9, 4662, 880, 7, 904, 2366, 4675, 7484, 1786, 23, 99, 86, 23, 10579, 9038, 116, 2, 2, 133, 1821, 9, 9466, 21, 2885, 11, 18283, 6, 959, 6, 419, 7484, 11, 2366, 8, 12418, 4675, 58, 10, 233, 9, 5, 1821, 9, 4662, 187, 5, 41102, 29, 4, 2477, 5, 1564, 6, 15740, 11, 5, 20842, 6, 230, 11286, 6, 8, 312, 9554, 12, 31157, 1758, 41621, 9, 9466, 6, 1171, 292, 6522, 9, 892, 126, 15064, 8, 12418, 4675, 6, 4747, 8, 43963, 4104, 32188, 4675, 6, 2366, 4675, 8, 30694, 17874, 6, 3034, 2866, ...]</td>\n",
       "      <td>19</td>\n",
       "      <td>48</td>\n",
       "      <td>50</td>\n",
       "      <td>SPAN_ANSWER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5733a70c4776f41900660f64</td>\n",
       "      <td>[0, 2264, 10014, 1639, 244, 19, 5, 1052, 9, 86, 13, 92, 521, 23, 10579, 9038, 116, 2, 2, 3684, 9, 10579, 9038, 18, 19555, 521, 32, 10, 233, 9, 65, 9, 5, 292, 19555, 8975, 23, 5, 334, 50, 32, 11, 5, 1234, 2041, 9, 9307, 586, 4, 20, 1234, 2041, 9, 9307, 586, 21, 2885, 11, 19515, 7, 4704, 11433, 19684, 11, 49, 78, 76, 23, 5, 334, 137, 51, 33, 2998, 10, 538, 4, 4028, 1294, 16, 576, 41, 5286, 11220, 31, 5, 586, 54, 2607, 106, 7, 2807, 4050, 14, 492, 106, 4895, 7, 143, 538, ...]</td>\n",
       "      <td>20</td>\n",
       "      <td>111</td>\n",
       "      <td>113</td>\n",
       "      <td>SPAN_ANSWER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5733ad384776f41900660fed</td>\n",
       "      <td>[0, 2264, 16, 5, 766, 9, 5, 1049, 5560, 23, 10579, 9038, 116, 2, 2, 133, 5560, 467, 9, 5, 2737, 16, 6408, 227, 5, 1049, 5560, 8, 349, 9, 5, 8975, 8, 1304, 4, 20, 1049, 745, 16, 5, 501, 12, 6462, 26164, 256, 4, 32899, 24035, 5672, 6, 2121, 11, 18733, 6, 61, 16, 5, 371, 745, 7, 790, 5, 1049, 2783, 9, 2799, 4, 20, 760, 9, 5, 5560, 16, 29191, 19, 5, 15690, 9, 3126, 21281, 1887, 30, 3025, 5388, 1120, 264, 2580, 4, 152, 21281, 16, 1406, 352, 684, 25, 22, 40121, 3955, 5772, 113, ...]</td>\n",
       "      <td>35</td>\n",
       "      <td>43</td>\n",
       "      <td>48</td>\n",
       "      <td>SPAN_ANSWER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5733ae924776f41900661017</td>\n",
       "      <td>[0, 6179, 171, 1788, 473, 5, 674, 1294, 23, 10579, 9038, 1504, 7, 892, 89, 116, 2, 2, 7199, 241, 9038, 16, 684, 13, 63, 2695, 18054, 6, 19, 5, 11433, 1380, 16914, 154, 11, 1136, 570, 13874, 155, 6, 36447, 31, 10, 3716, 9, 504, 6, 27915, 36, 1646, 4, 406, 23528, 20, 5286, 4392, 9, 5, 12751, 1380, 1388, 7, 731, 566, 5, 299, 158, 7, 379, 11, 5, 1226, 13, 632, 557, 6630, 4, 20, 2737, 3464, 10, 786, 12, 7110, 12127, 2088, 419, 814, 714, 14, 2386, 2641, 521, 7, 1701, 7988, 7, 10579, 9038, 25, ...]</td>\n",
       "      <td>43</td>\n",
       "      <td>147</td>\n",
       "      <td>150</td>\n",
       "      <td>SPAN_ANSWER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5733b1da4776f41900661068</td>\n",
       "      <td>[0, 1121, 99, 76, 222, 8098, 21008, 119, 1642, 12818, 16482, 261, 36105, 3092, 23, 10579, 9038, 116, 2, 2, 1121, 504, 6551, 6, 8098, 21008, 119, 36, 10567, 21008, 119, 18, 2138, 43, 1490, 41, 419, 2508, 10615, 341, 7, 8933, 5258, 7, 8386, 9, 16482, 261, 26832, 3092, 4, 8582, 43130, 6, 6020, 15385, 1628, 1059, 5, 78, 470, 7, 2142, 10, 6955, 1579, 4, 96, 36332, 6, 9510, 20487, 234, 13627, 605, 1245, 3744, 419, 173, 15, 3280, 11012, 14, 21, 341, 7, 1045, 3087, 1517, 20962, 4, 13019, 9, 1748, 17759, 23, 5, 2737, 880, 19, ...]</td>\n",
       "      <td>54</td>\n",
       "      <td>21</td>\n",
       "      <td>22</td>\n",
       "      <td>SPAN_ANSWER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5733b1da4776f4190066106b</td>\n",
       "      <td>[0, 32251, 1736, 1006, 15, 1377, 23, 10579, 9038, 14, 2140, 1412, 3087, 1517, 20962, 116, 2, 2, 1121, 504, 6551, 6, 8098, 21008, 119, 36, 10567, 21008, 119, 18, 2138, 43, 1490, 41, 419, 2508, 10615, 341, 7, 8933, 5258, 7, 8386, 9, 16482, 261, 26832, 3092, 4, 8582, 43130, 6, 6020, 15385, 1628, 1059, 5, 78, 470, 7, 2142, 10, 6955, 1579, 4, 96, 36332, 6, 9510, 20487, 234, 13627, 605, 1245, 3744, 419, 173, 15, 3280, 11012, 14, 21, 341, 7, 1045, 3087, 1517, 20962, 4, 13019, 9, 1748, 17759, 23, 5, 2737, 880, 19, 5, 745, ...]</td>\n",
       "      <td>57</td>\n",
       "      <td>68</td>\n",
       "      <td>73</td>\n",
       "      <td>SPAN_ANSWER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5733b5df4776f41900661107</td>\n",
       "      <td>[0, 6179, 171, 893, 4064, 11, 5, 10579, 9038, 5972, 8005, 12610, 1967, 116, 2, 2, 1121, 777, 5, 10579, 9038, 1294, 809, 22061, 9, 316, 6, 26340, 521, 6, 19, 290, 6, 36246, 37340, 24597, 6, 132, 6, 25352, 5318, 8, 2038, 8, 112, 6, 39785, 2038, 36, 22532, 6, 256, 4, 37165, 482, 2090, 6, 256, 4, 5404, 1592, 521, 4, 8582, 733, 2383, 1978, 207, 9, 521, 32, 408, 9, 16132, 6, 8, 1712, 2908, 207, 9, 521, 283, 31, 5, 4079, 16507, 315, 532, 6, 5, 1294, 809, 3372, 70, 654, 982, 8, 727, 749, 4, ...]</td>\n",
       "      <td>75</td>\n",
       "      <td>306</td>\n",
       "      <td>307</td>\n",
       "      <td>SPAN_ANSWER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>57338653d058e614000b5c81</td>\n",
       "      <td>[0, 1121, 99, 76, 21, 5, 4326, 6919, 23, 10579, 9038, 910, 16314, 11, 10, 668, 116, 2, 2, 713, 4326, 6919, 6, 8, 5, 5560, 2783, 6, 21, 4378, 4957, 30, 10, 668, 11, 587, 504, 5220, 6, 8, 5, 334, 1367, 1320, 8, 521, 58, 1051, 184, 4, 20, 2737, 3787, 6, 4967, 4, 14405, 179, 8, 5, 394, 23, 5, 86, 6, 5, 7161, 4, 2897, 2812, 1409, 6, 1320, 1904, 13, 5, 13407, 9, 5, 3184, 14, 56, 15740, 8077, 5, 1445, 589, 4, 8911, 21, 554, 15, 5, 601, 212, 9, 392, 8, 30, 5, ...]</td>\n",
       "      <td>89</td>\n",
       "      <td>36</td>\n",
       "      <td>37</td>\n",
       "      <td>SPAN_ANSWER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>57338724d058e614000b5ca0</td>\n",
       "      <td>[0, 11195, 54, 2922, 10, 34466, 1564, 189, 33, 57, 27686, 31, 3736, 61, 2589, 835, 528, 7, 5, 39167, 5571, 23, 5, 34466, 7540, 2, 2, 1121, 35284, 9510, 957, 11247, 1059, 394, 9, 10579, 9038, 6, 8, 11, 130, 107, 37, 2622, 41, 5286, 7977, 14, 1146, 5, 334, 62, 7, 632, 2820, 30, 15059, 5, 10371, 2088, 467, 8, 1375, 409, 31, 5, 2737, 18, 2065, 8447, 1168, 11599, 8, 15855, 9723, 4, 870, 5709, 6, 5, 34466, 8975, 6, 25753, 2485, 9, 5286, 35717, 6, 58, 11923, 7, 517, 7, 10, 467, 9, 10371, 3699, 4, ...]</td>\n",
       "      <td>97</td>\n",
       "      <td>106</td>\n",
       "      <td>108</td>\n",
       "      <td>SPAN_ANSWER</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from primeqa.mrc.data_models.target_type import TargetType\n",
    "\n",
    "def target_type_as_str(feature):\n",
    "    feature['target_type'] = TargetType(feature['target_type']).name\n",
    "    return feature\n",
    "\n",
    "random_train_dataset = train_dataset.filter(lambda feature: feature['example_idx'] in random_idxs).remove_columns(['attention_mask', 'offset_mapping'])\n",
    "show_elements(random_train_dataset.map(target_type_as_str))  # Show random train features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning\n",
    "\n",
    "Here we fine-tune the model on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dccstor/mabornea1/miniconda3/envs/notebook-primeqa/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 100\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:00, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to tmp\n",
      "Configuration saved in tmp/config.json\n",
      "Model weights saved in tmp/pytorch_model.bin\n",
      "tokenizer config file saved in tmp/tokenizer_config.json\n",
      "Special tokens file saved in tmp/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =       0.62\n",
      "  total_flos               =     9154GF\n",
      "  train_loss               =     4.3079\n",
      "  train_runtime            = 0:00:29.60\n",
      "  train_samples            =        100\n",
      "  train_samples_per_second =      3.378\n",
      "  train_steps_per_second   =      0.034\n"
     ]
    }
   ],
   "source": [
    "from operator import attrgetter\n",
    "import datasets\n",
    "from transformers import DataCollatorWithPadding\n",
    "from primeqa.mrc.data_models.eval_prediction_with_processing import EvalPredictionWithProcessing\n",
    "from primeqa.mrc.processors.postprocessors.squad import SQUADPostProcessor\n",
    "from primeqa.mrc.processors.postprocessors.scorers import SupportedSpanScorers\n",
    "from primeqa.mrc.metrics.squad import squad\n",
    "\n",
    "# If using mixed precision we pad for efficient hardware acceleration\n",
    "using_mixed_precision = any(attrgetter('fp16', 'bf16')(training_args))\n",
    "data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=64 if using_mixed_precision else None)\n",
    "\n",
    "# noinspection PyProtectedMember\n",
    "postprocessor = SQUADPostProcessor(\n",
    "    k=3,\n",
    "    n_best_size=20,\n",
    "    max_answer_length=30,\n",
    "    scorer_type=SupportedSpanScorers.WEIGHTED_SUM_TARGET_TYPE_AND_SCORE_DIFF,\n",
    "    single_context_multiple_passages=preprocessor._single_context_multiple_passages,\n",
    ")\n",
    "\n",
    "def compute_metrics(p: EvalPredictionWithProcessing):\n",
    "    eval_metrics = datasets.load_metric(squad.__file__)\n",
    "    return eval_metrics.compute(predictions=p.processed_predictions, references=p.label_ids)\n",
    "\n",
    "trainer = MRCTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset if training_args.do_train else None,\n",
    "    eval_dataset=eval_dataset if training_args.do_eval else None,\n",
    "    eval_examples=eval_examples if training_args.do_eval else None,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    post_process_function=postprocessor.process_references_and_predictions,  # see QATrainer in Huggingface\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "train_result = trainer.train()\n",
    "trainer.save_model()  # Saves the tokenizer too for easy upload\n",
    "\n",
    "metrics = train_result.metrics\n",
    "max_train_samples = max_train_samples or len(train_dataset)\n",
    "metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n",
    "\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "trainer.save_metrics(\"train\", metrics)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "Here we evaluate the model on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 548.78it/s]\n",
      "100%|██████████| 20/20 [00:00<00:00, 3911.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  epoch            =   0.62\n",
      "  eval_exact_match =    0.0\n",
      "  eval_f1          = 9.5732\n",
      "  eval_samples     =     20\n"
     ]
    }
   ],
   "source": [
    "metrics = trainer.evaluate()\n",
    "\n",
    "max_eval_samples = max_eval_samples or len(eval_dataset)\n",
    "metrics[\"eval_samples\"] = min(max_eval_samples, len(eval_dataset))\n",
    "\n",
    "trainer.log_metrics(\"eval\", metrics)\n",
    "trainer.save_metrics(\"eval\", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions\n",
    "\n",
    "Here we examine the model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': '573786b51c4567190057448d',\n",
      "  'prediction_text': 'effects of gravity might be observed in different'},\n",
      " {'id': '56e1ddfce3433e14004231d8',\n",
      "  'prediction_text': 'types of integer programming problems'},\n",
      " {'id': '56bf48cc3aeaaa14008c95af',\n",
      "  'prediction_text': 'Broncos last wore matching white jerseys'},\n",
      " {'id': '5725c337271a42140099d164',\n",
      "  'prediction_text': 'species, which live as parasites on the salps on which '\n",
      "                     'adults of their species feed. In favorable '\n",
      "                     'circumstances, ctenoph'},\n",
      " {'id': '5725e44238643c19005ace36',\n",
      "  'prediction_text': 'F. Gordon, Jr. Conrad and Bean carried the first'},\n",
      " {'id': '571ccfbadd7acb1400e4c164',\n",
      "  'prediction_text': 'liners in case of depressurization emergencies. Another '\n",
      "                     'air separation technology involves forcing air to '\n",
      "                     'dissolve through ceramic membranes based on zircon'},\n",
      " {'id': '56f827caa6d7ea1400e1743a',\n",
      "  'prediction_text': \"held to determine Luther's fate. The Emperor presented \"\n",
      "                     'the final draft of the Edict of Worms on 25 May 1521, '\n",
      "                     'declaring'},\n",
      " {'id': '56e127bccd28a01900c6765f', 'prediction_text': '70 pioneers in'},\n",
      " {'id': '572976183f37b31900478431',\n",
      "  'prediction_text': 'also cause starch buildup in the chloroplasts, possibly '\n",
      "                     'due to less sucrose being exported out'},\n",
      " {'id': '56e0f32d231d4119001ac4cb',\n",
      "  'prediction_text': 'Times as saying, \"I am in too much grief to talk. What '\n",
      "                     'can I say?\"'},\n",
      " {'id': '5730005db2c2fd1400568707', 'prediction_text': 'lated into'},\n",
      " {'id': '57273a465951b619008f86ff',\n",
      "  'prediction_text': 'Construction as an industry comprises six to nine '\n",
      "                     'percent of the gross domestic product of developed '\n",
      "                     'countries. Construction starts with planning,[citation '\n",
      "                     'needed'},\n",
      " {'id': '56d71d150d65d2140019836f',\n",
      "  'prediction_text': 'Bowl 50 Host Committee has vowed to be \"the most giving '\n",
      "                     'Super Bowl ever\", and will dedicate 25 percent of all '\n",
      "                     'money it raises for philanthropic causes'},\n",
      " {'id': '56bebde53aeaaa14008c9338',\n",
      "  'prediction_text': 'moved to the Monday evening and re-branded as Super Bowl '\n",
      "                     'Opening Night'},\n",
      " {'id': '56e10b6ee3433e1400422b25',\n",
      "  'prediction_text': 'aircraft.[improper synthesis?]'},\n",
      " {'id': '571c91c8dd7acb1400e4c10e',\n",
      "  'prediction_text': \"Earth's atmosphere (see Occurrence). O2 has a bond \"\n",
      "                     'length of 121 pm and a bond energy of 498 k'},\n",
      " {'id': '57265bdfdd62a815002e82a0',\n",
      "  'prediction_text': 'recovered within two model years of the 1973 crisis. The '\n",
      "                     'Cadillac DeVille and Fleetwood, Buick Electra, Old'},\n",
      " {'id': '57287fec4b864d1900164a3d',\n",
      "  'prediction_text': 'Instead, Kublai Khan, the founder'},\n",
      " {'id': '57308f6b8ab72b1400f9c581',\n",
      "  'prediction_text': 'East, this discourse uses the idea of place-based '\n",
      "                     'identities to create difference and distance between '\n",
      "                     '\"we\" the West and'},\n",
      " {'id': '56bebbbf3aeaaa14008c9317',\n",
      "  'prediction_text': 'suspended, and that the game would be named using Arabic '\n",
      "                     'numerals as Super Bowl 50 as opposed to'}]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "with open(os.path.join(output_dir, 'eval_predictions_processed.json'), 'r') as f:\n",
    "    predictions = json.load(f)\n",
    "\n",
    "pprint(predictions)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "26b36d7f53b70a7d0de2286810c52d6f1eaac7ef68662b9afae592a53538302d"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('notebook-primeqa')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
