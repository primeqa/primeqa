{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table Question generation: WikiSQL dataset\n",
    "In this notebook, we will see how to fine-tune and evaluate a question generation model on WikiSQL dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "We start by setting some parameters to configure the process.  Note that depending on the GPU being used you may need to tune the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path=\"t5-small\"\n",
    "modality=\"table\"\n",
    "dataset_name=\"wikisql\"\n",
    "max_len=200\n",
    "target_max_len=40\n",
    "output_dir=\"../../models/qg/wikisql_nb\"\n",
    "learning_rate=0.0001\n",
    "num_train_epochs=2\n",
    "per_device_train_batch_size=8\n",
    "per_device_eval_batch_size=32\n",
    "evaluation_strategy='epoch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    evaluation_strategy='epoch',\n",
    "    learning_rate=learning_rate,\n",
    "    prediction_loss_only=True,\n",
    "    remove_unused_columns=False,\n",
    "    )\n",
    "training_args.predict_with_generate=True\n",
    "training_args.remove_unused_columns = False\n",
    "training_args.prediction_loss_only = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## WikiSQL data\n",
    "Here we load one instance of WikiSQL and visualize it. <font color='red'>This part of the code is not needed to train the model </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset wiki_sql (/Users/saneem/.cache/huggingface/datasets/wiki_sql/default/0.1.0/7037bfe6a42b1ca2b6ac3ccacba5253b1825d31379e9cc626fc79a620977252d)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table:\n",
      " +-------------------+-------+---------------+----------------+------------------+---------------------+\n",
      "| Player            |   No. | Nationality   | Position       | Years for Jazz   | School/Club Team    |\n",
      "+===================+=======+===============+================+==================+=====================+\n",
      "| Fred Saunders     |    12 | United States | Forward        | 1977-78          | Syracuse            |\n",
      "+-------------------+-------+---------------+----------------+------------------+---------------------+\n",
      "| Danny Schayes     |    24 | United States | Forward-Center | 1981-83          | Syracuse            |\n",
      "+-------------------+-------+---------------+----------------+------------------+---------------------+\n",
      "| Carey Scurry      |    22 | United States | Forward        | 1985-88          | Long Island         |\n",
      "+-------------------+-------+---------------+----------------+------------------+---------------------+\n",
      "| Robert Smith      |     5 | United States | Guard          | 1979-80          | UNLV                |\n",
      "+-------------------+-------+---------------+----------------+------------------+---------------------+\n",
      "| Kirk Snyder       |     3 | United States | Guard          | 2004-05          | Nevada              |\n",
      "+-------------------+-------+---------------+----------------+------------------+---------------------+\n",
      "| Felton Spencer    |    50 | United States | Center         | 1993-96          | Louisville          |\n",
      "+-------------------+-------+---------------+----------------+------------------+---------------------+\n",
      "| Bud Stallworth    |    15 | United States | Guard-Forward  | 1974-77          | Kansas              |\n",
      "+-------------------+-------+---------------+----------------+------------------+---------------------+\n",
      "| John Starks       |     9 | United States | Shooting guard | 2000-02          | Oklahoma State      |\n",
      "+-------------------+-------+---------------+----------------+------------------+---------------------+\n",
      "| DeShawn Stevenson |     2 | United States | Shooting guard | 2000-04          | Washington Union HS |\n",
      "+-------------------+-------+---------------+----------------+------------------+---------------------+\n",
      "Question =  Which position does John Starks play?\n",
      "SQL =  SELECT Position FROM table WHERE Player = John Starks\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from tabulate import tabulate\n",
    "\n",
    "def print_wikisql_instance(train_instance):\n",
    "    table = train_instance['table']\n",
    "    print('Table:\\n',tabulate(table['rows'], headers=table['header'], tablefmt='grid'))\n",
    "\n",
    "    print('Question = ',train_instance['question'])\n",
    "    print('SQL = ', train_instance['sql']['human_readable'])\n",
    "\n",
    "train_instance = load_dataset('wikisql', split='train[1001:1002]')[0]\n",
    "print_wikisql_instance(train_instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SQL gets converted to a string format which goes as input to generator to generate question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset wiki_sql (/Users/saneem/.cache/huggingface/datasets/wiki_sql/default/0.1.0/7037bfe6a42b1ca2b6ac3ccacba5253b1825d31379e9cc626fc79a620977252d)\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1392.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question =  Which position does John Starks play?\n",
      "\n",
      "Input to generator =  select <<sep>> Position <<sep>> Player <<cond>> equal <<cond>> John Starks <<answer>> shooting guard <<header>> Player <<hsep>> No. <<hsep>> Nationality <<hsep>> Position <<hsep>> Years for Jazz <<hsep>> School/Club Team\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from primeqa.qg.processors.table_qg.wikisql_processor import WikiSqlDataset\n",
    "\n",
    "data = WikiSqlDataset()\n",
    "processed_data = data.preprocess_data_for_qg('train[1001:1002]')\n",
    "print('Question = ', processed_data['question'][0])\n",
    "print('\\nInput to generator = ', processed_data['input'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Loading the Model\n",
    "\n",
    "Here we load the model based on the model_name and modality parameter set above. For WikiSQL we keep modality='table'. Other option is modality='passage'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c139c5792ba47acacbf5d9e08d2b4dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/231M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1616ae7a9a0a4d4f90def08e9ac1b643",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/773k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "481f9e4c982b44ddb98f2524248cacee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.32M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from primeqa.qg.models.qg_model import QGModel\n",
    "\n",
    "qg_model = QGModel(model_name_or_path, modality=modality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data\n",
    "\n",
    "Here we load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset wiki_sql (/Users/saneem/.cache/huggingface/datasets/wiki_sql/default/0.1.0/7037bfe6a42b1ca2b6ac3ccacba5253b1825d31379e9cc626fc79a620977252d)\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 2875.57it/s]\n",
      "Parameter 'function'=<function QGDataLoader.convert_to_features at 0x7f9c234ce200> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62fa7684c56540a4b6ac7b69af217f3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/opt/miniconda3/envs/oneqa/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2277: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "Using custom data configuration default\n",
      "Reusing dataset wiki_sql (/Users/saneem/.cache/huggingface/datasets/wiki_sql/default/0.1.0/7037bfe6a42b1ca2b6ac3ccacba5253b1825d31379e9cc626fc79a620977252d)\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 2104.45it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a70a95b6492438abd64197fb80ba9db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from primeqa.qg.processors.data_loader import QGDataLoader\n",
    "\n",
    "qgdl = QGDataLoader(\n",
    "    tokenizer=qg_model.tokenizer,\n",
    "    dataset_name=dataset_name,\n",
    "    input_max_len=max_len,\n",
    "    target_max_len=target_max_len\n",
    "    )\n",
    "\n",
    "train_dataset = qgdl.create(\"train[:100]\")\n",
    "valid_dataset = qgdl.create(\"validation[:50]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train using QGTrainer\n",
    "Here we create a QG trainer with the training arguments defined above and use it to train on Wikisql training data (or any custom data following the same format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 100\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 26\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='26' max='26' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [26/26 01:51, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.906801</td>\n",
       "      <td>23.251000</td>\n",
       "      <td>12.696800</td>\n",
       "      <td>21.460000</td>\n",
       "      <td>21.459600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.777056</td>\n",
       "      <td>24.015600</td>\n",
       "      <td>13.354900</td>\n",
       "      <td>22.792700</td>\n",
       "      <td>22.706900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 32\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 32\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to ../../models/qg/wikisql_nb\n",
      "Configuration saved in ../../models/qg/wikisql_nb/config.json\n",
      "Model weights saved in ../../models/qg/wikisql_nb/pytorch_model.bin\n",
      "tokenizer config file saved in ../../models/qg/wikisql_nb/tokenizer_config.json\n",
      "Special tokens file saved in ../../models/qg/wikisql_nb/special_tokens_map.json\n",
      "Copy vocab file to ../../models/qg/wikisql_nb/spiece.model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 114.8694, 'train_samples_per_second': 1.741, 'train_steps_per_second': 0.226, 'total_flos': 10573578240000.0, 'train_loss': 3.134342780480018, 'epoch': 2.0}\n"
     ]
    }
   ],
   "source": [
    "from primeqa.qg.trainers.qg_trainer import QGTrainer\n",
    "from primeqa.qg.metrics.generation_metrics import rouge_metrics\n",
    "from primeqa.qg.utils.data_collator import T2TDataCollator\n",
    "import os\n",
    "\n",
    "compute_metrics = rouge_metrics(qg_model.tokenizer)\n",
    "\n",
    "trainer = QGTrainer(\n",
    "    model=qg_model.model,\n",
    "    tokenizer = qg_model.tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    valid_dataset=valid_dataset,\n",
    "    data_collator=T2TDataCollator(),\n",
    "    compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "train_results = trainer.train()\n",
    "trainer.save_model()\n",
    "print(train_results.metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Here we evaluate the trained model on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.7770562171936035, 'eval_rouge1': 24.0156, 'eval_rouge2': 13.3549, 'eval_rougeL': 22.7927, 'eval_rougeLsum': 22.7069, 'eval_runtime': 14.0715, 'eval_samples_per_second': 3.553, 'eval_steps_per_second': 0.142, 'epoch': 2.0}\n"
     ]
    }
   ],
   "source": [
    "metrics = trainer.evaluate()\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f2837b60cedc613a72d719d2d261dedab01e8683bba6b8605ad579171c0f5b25"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "2c35b992e6c7aefc6892dbea5982d2f0b243183ae5e95337e08b7ede6fdab7cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
