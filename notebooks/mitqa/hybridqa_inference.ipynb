{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running inference of the pre-trained model on HybridQA datset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"per_device_train_batch_size_rr\":8,\n",
    "    \"per_device_eval_batch_size_rr\":8,\n",
    "    \"rr_model_name\":\"bert-base-uncased\",\n",
    "    \"row_retriever_model_name_path\":\"/dccstor/cssblr/vishwajeet/git/hybridqa_primeqa/data/hybridqa/pretrained_models/rr.bin\",\n",
    "    \"pos_frac_per_epoch\":[0.3, 0.3, 0.1, 0.0001, 0.0001],\n",
    "    \"group_frac_per_epoch\":[0.0, 0.5, 1.0, 1.0, 1.0],\n",
    "    \"max_seq_length\":512,\n",
    "    \"per_gpu_train_batch_size\":8,\n",
    "    \"train_batch_size\":8,\n",
    "    \"per_gpu_eval_batch_size\":8,\n",
    "    \"eval_batch_size\":8,\n",
    "    \"max_query_length\":64,\n",
    "    \"threads\":1,\n",
    "    \"null_score_diff_threshold\":0.0,\n",
    "    \"n_best_size\":20,\n",
    "    \"do_predict_ae\":True,\n",
    "    \"n_gpu\":1,\n",
    "    \"max_answer_length\":30,\n",
    "    \"model_name_or_path_ae\":\"bert-base-uncased\",\n",
    "    \"model_type\":\"bert\",\n",
    "    \"doc_stride\":128,\n",
    "    \"pred_ans_file\":\"/dccstor/cssblr/vishwajeet/git/hybridqa_primeqa/data/hybridqa/predictions/answer_extractor_output_test.json\",\n",
    "    \"eval_file\":\"/dccstor/cssblr/vishwajeet/git/hybridqa_primeqa/data/hybridqa/ae_input_test.json\",\n",
    "    \"output_dir\":\"/dccstor/cssblr/vishwajeet/git/hybridqa_primeqa/data/hybridqa/models/answer_extractor/\",\n",
    "    \"model\":\"gpt2\",\n",
    "    \"top_k\":0,\n",
    "    \"top_p\":0.9,\n",
    "    \"seed_lg\":42,\n",
    "    \"batch_size_lg\":2,\n",
    "    \"linker_model\":\"/dccstor/cssblr/vishwajeet/git/hybridqa_primeqa/data/ottqa/models/link_generator/model-ep9.pt\",\n",
    "    \"max_source_len\":32,\n",
    "    \"max_target_len\":16,\n",
    "    \"do_all_lg\":True,\n",
    "    \"data_path_root\":\"/dccstor/cssblr/vishwajeet/git/hybridqa_primeqa/data/hybridqa/\",\n",
    "    \"dataset_name\":\"hybridqa\",\n",
    "    \"test_data_path\":\"/dccstor/cssblr/vishwajeet/git/hybridqa_primeqa/data/hybridqa/toy.json\",\n",
    "    \"collections_file\":\"linearized_tables.tsv\",\n",
    "    \"test\":True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dccstor/cssblr/vishwajeet/git/primeqaenv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from primeqa.mitqa.utils.arguments_utils import HybridQAArguments,LinkPredictorArguments, RRArguments,AEArguments\n",
    "\n",
    "\n",
    "hqa_parser = HfArgumentParser((HybridQAArguments,LinkPredictorArguments, RRArguments,AEArguments))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parse configs from config dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No GPU found. Please add GPU to your notebook\n",
      "{\"time\":\"2022-12-23 05:42:41,777\", \"name\": \"sentence_transformers.SentenceTransformer\", \"level\": \"INFO\", \"message\": \"Load pretrained SentenceTransformer: msmarco-distilbert-base-tas-b\"}\n",
      "{\"time\":\"2022-12-23 05:42:43,593\", \"name\": \"sentence_transformers.SentenceTransformer\", \"level\": \"INFO\", \"message\": \"Use pytorch device: cpu\"}\n",
      "Warning: No GPU found. Please add GPU to your notebook\n",
      "{\"time\":\"2022-12-23 05:42:44,287\", \"name\": \"sentence_transformers.SentenceTransformer\", \"level\": \"INFO\", \"message\": \"Load pretrained SentenceTransformer: msmarco-distilbert-base-tas-b\"}\n",
      "{\"time\":\"2022-12-23 05:42:45,739\", \"name\": \"sentence_transformers.SentenceTransformer\", \"level\": \"INFO\", \"message\": \"Use pytorch device: cpu\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.84it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.64it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.16it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.91it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.19it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.05it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.97it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.23it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.78it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.32it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.25it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.20it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.29it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.13it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.20it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.11it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.23it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.08it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.16it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.27it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.01it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.21it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.24it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.20it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.35it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.23it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.19it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.03it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.30it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.09it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.98it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.15it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.13it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.13it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.77it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.04it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.08it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.05it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.92it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.21it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.14it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.37it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.08it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.78it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.87it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.12it/s]\n",
      " 10%|█         | 2/20 [01:01<09:16, 30.92s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m hqa_args,lp_args,rr_args,ae_args,\u001b[39m=\u001b[39m hqa_parser\u001b[39m.\u001b[39mparse_dict(config)\n\u001b[1;32m     17\u001b[0m raw_test_data \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(\u001b[39mopen\u001b[39m(hqa_args\u001b[39m.\u001b[39mtest_data_path))\n\u001b[0;32m---> 18\u001b[0m test_data_processed \u001b[39m=\u001b[39m preprocess_data(hqa_args\u001b[39m.\u001b[39;49mdata_path_root,hqa_args\u001b[39m.\u001b[39;49mdataset_name,raw_test_data,split\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtest\u001b[39;49m\u001b[39m\"\u001b[39;49m,test\u001b[39m=\u001b[39;49mtest)\n\u001b[1;32m     19\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mInitial preprocessing done\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m rr \u001b[39m=\u001b[39m RowRetriever(hqa_args,rr_args)\n",
      "File \u001b[0;32m/dccstor/cssblr/vishwajeet/git/hybridqa_primeqa/primeqa/mitqa/processors/preprocessors/preprocess_raw_data.py:119\u001b[0m, in \u001b[0;36mpreprocess_data\u001b[0;34m(data_root_path, dataset_name, raw_data, split, test)\u001b[0m\n\u001b[1;32m    117\u001b[0m     npr \u001b[39m=\u001b[39m npr[\u001b[39m0\u001b[39m]\n\u001b[1;32m    118\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 119\u001b[0m     npr \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(get_top_k_passages(npr, question_str, \u001b[39m100\u001b[39;49m, r))\n\u001b[1;32m    121\u001b[0m npi[\u001b[39m'\u001b[39m\u001b[39mtable_passage_row\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m npr\n\u001b[1;32m    123\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m test:\n",
      "File \u001b[0;32m/dccstor/cssblr/vishwajeet/git/hybridqa_primeqa/primeqa/mitqa/processors/preprocessors/preprocess_raw_data.py:28\u001b[0m, in \u001b[0;36mget_top_k_passages\u001b[0;34m(passages, query, top_k, row)\u001b[0m\n\u001b[1;32m     26\u001b[0m         row_str \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m k \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m is \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m v \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m . \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     27\u001b[0m     passages \u001b[39m=\u001b[39m [row_str\u001b[39m+\u001b[39mpassage \u001b[39mfor\u001b[39;00m passage \u001b[39min\u001b[39;00m passages]\n\u001b[0;32m---> 28\u001b[0m corpus_embeddings \u001b[39m=\u001b[39m doc_retriever\u001b[39m.\u001b[39;49mencode(passages, convert_to_tensor\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, show_progress_bar\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     29\u001b[0m corpus_embeddings \u001b[39m=\u001b[39m util\u001b[39m.\u001b[39mnormalize_embeddings(corpus_embeddings)\n\u001b[1;32m     31\u001b[0m query_embeddings \u001b[39m=\u001b[39m doc_retriever\u001b[39m.\u001b[39mencode([query], convert_to_tensor\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/dccstor/cssblr/vishwajeet/git/primeqaenv/lib/python3.8/site-packages/sentence_transformers/SentenceTransformer.py:165\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[1;32m    162\u001b[0m features \u001b[39m=\u001b[39m batch_to_device(features, device)\n\u001b[1;32m    164\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 165\u001b[0m     out_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(features)\n\u001b[1;32m    167\u001b[0m     \u001b[39mif\u001b[39;00m output_value \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtoken_embeddings\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    168\u001b[0m         embeddings \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m/dccstor/cssblr/vishwajeet/git/primeqaenv/lib/python3.8/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m/dccstor/cssblr/vishwajeet/git/primeqaenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/dccstor/cssblr/vishwajeet/git/primeqaenv/lib/python3.8/site-packages/sentence_transformers/models/Transformer.py:66\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mtoken_type_ids\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m features:\n\u001b[1;32m     64\u001b[0m     trans_features[\u001b[39m'\u001b[39m\u001b[39mtoken_type_ids\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m features[\u001b[39m'\u001b[39m\u001b[39mtoken_type_ids\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m---> 66\u001b[0m output_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mauto_model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mtrans_features, return_dict\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     67\u001b[0m output_tokens \u001b[39m=\u001b[39m output_states[\u001b[39m0\u001b[39m]\n\u001b[1;32m     69\u001b[0m features\u001b[39m.\u001b[39mupdate({\u001b[39m'\u001b[39m\u001b[39mtoken_embeddings\u001b[39m\u001b[39m'\u001b[39m: output_tokens, \u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m: features[\u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m]})\n",
      "File \u001b[0;32m/dccstor/cssblr/vishwajeet/git/primeqaenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/dccstor/cssblr/vishwajeet/git/primeqaenv/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py:549\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[39mif\u001b[39;00m inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    548\u001b[0m     inputs_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(input_ids)  \u001b[39m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[0;32m--> 549\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[1;32m    550\u001b[0m     x\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    551\u001b[0m     attn_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    552\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    553\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    554\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    555\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    556\u001b[0m )\n",
      "File \u001b[0;32m/dccstor/cssblr/vishwajeet/git/primeqaenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/dccstor/cssblr/vishwajeet/git/primeqaenv/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py:327\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[39mif\u001b[39;00m output_hidden_states:\n\u001b[1;32m    325\u001b[0m     all_hidden_states \u001b[39m=\u001b[39m all_hidden_states \u001b[39m+\u001b[39m (hidden_state,)\n\u001b[0;32m--> 327\u001b[0m layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    328\u001b[0m     x\u001b[39m=\u001b[39;49mhidden_state, attn_mask\u001b[39m=\u001b[39;49mattn_mask, head_mask\u001b[39m=\u001b[39;49mhead_mask[i], output_attentions\u001b[39m=\u001b[39;49moutput_attentions\n\u001b[1;32m    329\u001b[0m )\n\u001b[1;32m    330\u001b[0m hidden_state \u001b[39m=\u001b[39m layer_outputs[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m    332\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m/dccstor/cssblr/vishwajeet/git/primeqaenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/dccstor/cssblr/vishwajeet/git/primeqaenv/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py:271\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    262\u001b[0m \u001b[39mParameters:\u001b[39;00m\n\u001b[1;32m    263\u001b[0m \u001b[39m    x: torch.tensor(bs, seq_length, dim)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[39m    torch.tensor(bs, seq_length, dim) The output of the transformer block contextualization.\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[39m# Self-Attention\u001b[39;00m\n\u001b[0;32m--> 271\u001b[0m sa_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m    272\u001b[0m     query\u001b[39m=\u001b[39;49mx,\n\u001b[1;32m    273\u001b[0m     key\u001b[39m=\u001b[39;49mx,\n\u001b[1;32m    274\u001b[0m     value\u001b[39m=\u001b[39;49mx,\n\u001b[1;32m    275\u001b[0m     mask\u001b[39m=\u001b[39;49mattn_mask,\n\u001b[1;32m    276\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    277\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    278\u001b[0m )\n\u001b[1;32m    279\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n\u001b[1;32m    280\u001b[0m     sa_output, sa_weights \u001b[39m=\u001b[39m sa_output  \u001b[39m# (bs, seq_length, dim), (bs, n_heads, seq_length, seq_length)\u001b[39;00m\n",
      "File \u001b[0;32m/dccstor/cssblr/vishwajeet/git/primeqaenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/dccstor/cssblr/vishwajeet/git/primeqaenv/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py:217\u001b[0m, in \u001b[0;36mMultiHeadSelfAttention.forward\u001b[0;34m(self, query, key, value, mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    214\u001b[0m     weights \u001b[39m=\u001b[39m weights \u001b[39m*\u001b[39m head_mask\n\u001b[1;32m    216\u001b[0m context \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmatmul(weights, v)  \u001b[39m# (bs, n_heads, q_length, dim_per_head)\u001b[39;00m\n\u001b[0;32m--> 217\u001b[0m context \u001b[39m=\u001b[39m unshape(context)  \u001b[39m# (bs, q_length, dim)\u001b[39;00m\n\u001b[1;32m    218\u001b[0m context \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout_lin(context)  \u001b[39m# (bs, q_length, dim)\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m/dccstor/cssblr/vishwajeet/git/primeqaenv/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py:198\u001b[0m, in \u001b[0;36mMultiHeadSelfAttention.forward.<locals>.unshape\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39munshape\u001b[39m(x):\n\u001b[1;32m    197\u001b[0m     \u001b[39m\"\"\"group heads\"\"\"\u001b[39;00m\n\u001b[0;32m--> 198\u001b[0m     \u001b[39mreturn\u001b[39;00m x\u001b[39m.\u001b[39;49mtranspose(\u001b[39m1\u001b[39;49m, \u001b[39m2\u001b[39;49m)\u001b[39m.\u001b[39;49mcontiguous()\u001b[39m.\u001b[39mview(bs, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_heads \u001b[39m*\u001b[39m dim_per_head)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "from primeqa.mitqa.utils.model_utils.row_retriever_MITQA import RowRetriever\n",
    "from primeqa.mitqa.utils.model_utils.reranker import re_rank_ae_output\n",
    "from primeqa.mitqa.utils.model_utils.process_row_retriever_output import preprocess_data_using_row_retrieval_scores,create_dataset_for_answer_extractor\n",
    "from primeqa.mitqa.utils.model_utils.answer_extractor_multi_Answer import run_answer_extractor\n",
    "from primeqa.mitqa.processors.preprocessors.preprocess_raw_data import preprocess_data\n",
    "import logging\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "from primeqa.mitqa.utils.arguments_utils import HybridQAArguments,LinkPredictorArguments, RRArguments,AEArguments\n",
    "\n",
    "# Read agruments and load the dataset\n",
    "test=True\n",
    "hqa_args,lp_args,rr_args,ae_args,= hqa_parser.parse_dict(config)\n",
    "raw_test_data = json.load(open(hqa_args.test_data_path))\n",
    "test_data_processed = preprocess_data(hqa_args.data_path_root,hqa_args.dataset_name,raw_test_data,split=\"test\",test=test)\n",
    "print(\"Initial preprocessing done\")\n",
    "rr = RowRetriever(hqa_args,rr_args)\n",
    "qid_scores_dict = rr.predict(test_data_processed)\n",
    "print(\"Row retrieval predictions Done\")\n",
    "test_processed_data = preprocess_data_using_row_retrieval_scores(raw_test_data,qid_scores_dict,test)\n",
    "print(\"Row retrieval output processed\")\n",
    "answer_extraction_data = create_dataset_for_answer_extractor(test_processed_data,hqa_args.data_path_root,test)\n",
    "print(\"Answer extraction data generated\")\n",
    "ae_output_path,ae_output_path_nbest = run_answer_extractor(ae_args,answer_extraction_data)\n",
    "print(ae_output_path)\n",
    "print(ae_output_path_nbest)\n",
    "re_ranked_output = re_rank_ae_output(qid_scores_dict,ae_output_path_nbest,ae_args.pred_ans_file) \n",
    "print(\"re-ranked output saved at \",re_ranked_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "63ab5b990377b92889cd2d9be280496c653542c57957c3f29826e5bc9caeafbc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
