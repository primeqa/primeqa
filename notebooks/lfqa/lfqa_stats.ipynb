{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.sentencizer.Sentencizer at 0x7fc710d60880>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load tydi json and compute stats on the questions and answers\n",
    "# this works for any data in Kilt-ELI5/LongNQ format\n",
    "import glob\n",
    "import gzip\n",
    "import json\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.add_pipe('sentencizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_json_from_file(gt_file_patterns, count=-1):\n",
    "    data = []\n",
    "    if gt_file_patterns.endswith('gz'):\n",
    "        f = gzip.open(gt_file_patterns, 'rt', encoding='utf-8')\n",
    "    else:\n",
    "        f = open(gt_file_patterns, 'rt', encoding='utf-8')\n",
    "    lines = f.readlines()\n",
    "    for line in lines[0:count]:\n",
    "        data.append(json.loads(line))\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "import tqdm\n",
    "\n",
    "def nlp_count(text):\n",
    "    tokens = [[token.text for token in sent] for sent in nlp(text, disable=['parser', 'tagger', 'ner']).sents]\n",
    "\n",
    "    token_count = 0\n",
    "\n",
    "    for sentence in tokens:\n",
    "        for word in sentence:\n",
    "            token_count += 1\n",
    "    return token_count, len(tokens)\n",
    "\n",
    "\n",
    "def compute_stats(data):\n",
    "\n",
    "    stats = {}\n",
    "    stats['q_words'] = 0\n",
    "    stats['p_words'] = 0\n",
    "    stats['a_words'] = 0\n",
    "    stats['a_sentences'] = 0\n",
    "    stats['a_per_q'] = 0\n",
    "    stats['s_per_p'] = 0\n",
    "    stats['passages'] = 0\n",
    "    stats['unanswerable'] = 0\n",
    "    stats['first_word'] = {}\n",
    "\n",
    "    for i in tqdm.tqdm(range(len(data))):\n",
    "                       \n",
    "        example = data[i]\n",
    "        \n",
    "        example_id = example['id']\n",
    "        question = example[\"input\"]\n",
    "\n",
    "        q_word = question.split()[0]\n",
    "\n",
    "        annotations = example['output']\n",
    "\n",
    "        if annotations == None:\n",
    "            continue\n",
    "\n",
    "        '''\n",
    "        # words in question\n",
    "        # words in passage\n",
    "        # words in answer\n",
    "        # sentences in answer\n",
    "        # of answers per q\n",
    "        # of passages\n",
    "        '''\n",
    "        if q_word in stats['first_word']:\n",
    "            stats['first_word'][q_word]+= 1\n",
    "        else:\n",
    "            stats['first_word'][q_word] = 1\n",
    "\n",
    "        token_count, _ = nlp_count(question)\n",
    "        stats['q_words'] += token_count\n",
    "\n",
    "        # if 'passages' in example:\n",
    "        #     for passage in example['passages']:\n",
    "        #         token_count, sentence_count = nlp_count(passage['title'] + \" \" + passage['text'])\n",
    "        #         stats['p_words'] += token_count\n",
    "        #         stats['s_per_p'] += sentence_count\n",
    "        #         stats['passages'] += 1\n",
    "\n",
    "        for answer in example['output']:\n",
    "            # unanswerable\n",
    "            if answer['answer'] == None or answer['answer'] == \"\":\n",
    "                stats['unanswerable'] += 1\n",
    "                continue\n",
    "            token_count, sentence_count = nlp_count(answer['answer'])\n",
    "            stats['a_words'] += token_count\n",
    "            stats['a_sentences'] += sentence_count\n",
    "            stats['a_per_q'] += 1\n",
    "\n",
    "    # print(stats)\n",
    "    print(f\"Queries\\t{len(data)}\")\n",
    "    print(f\"A per Q\\t{stats['a_per_q']/len(data)}\")\n",
    "    print(f\"WORDS in Q\\t{stats['q_words']/len(data)}\")\n",
    "    print(f\"WORDS in A\\t{stats['a_words']/stats['a_per_q']}\")\n",
    "    print(f\"SENTENCES in A\\t{stats['a_sentences']/stats['a_per_q']}\")\n",
    "    if stats['passages'] > 0:\n",
    "        print(f\"WORDS in P\\t{stats['p_words']/stats['passages']}\")\n",
    "        print(f\"S per P\\t{stats['s_per_p']/stats['passages']}\")\n",
    "    print(f\"Unanswerable\\t{stats['unanswerable']}\")\n",
    "\n",
    "    aggegrated_stats = [len(data),stats['a_per_q']/len(data),stats['q_words']/len(data),stats['a_sentences']/stats['a_per_q'],stats['unanswerable']]\n",
    "\n",
    "    for word in stats['first_word']:\n",
    "        if stats['first_word'][word] > 10:\n",
    "            print(f\"{word}\\t{stats['first_word'][word]}\")\n",
    "\n",
    "    return aggegrated_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_dir, count=-1):\n",
    "\n",
    "    files = glob.glob(data_dir)\n",
    "    data = []\n",
    "    for file_n in files:\n",
    "        data.extend(load_json_from_file(file_n, count))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev\n",
      "Queries\t40\n",
      "A per Q\t1.375\n",
      "WORDS in Q\t9.35\n",
      "WORDS in A\t41.872727272727275\n",
      "SENTENCES in A\t1.6545454545454545\n",
      "WORDS in P\t155.875\n",
      "S per P\t6.125\n",
      "Unanswerable\t0\n",
      "what\t11\n"
     ]
    }
   ],
   "source": [
    "train_data_dir = \"/dccstor/srosent2/generative/appen/final/longNQ/train/*jsonl\"\n",
    "dev_data_dir = \"/dccstor/srosent2/generative/appen/final/longNQ/dev/*jsonl\"\n",
    "test_data_dir = \"/dccstor/srosent2/generative/appen/final/longNQ/test/*jsonl\"\n",
    "\n",
    "# print('train')\n",
    "# compute_stats(load_data(train_data_dir))\n",
    "print('dev')\n",
    "compute_stats(load_data(dev_data_dir)[:40])\n",
    "# print('test')\n",
    "# compute_stats(load_data(test_data_dir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev\n",
      "Queries\t40\n",
      "A per Q\t2.0\n",
      "WORDS in Q\t9.625\n",
      "WORDS in A\t70.575\n",
      "SENTENCES in A\t2.9625\n",
      "Unanswerable\t0\n",
      "Who\t20\n"
     ]
    }
   ],
   "source": [
    "train_data_dir = \"/dccstor/srosent2/primeqa-mengxia/data/asqa/formatted/ASQA_train.json\"\n",
    "dev_data_dir = \"/dccstor/srosent2/primeqa-mengxia/data/asqa/formatted/ASQA_dev.json\"\n",
    "test_data_dir = \"/dccstor/srosent2/primeqa-mengxia/data/asqa/formatted/ASQA_test.json\"\n",
    "\n",
    "# print('train')\n",
    "# compute_stats(load_data(train_data_dir))\n",
    "print('dev')\n",
    "compute_stats(load_data(dev_data_dir)[:40])\n",
    "# print('test')\n",
    "# compute_stats(load_data(test_data_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev\n",
      "Queries\t40\n",
      "A per Q\t13.0\n",
      "WORDS in Q\t16.675\n",
      "WORDS in A\t105.28076923076924\n",
      "SENTENCES in A\t5.476923076923077\n",
      "WORDS in P\t107.0445\n",
      "S per P\t4.443\n",
      "Unanswerable\t0\n",
      "Why\t13\n"
     ]
    }
   ],
   "source": [
    "train_data_dir = \"/dccstor/srosent2/primeqa-mengxia/data/dpr-100passages_withkg_best_all/eli5-train*\"\n",
    "dev_data_dir = \"/dccstor/srosent2/primeqa-mengxia/data/dpr-100passages_withkg_best_all/eli5-dev*\"\n",
    "\n",
    "# print('train')\n",
    "# compute_stats(load_data(train_data_dir))\n",
    "print('dev')\n",
    "compute_stats(load_data(dev_data_dir)[:40])\n",
    "# print('test')\n",
    "# compute_stats(load_data(test_data_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-01 22:42:17.971421: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-01 22:42:20.979020: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-01 22:42:20.979098: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-01 22:42:21.029843: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-01 22:42:22.605370: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-01 22:42:28.170263: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "100%|██████████| 6253/6253 [08:48<00:00, 11.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dump train abstractive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 811/811 [00:08<00:00, 95.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dump test abstractive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 661/661 [00:05<00:00, 117.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dump validation abstractive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6253/6253 [08:50<00:00, 11.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dump train extractive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 811/811 [00:08<00:00, 96.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dump test extractive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 661/661 [00:05<00:00, 117.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dump validation extractive\n"
     ]
    }
   ],
   "source": [
    "# AquaMuse\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import tqdm \n",
    "\n",
    "json_data = []\n",
    "\n",
    "for data_type in ['abstractive', 'extractive']:\n",
    "    dataset = load_dataset(\"aquamuse\",data_type)\n",
    "    for split, dataset in dataset.items():\n",
    "\n",
    "        for i in tqdm.tqdm(range(len(dataset))):\n",
    "            json_example = {}\n",
    "            # dataset conversion\n",
    "            # query, input_urls, target\n",
    "            json_example['input'] = dataset['query'][i]\n",
    "            json_example['id'] = i\n",
    "            json_example['output'] = [{'answer':dataset['target'][i], 'meta':{'urls':dataset['input_urls'][i]}}]\n",
    "            json_data.append(json_example)\n",
    "\n",
    "        print(f\"dump {split} {data_type}\")\n",
    "        pd.DataFrame.from_dict(json_data).to_json(f\"/dccstor/srosent2/generative/external_datasets/aquamuse/{data_type}/{split}.jsonl\", orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'load_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/dccstor/srosent2/primeqa/primeqa/notebooks/lfqa/lfqa_stats.ipynb Cell 9\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcccxl013.pok.ibm.com/dccstor/srosent2/primeqa/primeqa/notebooks/lfqa/lfqa_stats.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# print('train')\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcccxl013.pok.ibm.com/dccstor/srosent2/primeqa/primeqa/notebooks/lfqa/lfqa_stats.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# compute_stats(load_data(train_data_dir))\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcccxl013.pok.ibm.com/dccstor/srosent2/primeqa/primeqa/notebooks/lfqa/lfqa_stats.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mdev\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bcccxl013.pok.ibm.com/dccstor/srosent2/primeqa/primeqa/notebooks/lfqa/lfqa_stats.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m compute_stats(load_data(dev_data_dir)[:\u001b[39m40\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcccxl013.pok.ibm.com/dccstor/srosent2/primeqa/primeqa/notebooks/lfqa/lfqa_stats.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# print('test')\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcccxl013.pok.ibm.com/dccstor/srosent2/primeqa/primeqa/notebooks/lfqa/lfqa_stats.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# compute_stats(load_data(test_data_dir))\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_data' is not defined"
     ]
    }
   ],
   "source": [
    "train_data_dir = \"/dccstor/srosent2/generative/external_datasets/aquamuse/abstractive/train.jsonl\"\n",
    "dev_data_dir = \"/dccstor/srosent2/generative/external_datasets/aquamuse/abstractive/validation.jsonl\"\n",
    "\n",
    "# print('train')\n",
    "# compute_stats(load_data(train_data_dir))\n",
    "print('dev')\n",
    "compute_stats(load_data(dev_data_dir)[:40])\n",
    "# print('test')\n",
    "# compute_stats(load_data(test_data_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 817/817 [00:07<00:00, 106.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dump validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Truthful QA\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import tqdm \n",
    "\n",
    "json_data = []\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"truthful_qa\",'generation')\n",
    "for split, dataset in dataset.items():\n",
    "\n",
    "    for i in tqdm.tqdm(range(len(dataset))):\n",
    "        # dataset conversion\n",
    "        # type, question, correct_answers, source\n",
    "        json_example = {}\n",
    "        json_example['input'] = dataset['question'][i]\n",
    "        json_example['id'] = i\n",
    "        json_example['output'] = []\n",
    "        for answer in dataset['correct_answers'][i]:\n",
    "            json_example['output'].append({'answer': answer, 'meta':{'urls':[dataset['source'][i]]}})\n",
    "        json_data.append(json_example)\n",
    "\n",
    "    print(f\"dump {split}\")\n",
    "    pd.DataFrame.from_dict(json_data).to_json(f\"/dccstor/srosent2/generative/external_datasets/truthful_qa/generation/{split}.jsonl\", orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = \"/dccstor/srosent2/generative/external_datasets/aquamuse/abstractive/train.jsonl\"\n",
    "dev_data_dir = \"/dccstor/srosent2/generative/external_datasets/aquamuse/abstractive/validation.jsonl\"\n",
    "\n",
    "# print('train')\n",
    "# compute_stats(load_data(train_data_dir))\n",
    "print('dev')\n",
    "compute_stats(load_data(dev_data_dir)[:40])\n",
    "# print('test')\n",
    "# compute_stats(load_data(test_data_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dump train\n",
      "dump test\n",
      "dump val\n"
     ]
    }
   ],
   "source": [
    "# expertQA\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "data_dir = glob.glob(\"/dccstor/srosent2/generative/external_datasets/expertqa/original_format/domain_lfqa*\")\n",
    "\n",
    "for data_split in data_dir:\n",
    "    json_data = []\n",
    "\n",
    "    split = data_split[len(\"/dccstor/srosent2/generative/external_datasets/expertqa/original_format/domain_lfqa_\"):-5]\n",
    "\n",
    "    data = pd.read_json(data_split, lines=True, orient='records')    \n",
    "\n",
    "    for i, row in data.iterrows():\n",
    "        json_example = {}\n",
    "        json_example['id'] = row['example_id']\n",
    "        json_example['input'] = row['question']\n",
    "        json_example['output'] = [{'answer': row['answer'], 'meta':{}}]\n",
    "        json_data.append(json_example)\n",
    "    print(f\"dump {split}\")\n",
    "    pd.DataFrame.from_dict(json_data).to_json(f\"/dccstor/srosent2/generative/external_datasets/expertqa/{split}.jsonl\", orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 904460/904460 [09:44<00:00, 1547.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dump train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 211255/211255 [02:35<00:00, 1360.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dump test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72474/72474 [00:52<00:00, 1387.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dump valid\n"
     ]
    }
   ],
   "source": [
    "# wikihowqa\n",
    "\n",
    "import glob\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "\n",
    "corpus = pd.read_csv(\"/dccstor/srosent2/generative/external_datasets/wikihowQA/original_format/summary.txt\", names=['id','summary','answer'],delimiter='\\t')   \n",
    "\n",
    "data_dir = glob.glob(\"/dccstor/srosent2/generative/external_datasets/wikihowQA/original_format/*.txt\")\n",
    "\n",
    "for data_split in data_dir:\n",
    "    \n",
    "    json_data = []\n",
    "\n",
    "    split = data_split[len(\"/dccstor/srosent2/generative/external_datasets/wikihowQA/original_format/\"):-4]\n",
    "\n",
    "    if split == \"summary\": \n",
    "        continue\n",
    "\n",
    "    data = pd.read_csv(data_split, delimiter='\\t', names=['input','pid','label'])   \n",
    "\n",
    "    for i in tqdm.tqdm(range(len(data))):\n",
    "        row = data.iloc[i]\n",
    "        json_example = {}\n",
    "        json_example['id'] = i\n",
    "        json_example['input'] = row['input']\n",
    "        answers = corpus[corpus['id'] == row['pid']]\n",
    "\n",
    "        if len(answers) > 1:\n",
    "            print(\"multiple answers\")\n",
    "       \n",
    "        json_example['passages'] = [{'title':\"\",'text':answers.iloc[0]['summary'],'sentences':\"\"}]\n",
    "        json_example['output'] = [{'answer': answers.iloc[0]['answer'], 'meta':{'docid':answers.iloc[0]['id']}}]\n",
    "        json_data.append(json_example)\n",
    "    print(f\"dump {split}\")\n",
    "    pd.DataFrame.from_dict(json_data).to_json(f\"/dccstor/srosent2/generative/external_datasets/wikihowQA/{split}.jsonl\", orient='records', lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dccstor/srosent2/generative/external_datasets/wikihowQA/test.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 16067/211254 [02:26<29:37, 109.81it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/dccstor/srosent2/primeqa/primeqa/notebooks/lfqa/lfqa_stats.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcccxl013.pok.ibm.com/dccstor/srosent2/primeqa/primeqa/notebooks/lfqa/lfqa_stats.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcccxl013.pok.ibm.com/dccstor/srosent2/primeqa/primeqa/notebooks/lfqa/lfqa_stats.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39mprint\u001b[39m(data_file)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bcccxl013.pok.ibm.com/dccstor/srosent2/primeqa/primeqa/notebooks/lfqa/lfqa_stats.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     all_stats[data_file[\u001b[39mlen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m/dccstor/srosent2/generative/external_datasets/\u001b[39m\u001b[39m\"\u001b[39m):\u001b[39m-\u001b[39m\u001b[39m6\u001b[39m]] \u001b[39m=\u001b[39m compute_stats(load_data(data_file))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcccxl013.pok.ibm.com/dccstor/srosent2/primeqa/primeqa/notebooks/lfqa/lfqa_stats.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mQueries,A per Q,WORDS in Q,WORDS in A,SENTENCES in A, UNANSWERABLE\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcccxl013.pok.ibm.com/dccstor/srosent2/primeqa/primeqa/notebooks/lfqa/lfqa_stats.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mfor\u001b[39;00m stat \u001b[39min\u001b[39;00m all_stats:\n",
      "\u001b[1;32m/dccstor/srosent2/primeqa/primeqa/notebooks/lfqa/lfqa_stats.ipynb Cell 14\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcccxl013.pok.ibm.com/dccstor/srosent2/primeqa/primeqa/notebooks/lfqa/lfqa_stats.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=67'>68</a>\u001b[0m     stats[\u001b[39m'\u001b[39m\u001b[39munanswerable\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcccxl013.pok.ibm.com/dccstor/srosent2/primeqa/primeqa/notebooks/lfqa/lfqa_stats.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=68'>69</a>\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bcccxl013.pok.ibm.com/dccstor/srosent2/primeqa/primeqa/notebooks/lfqa/lfqa_stats.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=69'>70</a>\u001b[0m token_count, sentence_count \u001b[39m=\u001b[39m nlp_count(answer[\u001b[39m'\u001b[39;49m\u001b[39manswer\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcccxl013.pok.ibm.com/dccstor/srosent2/primeqa/primeqa/notebooks/lfqa/lfqa_stats.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=70'>71</a>\u001b[0m stats[\u001b[39m'\u001b[39m\u001b[39ma_words\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m token_count\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcccxl013.pok.ibm.com/dccstor/srosent2/primeqa/primeqa/notebooks/lfqa/lfqa_stats.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=71'>72</a>\u001b[0m stats[\u001b[39m'\u001b[39m\u001b[39ma_sentences\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m sentence_count\n",
      "\u001b[1;32m/dccstor/srosent2/primeqa/primeqa/notebooks/lfqa/lfqa_stats.ipynb Cell 14\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcccxl013.pok.ibm.com/dccstor/srosent2/primeqa/primeqa/notebooks/lfqa/lfqa_stats.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnlp_count\u001b[39m(text):\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bcccxl013.pok.ibm.com/dccstor/srosent2/primeqa/primeqa/notebooks/lfqa/lfqa_stats.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     tokens \u001b[39m=\u001b[39m [[token\u001b[39m.\u001b[39mtext \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m sent] \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m nlp(text, disable\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mparser\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mtagger\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mner\u001b[39;49m\u001b[39m'\u001b[39;49m])\u001b[39m.\u001b[39msents]\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcccxl013.pok.ibm.com/dccstor/srosent2/primeqa/primeqa/notebooks/lfqa/lfqa_stats.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     token_count \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcccxl013.pok.ibm.com/dccstor/srosent2/primeqa/primeqa/notebooks/lfqa/lfqa_stats.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m tokens:\n",
      "File \u001b[0;32m/dccstor/srosent1/miniconda3/envs/primeqa_new/lib/python3.9/site-packages/spacy/language.py:1011\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m   1009\u001b[0m     error_handler \u001b[39m=\u001b[39m proc\u001b[39m.\u001b[39mget_error_handler()\n\u001b[1;32m   1010\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1011\u001b[0m     doc \u001b[39m=\u001b[39m proc(doc, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcomponent_cfg\u001b[39m.\u001b[39;49mget(name, {}))  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m   1012\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1013\u001b[0m     \u001b[39m# This typically happens if a component is not initialized\u001b[39;00m\n\u001b[1;32m   1014\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE109\u001b[39m.\u001b[39mformat(name\u001b[39m=\u001b[39mname)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m/dccstor/srosent1/miniconda3/envs/primeqa_new/lib/python3.9/site-packages/spacy/pipeline/trainable_pipe.pyx:52\u001b[0m, in \u001b[0;36mspacy.pipeline.trainable_pipe.TrainablePipe.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/dccstor/srosent1/miniconda3/envs/primeqa_new/lib/python3.9/site-packages/spacy/pipeline/tok2vec.py:125\u001b[0m, in \u001b[0;36mTok2Vec.predict\u001b[0;34m(self, docs)\u001b[0m\n\u001b[1;32m    123\u001b[0m     width \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mget_dim(\u001b[39m\"\u001b[39m\u001b[39mnO\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    124\u001b[0m     \u001b[39mreturn\u001b[39;00m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mops\u001b[39m.\u001b[39malloc((\u001b[39m0\u001b[39m, width)) \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m docs]\n\u001b[0;32m--> 125\u001b[0m tokvecs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mpredict(docs)\n\u001b[1;32m    126\u001b[0m \u001b[39mreturn\u001b[39;00m tokvecs\n",
      "File \u001b[0;32m/dccstor/srosent1/miniconda3/envs/primeqa_new/lib/python3.9/site-packages/thinc/model.py:315\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, X: InT) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m OutT:\n\u001b[1;32m    312\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Call the model's `forward` function with `is_train=False`, and return\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[39m    only the output, instead of the `(output, callback)` tuple.\u001b[39;00m\n\u001b[1;32m    314\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 315\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m/dccstor/srosent1/miniconda3/envs/primeqa_new/lib/python3.9/site-packages/thinc/layers/chain.py:55\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     53\u001b[0m callbacks \u001b[39m=\u001b[39m []\n\u001b[1;32m     54\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m---> 55\u001b[0m     Y, inc_layer_grad \u001b[39m=\u001b[39m layer(X, is_train\u001b[39m=\u001b[39;49mis_train)\n\u001b[1;32m     56\u001b[0m     callbacks\u001b[39m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     57\u001b[0m     X \u001b[39m=\u001b[39m Y\n",
      "File \u001b[0;32m/dccstor/srosent1/miniconda3/envs/primeqa_new/lib/python3.9/site-packages/thinc/model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    289\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[39m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49mis_train)\n",
      "File \u001b[0;32m/dccstor/srosent1/miniconda3/envs/primeqa_new/lib/python3.9/site-packages/thinc/layers/with_array.py:38\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, Xseq, is_train)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[39mreturn\u001b[39;00m model\u001b[39m.\u001b[39mlayers[\u001b[39m0\u001b[39m](Xseq, is_train)\n\u001b[1;32m     37\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 38\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(Tuple[SeqT, Callable], _list_forward(model, Xseq, is_train))\n",
      "File \u001b[0;32m/dccstor/srosent1/miniconda3/envs/primeqa_new/lib/python3.9/site-packages/thinc/layers/with_array.py:73\u001b[0m, in \u001b[0;36m_list_forward\u001b[0;34m(model, Xs, is_train)\u001b[0m\n\u001b[1;32m     71\u001b[0m lengths \u001b[39m=\u001b[39m layer\u001b[39m.\u001b[39mops\u001b[39m.\u001b[39masarray1i([\u001b[39mlen\u001b[39m(seq) \u001b[39mfor\u001b[39;00m seq \u001b[39min\u001b[39;00m Xs])\n\u001b[1;32m     72\u001b[0m Xf \u001b[39m=\u001b[39m layer\u001b[39m.\u001b[39mops\u001b[39m.\u001b[39mflatten(Xs, pad\u001b[39m=\u001b[39mpad)\n\u001b[0;32m---> 73\u001b[0m Yf, get_dXf \u001b[39m=\u001b[39m layer(Xf, is_train)\n\u001b[1;32m     75\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbackprop\u001b[39m(dYs: ListXd) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ListXd:\n\u001b[1;32m     76\u001b[0m     dYf \u001b[39m=\u001b[39m layer\u001b[39m.\u001b[39mops\u001b[39m.\u001b[39mflatten(dYs, pad\u001b[39m=\u001b[39mpad)\n",
      "File \u001b[0;32m/dccstor/srosent1/miniconda3/envs/primeqa_new/lib/python3.9/site-packages/thinc/model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    289\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[39m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49mis_train)\n",
      "File \u001b[0;32m/dccstor/srosent1/miniconda3/envs/primeqa_new/lib/python3.9/site-packages/thinc/layers/chain.py:55\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     53\u001b[0m callbacks \u001b[39m=\u001b[39m []\n\u001b[1;32m     54\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m---> 55\u001b[0m     Y, inc_layer_grad \u001b[39m=\u001b[39m layer(X, is_train\u001b[39m=\u001b[39;49mis_train)\n\u001b[1;32m     56\u001b[0m     callbacks\u001b[39m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     57\u001b[0m     X \u001b[39m=\u001b[39m Y\n",
      "File \u001b[0;32m/dccstor/srosent1/miniconda3/envs/primeqa_new/lib/python3.9/site-packages/thinc/model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    289\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[39m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49mis_train)\n",
      "File \u001b[0;32m/dccstor/srosent1/miniconda3/envs/primeqa_new/lib/python3.9/site-packages/thinc/layers/residual.py:41\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m         \u001b[39mreturn\u001b[39;00m d_output \u001b[39m+\u001b[39m dX\n\u001b[0;32m---> 41\u001b[0m Y, backprop_layer \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mlayers[\u001b[39m0\u001b[39;49m](X, is_train)\n\u001b[1;32m     42\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(X, \u001b[39mlist\u001b[39m):\n\u001b[1;32m     43\u001b[0m     \u001b[39mreturn\u001b[39;00m [X[i] \u001b[39m+\u001b[39m Y[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(X))], backprop\n",
      "File \u001b[0;32m/dccstor/srosent1/miniconda3/envs/primeqa_new/lib/python3.9/site-packages/thinc/model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    289\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[39m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49mis_train)\n",
      "File \u001b[0;32m/dccstor/srosent1/miniconda3/envs/primeqa_new/lib/python3.9/site-packages/thinc/layers/chain.py:55\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     53\u001b[0m callbacks \u001b[39m=\u001b[39m []\n\u001b[1;32m     54\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m---> 55\u001b[0m     Y, inc_layer_grad \u001b[39m=\u001b[39m layer(X, is_train\u001b[39m=\u001b[39;49mis_train)\n\u001b[1;32m     56\u001b[0m     callbacks\u001b[39m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     57\u001b[0m     X \u001b[39m=\u001b[39m Y\n",
      "File \u001b[0;32m/dccstor/srosent1/miniconda3/envs/primeqa_new/lib/python3.9/site-packages/thinc/model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    289\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[39m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49mis_train)\n",
      "File \u001b[0;32m/dccstor/srosent1/miniconda3/envs/primeqa_new/lib/python3.9/site-packages/thinc/layers/chain.py:55\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     53\u001b[0m callbacks \u001b[39m=\u001b[39m []\n\u001b[1;32m     54\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m---> 55\u001b[0m     Y, inc_layer_grad \u001b[39m=\u001b[39m layer(X, is_train\u001b[39m=\u001b[39;49mis_train)\n\u001b[1;32m     56\u001b[0m     callbacks\u001b[39m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     57\u001b[0m     X \u001b[39m=\u001b[39m Y\n",
      "    \u001b[0;31m[... skipping similar frames: Model.__call__ at line 291 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m/dccstor/srosent1/miniconda3/envs/primeqa_new/lib/python3.9/site-packages/thinc/layers/chain.py:55\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     53\u001b[0m callbacks \u001b[39m=\u001b[39m []\n\u001b[1;32m     54\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m---> 55\u001b[0m     Y, inc_layer_grad \u001b[39m=\u001b[39m layer(X, is_train\u001b[39m=\u001b[39;49mis_train)\n\u001b[1;32m     56\u001b[0m     callbacks\u001b[39m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     57\u001b[0m     X \u001b[39m=\u001b[39m Y\n",
      "File \u001b[0;32m/dccstor/srosent1/miniconda3/envs/primeqa_new/lib/python3.9/site-packages/thinc/model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    289\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[39m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49mis_train)\n",
      "File \u001b[0;32m/dccstor/srosent1/miniconda3/envs/primeqa_new/lib/python3.9/site-packages/thinc/layers/maxout.py:56\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     54\u001b[0m Y \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mops\u001b[39m.\u001b[39mreshape1f(b, nO \u001b[39m*\u001b[39m nP)\n\u001b[1;32m     55\u001b[0m Z \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mops\u001b[39m.\u001b[39mreshape3f(Y, Y\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], nO, nP)\n\u001b[0;32m---> 56\u001b[0m best, which \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mops\u001b[39m.\u001b[39;49mmaxout(Z)\n\u001b[1;32m     58\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbackprop\u001b[39m(d_best: OutT) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m InT:\n\u001b[1;32m     59\u001b[0m     dZ \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mops\u001b[39m.\u001b[39mbackprop_maxout(d_best, which, nP)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "data_dir = \"/dccstor/srosent2/generative/external_datasets/**/*.jsonl\"\n",
    "\n",
    "all_data_files = glob.glob(data_dir, recursive=True)\n",
    "all_data_files.extend(glob.glob(\"/dccstor/srosent2/generative/appen/final/longNQ/*/*.jsonl\"))\n",
    "all_data_files.extend(glob.glob(\"/dccstor/srosent2/primeqa-mengxia/data/asqa/formatted/ASQA_*.json\"))\n",
    "all_data_files.extend(glob.glob(\"/dccstor/srosent2/primeqa-mengxia/data/dpr-100passages_withkg_best_all/eli5-*\"))\n",
    "all_stats = {}\n",
    "\n",
    "for data_file in all_data_files:\n",
    "    if \"original\" in data_file:\n",
    "        continue\n",
    "    print(data_file)\n",
    "    all_stats[data_file[len(\"/dccstor/srosent2/generative/external_datasets/\"):-6]] = compute_stats(load_data(data_file))\n",
    "\n",
    "print(\"Queries,A per Q,WORDS in Q,WORDS in A,SENTENCES in A, UNANSWERABLE\")\n",
    "for stat in all_stats:\n",
    "    s=[str(i) for i in all_stats[stat]]\n",
    "    stats_as_string = ','.join(s)\n",
    "    print(f\"{stat},{stats_as_string}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('primeqaenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "229bd894a0cdb05b7ee80ea2bc43559a301775857073a25e64e4f441f37822ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
