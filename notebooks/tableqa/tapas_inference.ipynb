{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table Question Generation using TAPAS: Inference and Evaluation examples\n",
    "\n",
    "In this notebook, we will show how to use ReaderComponent class to instantiate Tapas and use it to predict(), eval() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "\n",
    "If not already done, make sure to install PrimeQA with notebooks extras before getting started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from primeqa.components.reader_component import Reader\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = ReaderComponent('TapasModel',\"../../primeqa/tableqa/tapas/tapas_config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Actors Number of movies\n",
      "0           Brad Pitt               87\n",
      "1  Leonardo Di Caprio               53\n",
      "2      George Clooney               69\n",
      "in predict for TapasModel with data:  {'Actors': ['Brad Pitt', 'Leonardo Di Caprio', 'George Clooney'], 'Number of movies': ['87', '53', '69']}  ,queries: ['how many movies Brad Pitt acted in', 'Name the actor who has been in 53 movies']\n",
      "loading from config at  ../../primeqa/tableqa/tapas/tapas_config.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a37e86feaf04cf09e52f686f71dd8e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.56k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8007373499e54b30b07297cea51f1fe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/443M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e6b7ae3024d4928be49f25123aa65e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6e1ae2b70804f2fb640cbb38246ff91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/154 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3d4049309c7462d9c1a2923e0a54aa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/826 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answers {'how many movies Brad Pitt acted in': '87', 'Name the actor who has been in 53 movies': 'Leonardo Di Caprio'}\n"
     ]
    }
   ],
   "source": [
    "# Load the Table \n",
    "data = {\"Actors\": [\"Brad Pitt\", \"Leonardo Di Caprio\",\n",
    "                        \"George Clooney\"], \"Number of movies\": [\"87\", \"53\", \"69\"]}\n",
    "print(pd.DataFrame.from_dict(data))\n",
    "\n",
    "queries = [\"how many movies Brad Pitt acted in\", \"Name the actor who has been in 53 movies\"]\n",
    "answers = reader.predict(data,queries)\n",
    "print(\"answers\" , answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading from config at  ../../primeqa/tableqa/tapas/tapas_config.json\n",
      "Preprocessing wikisql dataset\n",
      "{\"time\":\"2022-12-06 06:06:21,956\", \"name\": \"nlp.utils.file_utils\", \"level\": \"INFO\", \"message\": \"https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/wikisql/wikisql.py not found in cache or force_download set to True, downloading to /u/jaydesen/.cache/huggingface/datasets/tmp2doc62og\"}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5faf1412acf9426298b91216b5cf13bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/6.48k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"time\":\"2022-12-06 06:06:22,117\", \"name\": \"nlp.utils.file_utils\", \"level\": \"INFO\", \"message\": \"storing https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/wikisql/wikisql.py in cache at /u/jaydesen/.cache/huggingface/datasets/e6305d5981c5107101189ff917073a7286c4e71db220340fec67e0f69bb6bbdd.e1797f2c1ffe28c0d770867818c0e7df49f220a07f4feb608ec8939227b2dbdc.py\"}\n",
      "{\"time\":\"2022-12-06 06:06:22,135\", \"name\": \"nlp.utils.file_utils\", \"level\": \"INFO\", \"message\": \"creating metadata file for /u/jaydesen/.cache/huggingface/datasets/e6305d5981c5107101189ff917073a7286c4e71db220340fec67e0f69bb6bbdd.e1797f2c1ffe28c0d770867818c0e7df49f220a07f4feb608ec8939227b2dbdc.py\"}\n",
      "{\"time\":\"2022-12-06 06:06:22,360\", \"name\": \"nlp.utils.file_utils\", \"level\": \"INFO\", \"message\": \"https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/wikisql/dataset_infos.json not found in cache or force_download set to True, downloading to /u/jaydesen/.cache/huggingface/datasets/tmpvt2wpn09\"}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c00bc6afabe4b9cac1ac1dc22aa29fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.76k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"time\":\"2022-12-06 06:06:22,543\", \"name\": \"nlp.utils.file_utils\", \"level\": \"INFO\", \"message\": \"storing https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/wikisql/dataset_infos.json in cache at /u/jaydesen/.cache/huggingface/datasets/72451b7b45a71196a9b3b239e442b8d029822cf3c5acd3058c9187ba5ba909a7.5f144af9b449b2cabe9f64a832ab89de15c68068ba810280409be92bc03b5652\"}\n",
      "{\"time\":\"2022-12-06 06:06:22,560\", \"name\": \"nlp.utils.file_utils\", \"level\": \"INFO\", \"message\": \"creating metadata file for /u/jaydesen/.cache/huggingface/datasets/72451b7b45a71196a9b3b239e442b8d029822cf3c5acd3058c9187ba5ba909a7.5f144af9b449b2cabe9f64a832ab89de15c68068ba810280409be92bc03b5652\"}\n",
      "{\"time\":\"2022-12-06 06:06:22,648\", \"name\": \"nlp.load\", \"level\": \"INFO\", \"message\": \"Checking /u/jaydesen/.cache/huggingface/datasets/e6305d5981c5107101189ff917073a7286c4e71db220340fec67e0f69bb6bbdd.e1797f2c1ffe28c0d770867818c0e7df49f220a07f4feb608ec8939227b2dbdc.py for additional imports.\"}\n",
      "{\"time\":\"2022-12-06 06:06:22,690\", \"name\": \"nlp.load\", \"level\": \"INFO\", \"message\": \"Found main folder for dataset https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/wikisql/wikisql.py at /dccstor/irl-tableqa/jaydeep/tu4.24.0/lib/python3.7/site-packages/nlp/datasets/wikisql\"}\n",
      "{\"time\":\"2022-12-06 06:06:22,708\", \"name\": \"nlp.load\", \"level\": \"INFO\", \"message\": \"Found specific version folder for dataset https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/wikisql/wikisql.py at /dccstor/irl-tableqa/jaydeep/tu4.24.0/lib/python3.7/site-packages/nlp/datasets/wikisql/6c75e26bb08db6bca2dbc201e89b574f053fc25492ef8bbf2614463c14436618\"}\n",
      "{\"time\":\"2022-12-06 06:06:22,732\", \"name\": \"nlp.load\", \"level\": \"INFO\", \"message\": \"Found script file from https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/wikisql/wikisql.py to /dccstor/irl-tableqa/jaydeep/tu4.24.0/lib/python3.7/site-packages/nlp/datasets/wikisql/6c75e26bb08db6bca2dbc201e89b574f053fc25492ef8bbf2614463c14436618/wikisql.py\"}\n",
      "{\"time\":\"2022-12-06 06:06:22,765\", \"name\": \"nlp.load\", \"level\": \"INFO\", \"message\": \"Found dataset infos file from https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/wikisql/dataset_infos.json to /dccstor/irl-tableqa/jaydeep/tu4.24.0/lib/python3.7/site-packages/nlp/datasets/wikisql/6c75e26bb08db6bca2dbc201e89b574f053fc25492ef8bbf2614463c14436618/dataset_infos.json\"}\n",
      "{\"time\":\"2022-12-06 06:06:22,776\", \"name\": \"nlp.load\", \"level\": \"INFO\", \"message\": \"Found metadata file for dataset https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/wikisql/wikisql.py at /dccstor/irl-tableqa/jaydeep/tu4.24.0/lib/python3.7/site-packages/nlp/datasets/wikisql/6c75e26bb08db6bca2dbc201e89b574f053fc25492ef8bbf2614463c14436618/wikisql.json\"}\n",
      "{\"time\":\"2022-12-06 06:06:22,867\", \"name\": \"nlp.builder\", \"level\": \"WARNING\", \"message\": \"Using custom data configuration default\"}\n",
      "{\"time\":\"2022-12-06 06:06:22,868\", \"name\": \"nlp.info\", \"level\": \"INFO\", \"message\": \"Loading Dataset Infos from /dccstor/irl-tableqa/jaydeep/tu4.24.0/lib/python3.7/site-packages/nlp/datasets/wikisql/6c75e26bb08db6bca2dbc201e89b574f053fc25492ef8bbf2614463c14436618\"}\n",
      "{\"time\":\"2022-12-06 06:06:22,895\", \"name\": \"nlp.builder\", \"level\": \"INFO\", \"message\": \"Generating dataset wiki_sql (/u/jaydesen/.cache/huggingface/datasets/wiki_sql/default/0.1.0/6c75e26bb08db6bca2dbc201e89b574f053fc25492ef8bbf2614463c14436618)\"}\n",
      "Downloading and preparing dataset wiki_sql/default (download: 24.95 MiB, generated: 147.57 MiB, post-processed: Unknown sizetotal: 172.52 MiB) to /u/jaydesen/.cache/huggingface/datasets/wiki_sql/default/0.1.0/6c75e26bb08db6bca2dbc201e89b574f053fc25492ef8bbf2614463c14436618...\n",
      "{\"time\":\"2022-12-06 06:06:23,104\", \"name\": \"nlp.builder\", \"level\": \"INFO\", \"message\": \"Dataset not on Hf google storage. Downloading and preparing it from source\"}\n",
      "{\"time\":\"2022-12-06 06:06:23,663\", \"name\": \"nlp.utils.file_utils\", \"level\": \"INFO\", \"message\": \"https://github.com/salesforce/WikiSQL/raw/master/data.tar.bz2 not found in cache or force_download set to True, downloading to /u/jaydesen/.cache/huggingface/datasets/downloads/tmpp3m5afsc\"}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f10b44ee2e42433db51db93c51f46dab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/26.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"time\":\"2022-12-06 06:06:24,572\", \"name\": \"nlp.utils.file_utils\", \"level\": \"INFO\", \"message\": \"storing https://github.com/salesforce/WikiSQL/raw/master/data.tar.bz2 in cache at /u/jaydesen/.cache/huggingface/datasets/downloads/9ae4ed9eb5d4ba6eeac9f73869e0ffa6388dd495b3c30a7eabc1be501560a537.aeb629a1be711b831af28a54590a2923088b36476da553ab5da3bd54f0ffb6fb\"}\n",
      "{\"time\":\"2022-12-06 06:06:24,592\", \"name\": \"nlp.utils.file_utils\", \"level\": \"INFO\", \"message\": \"creating metadata file for /u/jaydesen/.cache/huggingface/datasets/downloads/9ae4ed9eb5d4ba6eeac9f73869e0ffa6388dd495b3c30a7eabc1be501560a537.aeb629a1be711b831af28a54590a2923088b36476da553ab5da3bd54f0ffb6fb\"}\n",
      "{\"time\":\"2022-12-06 06:06:29,901\", \"name\": \"nlp.utils.info_utils\", \"level\": \"INFO\", \"message\": \"All the checksums matched successfully for dataset source files\"}\n",
      "{\"time\":\"2022-12-06 06:06:29,902\", \"name\": \"nlp.builder\", \"level\": \"INFO\", \"message\": \"Generating split test\"}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "301c30a7a77e49888a46a7d75e89bb68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"time\":\"2022-12-06 06:06:40,275\", \"name\": \"nlp.arrow_writer\", \"level\": \"INFO\", \"message\": \"Done writing 15878 examples in 32234609 bytes /u/jaydesen/.cache/huggingface/datasets/wiki_sql/default/0.1.0/6c75e26bb08db6bca2dbc201e89b574f053fc25492ef8bbf2614463c14436618.incomplete/wiki_sql-test.arrow.\"}\n",
      "{\"time\":\"2022-12-06 06:06:40,277\", \"name\": \"nlp.builder\", \"level\": \"INFO\", \"message\": \"Generating split validation\"}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a88a2a8d4a2141e2b433d8297e3e18a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"time\":\"2022-12-06 06:06:45,229\", \"name\": \"nlp.arrow_writer\", \"level\": \"INFO\", \"message\": \"Done writing 8421 examples in 15159238 bytes /u/jaydesen/.cache/huggingface/datasets/wiki_sql/default/0.1.0/6c75e26bb08db6bca2dbc201e89b574f053fc25492ef8bbf2614463c14436618.incomplete/wiki_sql-validation.arrow.\"}\n",
      "{\"time\":\"2022-12-06 06:06:45,231\", \"name\": \"nlp.builder\", \"level\": \"INFO\", \"message\": \"Generating split train\"}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24b120ef71bc4204a2fb4ef18c26f8f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"time\":\"2022-12-06 06:07:20,214\", \"name\": \"nlp.arrow_writer\", \"level\": \"INFO\", \"message\": \"Done writing 56355 examples in 107345461 bytes /u/jaydesen/.cache/huggingface/datasets/wiki_sql/default/0.1.0/6c75e26bb08db6bca2dbc201e89b574f053fc25492ef8bbf2614463c14436618.incomplete/wiki_sql-train.arrow.\"}\n",
      "{\"time\":\"2022-12-06 06:07:20,216\", \"name\": \"nlp.utils.info_utils\", \"level\": \"INFO\", \"message\": \"All the splits matched successfully.\"}\n",
      "Dataset wiki_sql downloaded and prepared to /u/jaydesen/.cache/huggingface/datasets/wiki_sql/default/0.1.0/6c75e26bb08db6bca2dbc201e89b574f053fc25492ef8bbf2614463c14436618. Subsequent calls will reuse this data.\n",
      "{\"time\":\"2022-12-06 06:07:20,301\", \"name\": \"nlp.builder\", \"level\": \"INFO\", \"message\": \"Constructing Dataset for split validation, from /u/jaydesen/.cache/huggingface/datasets/wiki_sql/default/0.1.0/6c75e26bb08db6bca2dbc201e89b574f053fc25492ef8bbf2614463c14436618\"}\n",
      "{\"time\":\"2022-12-06 06:07:20,317\", \"name\": \"nlp.utils.info_utils\", \"level\": \"INFO\", \"message\": \"Unable to verify checksums.\"}\n",
      "{\"time\":\"2022-12-06 06:07:20,884\", \"name\": \"nlp.load\", \"level\": \"INFO\", \"message\": \"Checking /u/jaydesen/.cache/huggingface/datasets/e6305d5981c5107101189ff917073a7286c4e71db220340fec67e0f69bb6bbdd.e1797f2c1ffe28c0d770867818c0e7df49f220a07f4feb608ec8939227b2dbdc.py for additional imports.\"}\n",
      "{\"time\":\"2022-12-06 06:07:20,922\", \"name\": \"nlp.load\", \"level\": \"INFO\", \"message\": \"Found main folder for dataset https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/wikisql/wikisql.py at /dccstor/irl-tableqa/jaydeep/tu4.24.0/lib/python3.7/site-packages/nlp/datasets/wikisql\"}\n",
      "{\"time\":\"2022-12-06 06:07:20,945\", \"name\": \"nlp.load\", \"level\": \"INFO\", \"message\": \"Found specific version folder for dataset https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/wikisql/wikisql.py at /dccstor/irl-tableqa/jaydeep/tu4.24.0/lib/python3.7/site-packages/nlp/datasets/wikisql/6c75e26bb08db6bca2dbc201e89b574f053fc25492ef8bbf2614463c14436618\"}\n",
      "{\"time\":\"2022-12-06 06:07:20,978\", \"name\": \"nlp.load\", \"level\": \"INFO\", \"message\": \"Found script file from https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/wikisql/wikisql.py to /dccstor/irl-tableqa/jaydeep/tu4.24.0/lib/python3.7/site-packages/nlp/datasets/wikisql/6c75e26bb08db6bca2dbc201e89b574f053fc25492ef8bbf2614463c14436618/wikisql.py\"}\n",
      "{\"time\":\"2022-12-06 06:07:21,003\", \"name\": \"nlp.load\", \"level\": \"INFO\", \"message\": \"Found dataset infos file from https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/wikisql/dataset_infos.json to /dccstor/irl-tableqa/jaydeep/tu4.24.0/lib/python3.7/site-packages/nlp/datasets/wikisql/6c75e26bb08db6bca2dbc201e89b574f053fc25492ef8bbf2614463c14436618/dataset_infos.json\"}\n",
      "{\"time\":\"2022-12-06 06:07:21,011\", \"name\": \"nlp.load\", \"level\": \"INFO\", \"message\": \"Found metadata file for dataset https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/wikisql/wikisql.py at /dccstor/irl-tableqa/jaydeep/tu4.24.0/lib/python3.7/site-packages/nlp/datasets/wikisql/6c75e26bb08db6bca2dbc201e89b574f053fc25492ef8bbf2614463c14436618/wikisql.json\"}\n",
      "{\"time\":\"2022-12-06 06:07:21,014\", \"name\": \"nlp.builder\", \"level\": \"WARNING\", \"message\": \"Using custom data configuration default\"}\n",
      "{\"time\":\"2022-12-06 06:07:21,015\", \"name\": \"nlp.info\", \"level\": \"INFO\", \"message\": \"Loading Dataset Infos from /dccstor/irl-tableqa/jaydeep/tu4.24.0/lib/python3.7/site-packages/nlp/datasets/wikisql/6c75e26bb08db6bca2dbc201e89b574f053fc25492ef8bbf2614463c14436618\"}\n",
      "{\"time\":\"2022-12-06 06:07:21,040\", \"name\": \"nlp.builder\", \"level\": \"INFO\", \"message\": \"Overwrite dataset info from restored data version.\"}\n",
      "{\"time\":\"2022-12-06 06:07:21,041\", \"name\": \"nlp.info\", \"level\": \"INFO\", \"message\": \"Loading Dataset info from /u/jaydesen/.cache/huggingface/datasets/wiki_sql/default/0.1.0/6c75e26bb08db6bca2dbc201e89b574f053fc25492ef8bbf2614463c14436618\"}\n",
      "{\"time\":\"2022-12-06 06:07:21,059\", \"name\": \"nlp.builder\", \"level\": \"INFO\", \"message\": \"Reusing dataset wiki_sql (/u/jaydesen/.cache/huggingface/datasets/wiki_sql/default/0.1.0/6c75e26bb08db6bca2dbc201e89b574f053fc25492ef8bbf2614463c14436618)\"}\n",
      "{\"time\":\"2022-12-06 06:07:21,061\", \"name\": \"nlp.builder\", \"level\": \"INFO\", \"message\": \"Constructing Dataset for split train, from /u/jaydesen/.cache/huggingface/datasets/wiki_sql/default/0.1.0/6c75e26bb08db6bca2dbc201e89b574f053fc25492ef8bbf2614463c14436618\"}\n",
      "{\"time\":\"2022-12-06 06:07:21,083\", \"name\": \"nlp.utils.info_utils\", \"level\": \"INFO\", \"message\": \"Unable to verify checksums.\"}\n",
      "Preprocessing done\n",
      "Preprocessing done\n",
      "{\"time\":\"2022-12-06 06:13:57,961\", \"name\": \"primeqa.tableqa.tapas.models.tapas_model\", \"level\": \"INFO\", \"message\": \"*** Evaluate ***\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2473\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='310' max='310' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [310/310 33:42]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  eval_Denotation accuracy = 0.8658\n"
     ]
    }
   ],
   "source": [
    "eval_result= reader.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "462c2d4a549f7e8ad9e3a5a27df1b3303915605cb9c357c0b44397d41ee24173"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
