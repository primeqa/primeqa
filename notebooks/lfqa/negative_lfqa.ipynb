{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "longNQ = pd.read_json(\"/dccstor/srosent2/generative/appen/NQ_formatted_answered_multiple-9.11.23.json\", orient='records', lines=True, dtype={\"id\":str})\n",
    "longNQ.sample(5)\n",
    "longNQTrain = pd.read_json(\"/dccstor/srosent2/generative/appen/NQ_formatted_answered_multiple-9.11.23_train.json\", orient='records', lines=True, dtype={\"id\":str})\n",
    "longNQDev = pd.read_json(\"/dccstor/srosent2/generative/appen/NQ_formatted_answered_multiple-9.11.23_dev.json\", orient='records', lines=True, dtype={\"id\":str})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2210\n",
      "2379\n",
      "169\n",
      "what          642\n",
      "where         498\n",
      "who           384\n",
      "when          243\n",
      "how           174\n",
      "why            98\n",
      "the            46\n",
      "which          19\n",
      "what's         17\n",
      "is             15\n",
      "difference     13\n",
      "in             10\n",
      "describe        9\n",
      "meaning         8\n",
      "summary         8\n",
      "can             7\n",
      "do              6\n",
      "a               5\n",
      "explain         4\n",
      "does            4\n",
      "OTHER         169\n",
      "Name: input, dtype: int64\n",
      "2379\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "longNQ_first_word  = longNQ['input'].str.split().str.get(0)\n",
    "long_NQ_first_word_counts = longNQ_first_word.value_counts()[0:20] #/len(longNQ)\n",
    "print(long_NQ_first_word_counts.sum())\n",
    "long_NQ_first_word_counts['OTHER'] = 2379-long_NQ_first_word_counts.sum()\n",
    "print(long_NQ_first_word_counts.sum())\n",
    "print(long_NQ_first_word_counts['OTHER'])\n",
    "word_dist = []\n",
    "\n",
    "for word in long_NQ_first_word_counts.keys():\n",
    "    occurrence = long_NQ_first_word_counts[word]\n",
    "    word_dist.extend([word]*occurrence)\n",
    "\n",
    "print(long_NQ_first_word_counts)\n",
    "print(len(word_dist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'what', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'where', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'who', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'when', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'how', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'why', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'the', 'which', 'which', 'which', 'which', 'which', 'which', 'which', 'which', 'which', 'which', 'which', 'which', 'which', 'which', 'which', 'which', 'which', 'which', 'which', \"what's\", \"what's\", \"what's\", \"what's\", \"what's\", \"what's\", \"what's\", \"what's\", \"what's\", \"what's\", \"what's\", \"what's\", \"what's\", \"what's\", \"what's\", \"what's\", \"what's\", 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'difference', 'difference', 'difference', 'difference', 'difference', 'difference', 'difference', 'difference', 'difference', 'difference', 'difference', 'difference', 'difference', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'in', 'describe', 'describe', 'describe', 'describe', 'describe', 'describe', 'describe', 'describe', 'describe', 'meaning', 'meaning', 'meaning', 'meaning', 'meaning', 'meaning', 'meaning', 'meaning', 'summary', 'summary', 'summary', 'summary', 'summary', 'summary', 'summary', 'summary', 'can', 'can', 'can', 'can', 'can', 'can', 'can', 'do', 'do', 'do', 'do', 'do', 'do', 'a', 'a', 'a', 'a', 'a', 'explain', 'explain', 'explain', 'explain', 'does', 'does', 'does', 'does', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER', 'OTHER']\n",
      "659\n",
      "where\n"
     ]
    }
   ],
   "source": [
    "from random import *\n",
    "\n",
    "print(word_dist)\n",
    "random_num = randint(0,len(word_dist))\n",
    "print(random_num)\n",
    "print(word_dist[random_num])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_plaintext</th>\n",
       "      <th>document_title</th>\n",
       "      <th>document_url</th>\n",
       "      <th>example_id</th>\n",
       "      <th>language</th>\n",
       "      <th>question_text</th>\n",
       "      <th>passage_answer_candidates</th>\n",
       "      <th>annotations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Over the Rainbow - wikipedia Over the Rainbow ...</td>\n",
       "      <td>Over the Rainbow</td>\n",
       "      <td>https://en.wikipedia.org//w/index.php?title=Ov...</td>\n",
       "      <td>-5436994632006702327</td>\n",
       "      <td>english</td>\n",
       "      <td>who sings new version of somewhere over the ra...</td>\n",
       "      <td>[{'plaintext_start_byte': 143, 'plaintext_end_...</td>\n",
       "      <td>[{'annotation_id': '953612135555563607', 'mini...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Software bug - Wikipedia Software bug To repor...</td>\n",
       "      <td>Software bug</td>\n",
       "      <td>https://en.wikipedia.org//w/index.php?title=So...</td>\n",
       "      <td>-8639769156842379553</td>\n",
       "      <td>english</td>\n",
       "      <td>where did the term bug in software come from</td>\n",
       "      <td>[{'plaintext_start_byte': 111, 'plaintext_end_...</td>\n",
       "      <td>[{'annotation_id': '1928398121140643751', 'min...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Garden of Eden - wikipedia Garden of Eden Jump...</td>\n",
       "      <td>Garden of Eden</td>\n",
       "      <td>https://en.wikipedia.org//w/index.php?title=Ga...</td>\n",
       "      <td>4347940021241157002</td>\n",
       "      <td>english</td>\n",
       "      <td>where would the garden of eden be located today</td>\n",
       "      <td>[{'plaintext_start_byte': 357, 'plaintext_end_...</td>\n",
       "      <td>[{'annotation_id': '9862469584246313719', 'min...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Escape pod - wikipedia Escape pod Jump to : na...</td>\n",
       "      <td>Escape pod</td>\n",
       "      <td>https://en.wikipedia.org//w/index.php?title=Es...</td>\n",
       "      <td>-7511794130112342710</td>\n",
       "      <td>english</td>\n",
       "      <td>does air force one really have an escape pod</td>\n",
       "      <td>[{'plaintext_start_byte': 338, 'plaintext_end_...</td>\n",
       "      <td>[{'annotation_id': '16570227792216399264', 'mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Downward Dog ( TV series ) - wikipedia Downwar...</td>\n",
       "      <td>Downward Dog (TV series)</td>\n",
       "      <td>https://en.wikipedia.org//w/index.php?title=Do...</td>\n",
       "      <td>-6852961558292881748</td>\n",
       "      <td>english</td>\n",
       "      <td>who plays the dogs voice in downward dog</td>\n",
       "      <td>[{'plaintext_start_byte': 96, 'plaintext_end_b...</td>\n",
       "      <td>[{'annotation_id': '10113742779883724732', 'mi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  document_plaintext  \\\n",
       "0  Over the Rainbow - wikipedia Over the Rainbow ...   \n",
       "1  Software bug - Wikipedia Software bug To repor...   \n",
       "2  Garden of Eden - wikipedia Garden of Eden Jump...   \n",
       "3  Escape pod - wikipedia Escape pod Jump to : na...   \n",
       "4  Downward Dog ( TV series ) - wikipedia Downwar...   \n",
       "\n",
       "             document_title  \\\n",
       "0          Over the Rainbow   \n",
       "1              Software bug   \n",
       "2            Garden of Eden   \n",
       "3                Escape pod   \n",
       "4  Downward Dog (TV series)   \n",
       "\n",
       "                                        document_url            example_id  \\\n",
       "0  https://en.wikipedia.org//w/index.php?title=Ov...  -5436994632006702327   \n",
       "1  https://en.wikipedia.org//w/index.php?title=So...  -8639769156842379553   \n",
       "2  https://en.wikipedia.org//w/index.php?title=Ga...   4347940021241157002   \n",
       "3  https://en.wikipedia.org//w/index.php?title=Es...  -7511794130112342710   \n",
       "4  https://en.wikipedia.org//w/index.php?title=Do...  -6852961558292881748   \n",
       "\n",
       "  language                                      question_text  \\\n",
       "0  english  who sings new version of somewhere over the ra...   \n",
       "1  english       where did the term bug in software come from   \n",
       "2  english    where would the garden of eden be located today   \n",
       "3  english       does air force one really have an escape pod   \n",
       "4  english           who plays the dogs voice in downward dog   \n",
       "\n",
       "                           passage_answer_candidates  \\\n",
       "0  [{'plaintext_start_byte': 143, 'plaintext_end_...   \n",
       "1  [{'plaintext_start_byte': 111, 'plaintext_end_...   \n",
       "2  [{'plaintext_start_byte': 357, 'plaintext_end_...   \n",
       "3  [{'plaintext_start_byte': 338, 'plaintext_end_...   \n",
       "4  [{'plaintext_start_byte': 96, 'plaintext_end_b...   \n",
       "\n",
       "                                         annotations  \n",
       "0  [{'annotation_id': '953612135555563607', 'mini...  \n",
       "1  [{'annotation_id': '1928398121140643751', 'min...  \n",
       "2  [{'annotation_id': '9862469584246313719', 'min...  \n",
       "3  [{'annotation_id': '16570227792216399264', 'mi...  \n",
       "4  [{'annotation_id': '10113742779883724732', 'mi...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "originalNQfiles = glob.glob(\"/dccstor/srosent2/primeqa/data/train/nq-full/*\")\n",
    "\n",
    "originalNQ = pd.concat([pd.read_json(f,orient='records',lines=True, dtype={\"example_id\":str}) for f in originalNQfiles])\n",
    "\n",
    "originalNQ.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_plaintext</th>\n",
       "      <th>document_title</th>\n",
       "      <th>document_url</th>\n",
       "      <th>example_id</th>\n",
       "      <th>language</th>\n",
       "      <th>question_text</th>\n",
       "      <th>passage_answer_candidates</th>\n",
       "      <th>annotations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Agriculture in the United States - wikipedia A...</td>\n",
       "      <td>Agriculture in the United States</td>\n",
       "      <td>https://en.wikipedia.org//w/index.php?title=Ag...</td>\n",
       "      <td>-6570496346595660652</td>\n",
       "      <td>english</td>\n",
       "      <td>what are the main crops grown in the united st...</td>\n",
       "      <td>[{'plaintext_start_byte': 208, 'plaintext_end_...</td>\n",
       "      <td>[{'annotation_id': '2703952710185985039', 'min...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>List of nominated members of Rajya Sabha - wik...</td>\n",
       "      <td>List of nominated members of Rajya Sabha</td>\n",
       "      <td>https://en.wikipedia.org//w/index.php?title=Li...</td>\n",
       "      <td>4111902318448915849</td>\n",
       "      <td>english</td>\n",
       "      <td>who was the first nominated lady for rajya sabha</td>\n",
       "      <td>[{'plaintext_start_byte': 178, 'plaintext_end_...</td>\n",
       "      <td>[{'annotation_id': '3736471364300862172', 'min...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chow Chow - wikipedia Chow Chow Jump to : navi...</td>\n",
       "      <td>Chow Chow</td>\n",
       "      <td>https://en.wikipedia.org//w/index.php?title=Ch...</td>\n",
       "      <td>-7310327793190149898</td>\n",
       "      <td>english</td>\n",
       "      <td>difference of lion and bear type chow chow</td>\n",
       "      <td>[{'plaintext_start_byte': 155, 'plaintext_end_...</td>\n",
       "      <td>[{'annotation_id': '16611773917906236484', 'mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kick - off ( association football ) - wikipedi...</td>\n",
       "      <td>Kick-off (association football)</td>\n",
       "      <td>https://en.wikipedia.org//w/index.php?title=Ki...</td>\n",
       "      <td>3599421094587589904</td>\n",
       "      <td>english</td>\n",
       "      <td>who kicks the ball first to start a football game</td>\n",
       "      <td>[{'plaintext_start_byte': 169, 'plaintext_end_...</td>\n",
       "      <td>[{'annotation_id': '3982994024022460473', 'min...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Amazing Race 29 - wikipedia The Amazing Ra...</td>\n",
       "      <td>The Amazing Race 29</td>\n",
       "      <td>https://en.wikipedia.org//w/index.php?title=Th...</td>\n",
       "      <td>-4960762045917431768</td>\n",
       "      <td>english</td>\n",
       "      <td>who came in last place on amazing race</td>\n",
       "      <td>[{'plaintext_start_byte': 82, 'plaintext_end_b...</td>\n",
       "      <td>[{'annotation_id': '12053141154264051243', 'mi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  document_plaintext  \\\n",
       "0  Agriculture in the United States - wikipedia A...   \n",
       "1  List of nominated members of Rajya Sabha - wik...   \n",
       "2  Chow Chow - wikipedia Chow Chow Jump to : navi...   \n",
       "3  Kick - off ( association football ) - wikipedi...   \n",
       "4  The Amazing Race 29 - wikipedia The Amazing Ra...   \n",
       "\n",
       "                             document_title  \\\n",
       "0          Agriculture in the United States   \n",
       "1  List of nominated members of Rajya Sabha   \n",
       "2                                 Chow Chow   \n",
       "3           Kick-off (association football)   \n",
       "4                       The Amazing Race 29   \n",
       "\n",
       "                                        document_url            example_id  \\\n",
       "0  https://en.wikipedia.org//w/index.php?title=Ag...  -6570496346595660652   \n",
       "1  https://en.wikipedia.org//w/index.php?title=Li...   4111902318448915849   \n",
       "2  https://en.wikipedia.org//w/index.php?title=Ch...  -7310327793190149898   \n",
       "3  https://en.wikipedia.org//w/index.php?title=Ki...   3599421094587589904   \n",
       "4  https://en.wikipedia.org//w/index.php?title=Th...  -4960762045917431768   \n",
       "\n",
       "  language                                      question_text  \\\n",
       "0  english  what are the main crops grown in the united st...   \n",
       "1  english   who was the first nominated lady for rajya sabha   \n",
       "2  english         difference of lion and bear type chow chow   \n",
       "3  english  who kicks the ball first to start a football game   \n",
       "4  english             who came in last place on amazing race   \n",
       "\n",
       "                           passage_answer_candidates  \\\n",
       "0  [{'plaintext_start_byte': 208, 'plaintext_end_...   \n",
       "1  [{'plaintext_start_byte': 178, 'plaintext_end_...   \n",
       "2  [{'plaintext_start_byte': 155, 'plaintext_end_...   \n",
       "3  [{'plaintext_start_byte': 169, 'plaintext_end_...   \n",
       "4  [{'plaintext_start_byte': 82, 'plaintext_end_b...   \n",
       "\n",
       "                                         annotations  \n",
       "0  [{'annotation_id': '2703952710185985039', 'min...  \n",
       "1  [{'annotation_id': '3736471364300862172', 'min...  \n",
       "2  [{'annotation_id': '16611773917906236484', 'mi...  \n",
       "3  [{'annotation_id': '3982994024022460473', 'min...  \n",
       "4  [{'annotation_id': '12053141154264051243', 'mi...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "originalNQdevfiles = glob.glob(\"/dccstor/srosent2/primeqa/data/dev/nq-full/*\")\n",
    "\n",
    "originalNQdev = pd.concat([pd.read_json(f,orient='records',lines=True, dtype={\"example_id\":str}) for f in originalNQdevfiles])\n",
    "\n",
    "originalNQdev.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    }
   ],
   "source": [
    "# filter to only negatives\n",
    "negativeNQids1word = {}\n",
    "for i, row in originalNQ.iterrows():\n",
    "    if row['annotations'][0]['passage_answer']['candidate_index'] == -1:\n",
    "        first_word = row['question_text'].split()[0]\n",
    "\n",
    "        if first_word not in word_dist:\n",
    "            first_word = \"OTHER\"\n",
    "        if first_word not in negativeNQids1word:\n",
    "            negativeNQids1word[first_word] = set()\n",
    "        negativeNQids1word[first_word].add(row['example_id'])\n",
    "        \n",
    "print(len(negativeNQids1word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     qid                                               text relevant answers\n",
      "36    37  where's the peck of pickled peppers peter pipe...       -1       -\n",
      "447  448                who said i have just begun to fight       -1       -\n",
      "375  376      season 10 of comedians in cars getting coffee       -1       -\n",
      "142  143  non aqueous titration of weak bases with perch...       -1       -\n",
      "436  437          where does the tv show bellvue take place       -1       -\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "# pre-existing negs\n",
    "import pandas as pd\n",
    "\n",
    "existing_data = pd.read_csv('/dccstor/srosent2/generative/hans-dev-questions.tsv',delimiter='\\t')\n",
    "\n",
    "existing_negs = existing_data[existing_data['relevant'] == '-1']\n",
    "existing_pos = existing_data[existing_data['relevant'] != '-1']\n",
    "print(existing_negs.sample(5))\n",
    "print(len(existing_negs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "{   'OTHER': 99,\n",
      "    'a': 3,\n",
      "    'can': 4,\n",
      "    'describe': 2,\n",
      "    'difference': 3,\n",
      "    'do': 1,\n",
      "    'does': 3,\n",
      "    'explain': 2,\n",
      "    'how': 17,\n",
      "    'in': 1,\n",
      "    'is': 7,\n",
      "    'meaning': 1,\n",
      "    'the': 16,\n",
      "    'what': 32,\n",
      "    \"what's\": 1,\n",
      "    'when': 30,\n",
      "    'where': 24,\n",
      "    'which': 11,\n",
      "    'who': 39,\n",
      "    'why': 4}\n",
      "300\n",
      "16\n",
      "{   'OTHER': 20,\n",
      "    'describe': 2,\n",
      "    'difference': 1,\n",
      "    'how': 17,\n",
      "    'in': 1,\n",
      "    'is': 4,\n",
      "    'meaning': 1,\n",
      "    'summary': 1,\n",
      "    'the': 3,\n",
      "    'what': 91,\n",
      "    \"what's\": 2,\n",
      "    'when': 23,\n",
      "    'where': 65,\n",
      "    'which': 7,\n",
      "    'who': 50,\n",
      "    'why': 12}\n",
      "300\n",
      "16\n",
      "{   'OTHER': 20,\n",
      "    'describe': 2,\n",
      "    'difference': 1,\n",
      "    'how': 17,\n",
      "    'in': 1,\n",
      "    'is': 4,\n",
      "    'meaning': 1,\n",
      "    'summary': 1,\n",
      "    'the': 3,\n",
      "    'what': 91,\n",
      "    \"what's\": 2,\n",
      "    'when': 23,\n",
      "    'where': 65,\n",
      "    'which': 7,\n",
      "    'who': 50,\n",
      "    'why': 12}\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "# filter to only negatives\n",
    "import pprint \n",
    "\n",
    "def filter_original(thisNQ, existing, fname, id_key, thisLongNQ):\n",
    "    existingids1worddev = {}\n",
    "    dev_ids = set()\n",
    "\n",
    "    for i, row in existing.iterrows():\n",
    "        first_word = row['text'].split()[0]\n",
    "        existing = False\n",
    "\n",
    "        # check if this question is in existing negs.\n",
    "        recordNQ = thisNQ[thisNQ['question_text'] == row['text']]\n",
    "        if len(recordNQ) > 0:\n",
    "            existing = True\n",
    "            dev_ids.add(recordNQ.iloc[0]['example_id'])\n",
    "\n",
    "            if first_word not in word_dist:\n",
    "                first_word = \"OTHER\"\n",
    "            if first_word not in existingids1worddev:\n",
    "                existingids1worddev[first_word] = 0\n",
    "            existingids1worddev[first_word] += 1\n",
    "    print(len(existingids1worddev))\n",
    "\n",
    "    pp = pprint.PrettyPrinter(indent=4)\n",
    "    pp.pprint(existingids1worddev)\n",
    "    print(len(dev_ids))\n",
    "    dataNQ= thisLongNQ[thisLongNQ[id_key].isin(list(dev_ids))]\n",
    "    dataNQ.to_json(f\"/dccstor/srosent2/generative/appen/final/{fname}.jsonl\", orient='records', lines=True)\n",
    "    return dev_ids\n",
    "\n",
    "used_ids = []\n",
    "used_ids.extend(filter_original(originalNQdev, existing_negs, 'longNQ_dev_unanswerable_tydi', 'example_id', originalNQdev))\n",
    "used_ids.extend(filter_original(originalNQ, existing_pos, 'longNQ_dev_answerable_tydi', 'example_id', originalNQ))\n",
    "used_ids.extend(filter_original(originalNQ, existing_pos, 'longNQ_dev_answerable_eli5', 'id', longNQ))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train: 1813\n",
    "# Dev: 300: 300 train (from hans)\n",
    "# Test: 67 dev + 233 more\n",
    "# NA train: 1813 from NQ train following distribution\n",
    "# NA dev: 300 from hans \n",
    "# NA test: 300 following distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2379\n",
      "{'what': 617, 'when': 252, 'why': 106, 'who': 389, 'the': 51, 'where': 480, 'can': 8, 'how': 192, 'OTHER': 177, \"what's\": 17, 'which': 19, 'summary': 6, 'is': 13, 'difference': 11, 'in': 6, 'a': 3, 'meaning': 14, 'do': 6, 'describe': 7, 'explain': 3, 'does': 2}\n"
     ]
    }
   ],
   "source": [
    "# NA train: 1813\n",
    "\n",
    "from random import *\n",
    "\n",
    "na_ids = set()\n",
    "na_train_words = {}\n",
    "\n",
    "while len(na_ids) < 2379:\n",
    "    random_num = randint(0,len(word_dist)-1)\n",
    "    random_id = list(negativeNQids1word[word_dist[random_num]])[randint(0,len(negativeNQids1word[word_dist[random_num]])-1)]\n",
    "    num_tries = 5\n",
    "    while random_id in na_ids and num_tries > 0:\n",
    "        random_id = list(negativeNQids1word[word_dist[random_num]])[randint(0,len(negativeNQids1word[word_dist[random_num]])-1)]\n",
    "        num_tries -= 1\n",
    "    if random_id in na_ids:\n",
    "        print(\"retry\")\n",
    "        continue\n",
    "    if word_dist[random_num] not in na_train_words:\n",
    "        na_train_words[word_dist[random_num]] = 0\n",
    "    na_train_words[word_dist[random_num]] += 1\n",
    "    na_ids.add(random_id)\n",
    "print(len(na_ids))\n",
    "print(na_train_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   'OTHER': 177,\n",
      "    'a': 3,\n",
      "    'can': 8,\n",
      "    'describe': 7,\n",
      "    'difference': 11,\n",
      "    'do': 6,\n",
      "    'does': 2,\n",
      "    'explain': 3,\n",
      "    'how': 192,\n",
      "    'in': 6,\n",
      "    'is': 13,\n",
      "    'meaning': 14,\n",
      "    'summary': 6,\n",
      "    'the': 51,\n",
      "    'what': 617,\n",
      "    \"what's\": 17,\n",
      "    'when': 252,\n",
      "    'where': 480,\n",
      "    'which': 19,\n",
      "    'who': 389,\n",
      "    'why': 106}\n"
     ]
    }
   ],
   "source": [
    "import pprint \n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "pp.pprint(na_train_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NA train: 1791 from NQ train following distribution\n",
    "\n",
    "trainNQNA = originalNQ[originalNQ['example_id'].isin(na_ids)][:1791]\n",
    "trainNQNA.to_json(\"/dccstor/srosent2/generative/appen/final/longNQ_train_unanswerable_tydi.jsonl\", orient='records', lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "17\n",
      "retry\n",
      "retry\n",
      "retry\n",
      "retry\n",
      "retry\n",
      "retry\n",
      "retry\n",
      "retry\n",
      "retry\n",
      "retry\n",
      "retry\n",
      "retry\n",
      "retry\n",
      "retry\n",
      "retry\n",
      "retry\n",
      "retry\n",
      "retry\n",
      "retry\n",
      "retry\n",
      "retry\n",
      "retry\n",
      "retry\n",
      "retry\n",
      "retry\n",
      "retry\n",
      "retry\n",
      "retry\n",
      "retry\n",
      "retry\n",
      "retry\n",
      "retry\n",
      "retry\n",
      "retry\n",
      "retry\n",
      "retry\n",
      "retry\n",
      "retry\n",
      "retry\n",
      "retry\n",
      "retry\n",
      "retry\n",
      "retry\n",
      "retry\n",
      "retry\n",
      "retry\n",
      "retry\n",
      "retry\n",
      "retry\n",
      "retry\n",
      "retry\n",
      "300\n",
      "{'when': 27, 'who': 58, 'where': 52, 'in': 2, 'OTHER': 31, 'how': 24, 'what': 70, \"what's\": 4, 'the': 7, 'why': 15, 'which': 2, 'summary': 2, 'meaning': 1, 'is': 1, 'can': 1, 'describe': 2, 'difference': 1}\n"
     ]
    }
   ],
   "source": [
    "# make test sets \n",
    "negativeNQids1worddev = {}\n",
    "existing_negsids1worddev = {}\n",
    "\n",
    "def allNA(annotations):\n",
    "    count = 0\n",
    "    for annotation in annotations:\n",
    "        if annotation['passage_answer']['candidate_index'] != -1:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "for i, row in originalNQdev.iterrows():\n",
    "    if allNA(row['annotations']):\n",
    "        first_word = row['question_text'].split()[0]\n",
    "        existing = False\n",
    "\n",
    "        # check if this question is in existing negs.\n",
    "        if len(existing_negs[row['question_text'] == existing_negs['text']]) > 0:\n",
    "            existing = True\n",
    "\n",
    "        if first_word not in word_dist:\n",
    "            first_word = \"OTHER\"\n",
    "        if first_word not in negativeNQids1worddev:\n",
    "            negativeNQids1worddev[first_word] = set()\n",
    "        if existing and first_word not in existing_negsids1worddev:\n",
    "            existing_negsids1worddev[first_word] = set()\n",
    "        negativeNQids1worddev[first_word].add(row['example_id'])\n",
    "        if existing: \n",
    "            existing_negsids1worddev[first_word].add(row['example_id'])\n",
    "print(len(negativeNQids1worddev))\n",
    "print(len(existing_negsids1worddev))\n",
    "\n",
    "from random import *\n",
    "\n",
    "na_dev_ids = set()\n",
    "na_dev_words = {}\n",
    "\n",
    "while len(na_dev_ids) < 300:\n",
    "    random_num = randint(0,len(word_dist)-1)\n",
    "    random_id = list(negativeNQids1worddev[word_dist[random_num]])[randint(0,len(negativeNQids1worddev[word_dist[random_num]])-1)]\n",
    "\n",
    "    if random_id in used_ids:\n",
    "        continue\n",
    "    num_tries = 5\n",
    "    while random_id in na_ids and num_tries > 0:\n",
    "        random_id = list(negativeNQids1worddev[word_dist[random_num]])[randint(0,len(negativeNQids1worddev[word_dist[random_num]])-1)]\n",
    "        num_tries -= 1\n",
    "    if random_id in na_dev_ids:\n",
    "        print(\"retry\")\n",
    "        continue\n",
    "    if word_dist[random_num] not in na_dev_words:\n",
    "        na_dev_words[word_dist[random_num]] = 0\n",
    "    na_dev_words[word_dist[random_num]] += 1\n",
    "    na_dev_ids.add(random_id)\n",
    "print(len(na_dev_ids))\n",
    "print(na_dev_words)\n",
    "\n",
    "testNQNA = originalNQdev[originalNQdev['example_id'].isin(na_dev_ids)]\n",
    "testNQNA.to_json(\"/dccstor/srosent2/generative/appen/final/longNQ_test_unanswerable_tydi.jsonl\", orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025\n",
      "66\n"
     ]
    }
   ],
   "source": [
    "# Train: 1813\n",
    "# Test: 67 dev + 233 more\n",
    "\n",
    "longNQTrain = pd.read_json(\"/dccstor/srosent2/generative/appen/NQ_formatted_answered_multiple-9.15.23_train.json\", orient='records', lines=True, dtype={\"id\":str})\n",
    "longNQTrain = longNQTrain[~longNQTrain['id'].isin(used_ids)]\n",
    "longNQDev = pd.read_json(\"/dccstor/srosent2/generative/appen/NQ_formatted_answered_multiple-9.15.23_dev.json\", orient='records', lines=True, dtype={\"id\":str})\n",
    "longNQDev = longNQDev[~longNQDev['id'].isin(used_ids)]\n",
    "\n",
    "print(len(longNQTrain))\n",
    "print(len(longNQDev))\n",
    "\n",
    "longNQTrainShuffled = longNQTrain.sample(frac=1)\n",
    "\n",
    "longNQTrainNew = longNQTrainShuffled[234:]\n",
    "longNQDevNew = pd.concat([longNQTrainShuffled[0:234],longNQDev])\n",
    "\n",
    "longNQTrainNew.to_json(f\"/dccstor/srosent2/generative/appen/final/longNQ_train_answerable_eli5.jsonl\", orient='records', lines=True)\n",
    "longNQDevNew.to_json(f\"/dccstor/srosent2/generative/appen/final/longNQ_test_answerable_eli5.jsonl\", orient='records', lines=True)\n",
    "originalNQ[originalNQ['example_id'].isin(list(longNQTrainNew['id']))].to_json(f\"/dccstor/srosent2/generative/appen/final/longNQ_train_answerable_tydi.jsonl\", orient='records', lines=True)\n",
    "pd.concat([originalNQ[originalNQ['example_id'].isin(list(longNQDevNew['id']))],originalNQdev[originalNQdev['example_id'].isin(list(longNQDevNew['id']))]]).to_json(f\"/dccstor/srosent2/generative/appen/final/longNQ_test_answerable_tydi.jsonl\", orient='records', lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dccstor/srosent2/generative/appen/final/longNQ/dev/longNQ_dev_answerable.jsonl\n",
      "file: 300\n",
      "train: 300\n",
      "dev: 0\n",
      "/dccstor/srosent2/generative/appen/final/longNQ/dev/longNQ_dev_unanswerable.jsonl\n",
      "file: 300\n",
      "train: 0\n",
      "dev: 300\n",
      "/dccstor/srosent2/generative/appen/final/longNQ/train/longNQ_train_answerable.jsonl\n",
      "file: 1791\n",
      "train: 1791\n",
      "dev: 0\n",
      "/dccstor/srosent2/generative/appen/final/longNQ/train/longNQ_train_unanswerable.jsonl\n",
      "file: 1791\n",
      "train: 1791\n",
      "dev: 0\n",
      "/dccstor/srosent2/generative/appen/final/longNQ/test/longNQ_test_unanswerable.jsonl\n",
      "file: 300\n",
      "train: 0\n",
      "dev: 300\n",
      "/dccstor/srosent2/generative/appen/final/longNQ/test/longNQ_test_answerable.jsonl\n",
      "file: 300\n",
      "train: 234\n",
      "dev: 66\n",
      "/dccstor/srosent2/generative/appen/final/original_tydi/dev/longNQ_dev_answerable_tydi.jsonl\n",
      "file: 300\n",
      "train: 300\n",
      "dev: 0\n",
      "/dccstor/srosent2/generative/appen/final/original_tydi/dev/longNQ_dev_unanswerable_tydi.jsonl\n",
      "file: 300\n",
      "train: 0\n",
      "dev: 300\n",
      "/dccstor/srosent2/generative/appen/final/original_tydi/train/longNQ_train_answerable_tydi.jsonl\n",
      "file: 1791\n",
      "train: 1791\n",
      "dev: 0\n",
      "/dccstor/srosent2/generative/appen/final/original_tydi/train/longNQ_train_unanswerable_tydi.jsonl\n",
      "file: 1791\n",
      "train: 1791\n",
      "dev: 0\n",
      "/dccstor/srosent2/generative/appen/final/original_tydi/test/longNQ_test_unanswerable_tydi.jsonl\n",
      "file: 300\n",
      "train: 0\n",
      "dev: 300\n",
      "/dccstor/srosent2/generative/appen/final/original_tydi/test/longNQ_test_answerable_tydi.jsonl\n",
      "file: 300\n",
      "train: 234\n",
      "dev: 66\n"
     ]
    }
   ],
   "source": [
    "# check that splits are compatible with original NQ\n",
    "import pandas as pd\n",
    "\n",
    "final_files = glob.glob(\"/dccstor/srosent2/generative/appen/final/*/*/*\")\n",
    "\n",
    "for final_file in final_files:\n",
    "    id_key = 'id'\n",
    "    if 'tydi' in final_file:\n",
    "        id_key = 'example_id'\n",
    "    data = pd.read_json(final_file, orient='records', lines=True, dtype={id_key:str})\n",
    "\n",
    "    print(final_file)\n",
    "    print(f\"file: {len(data)}\")\n",
    "    print(f\"train: {len(originalNQ[originalNQ['example_id'].isin(list(data[id_key]))])}\")\n",
    "    print(f\"dev: {len(originalNQdev[originalNQdev['example_id'].isin(list(data[id_key]))])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dccstor/srosent2/generative/appen/final/longNQ/dev/longNQ_dev_answerable.jsonl\n",
      "file: 300\n",
      "train: 287\n",
      "dev: 0\n",
      "/dccstor/srosent2/generative/appen/final/longNQ/train/longNQ_train_answerable.jsonl\n",
      "file: 1791\n",
      "train: 1791\n",
      "dev: 0\n",
      "/dccstor/srosent2/generative/appen/final/longNQ/test/longNQ_test_answerable.jsonl\n",
      "file: 300\n",
      "train: 234\n",
      "dev: 66\n",
      "/dccstor/srosent2/generative/appen/final/original_tydi/dev/longNQ_dev_answerable_tydi.jsonl\n",
      "file: 300\n",
      "train: 287\n",
      "dev: 0\n",
      "/dccstor/srosent2/generative/appen/final/original_tydi/train/longNQ_train_answerable_tydi.jsonl\n",
      "file: 1791\n",
      "train: 1791\n",
      "dev: 0\n",
      "/dccstor/srosent2/generative/appen/final/original_tydi/test/longNQ_test_answerable_tydi.jsonl\n",
      "file: 300\n",
      "train: 234\n",
      "dev: 66\n"
     ]
    }
   ],
   "source": [
    "# check that splits are compatible with original NQ\n",
    "import pandas as pd\n",
    "\n",
    "longNQTrain = pd.read_json(\"/dccstor/srosent2/generative/appen/NQ_formatted_answered_multiple-9.15.23_train.json\", orient='records', lines=True, dtype={\"id\":str})\n",
    "longNQDev = pd.read_json(\"/dccstor/srosent2/generative/appen/NQ_formatted_answered_multiple-9.15.23_dev.json\", orient='records', lines=True, dtype={\"id\":str})\n",
    "\n",
    "final_files = glob.glob(\"/dccstor/srosent2/generative/appen/final/*/*/*_answerable*\")\n",
    "\n",
    "for final_file in final_files:\n",
    "    id_key = 'id'\n",
    "    if 'tydi' in final_file:\n",
    "        id_key = 'example_id'\n",
    "    data = pd.read_json(final_file, orient='records', lines=True, dtype={id_key:str})\n",
    "\n",
    "    print(final_file)\n",
    "    print(f\"file: {len(data)}\")\n",
    "    print(f\"train: {len(longNQTrain[longNQTrain['id'].isin(list(data[id_key]))])}\")\n",
    "    print(f\"dev: {len(longNQDev[longNQDev['id'].isin(list(data[id_key]))])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dccstor/srosent2/generative/appen/final/longNQ/dev/longNQ_dev_answerable.jsonl: 13\n",
      "/dccstor/srosent2/generative/appen/final/longNQ/train/longNQ_train_answerable.jsonl: 0\n",
      "/dccstor/srosent2/generative/appen/final/longNQ/test/longNQ_test_answerable.jsonl: 0\n",
      "4782\n",
      "4782\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# check non-consecutive\n",
    "# check that IDs are all unique YES\n",
    "# check that TyDi and ELI5 splits have same IDs YES\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "final_files = glob.glob(\"/dccstor/srosent2/generative/appen/final/*/*/*\")\n",
    "\n",
    "seen_ids_tydi = set()\n",
    "seen_ids_eli5 = set()\n",
    "\n",
    "for final_file in final_files:\n",
    "    id_key = 'id'\n",
    "    if 'tydi' in final_file:\n",
    "        id_key = 'example_id'\n",
    "        data = pd.read_json(final_file, orient='records', lines=True, dtype={id_key:str})\n",
    "        seen_ids_tydi.update(data[id_key])\n",
    "    else:\n",
    "        data = pd.read_json(final_file, orient='records', lines=True, dtype={id_key:str})\n",
    "        seen_ids_eli5.update(data[id_key])\n",
    "\n",
    "        if \"_answerable\" in final_file:\n",
    "            num_false = 0\n",
    "            for i, row in data.iterrows():\n",
    "                if row['output'][0]['meta']['non_consecutive'] == False:\n",
    "                    num_false += 1\n",
    "            print(f\"{final_file}: {num_false}\")\n",
    "\n",
    "print(len(seen_ids_tydi))\n",
    "print(len(seen_ids_eli5))\n",
    "\n",
    "print(seen_ids_tydi == seen_ids_eli5)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "primeqa_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
