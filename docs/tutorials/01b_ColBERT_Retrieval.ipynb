{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "764f905e",
   "metadata": {},
   "source": [
    "# Tutorial: Build a search index using DPR #\n",
    "\n",
    "In this tutorial, we will learn how to build a Neural Search index over your document collection. The algorithm displayed here is called Dense Passage Retrieval (DPR) as described in Karpukhin et al., \"Dense Passage Retrieval for Open-Domain Question Answering\" [here](https://arxiv.org/pdf/2004.04906.pdf).\n",
    "\n",
    "For the purposes of making this tutorial easy to understand we show the steps using a very small document collection. Note that this technique can be used to scale to millions of documents. We have tested upto 21 million Wikipedia passages!!!\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4fca36ec",
   "metadata": {},
   "source": [
    "## Preparing a Colab Environment to run this tutorial ##\n",
    "\n",
    "Make sure to \"Enable GPU Runtime\" -> make a URL with a page with screenshots on how to do this."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c5e3426d",
   "metadata": {},
   "source": [
    "## Installing PrimeQA\n",
    "\n",
    "First, we need to include the required modules.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c4d5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "pip install --upgrade pip\n",
    "pip install primeqa"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7e289b31",
   "metadata": {},
   "source": [
    "## Pre-process your document collection here to be ready to be stored in your Neural Search Index.\n",
    "\n",
    "TODO- add some steps after this to ingest from the sample wikipedia docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d6fde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save your input document as a .tsv\n",
    "import pandas as pd\n",
    "url='https://drive.google.com/file/d/1LULJRPgN_hfuI2kG-wH4FUwXCCdDh9zh/view?usp=sharing'\n",
    "url='https://drive.google.com/uc?id=' + url.split('/')[-2]\n",
    "df = pd.read_csv(url)\n",
    "df.to_csv('input.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2b2b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use DocumentCollection class to convert your input.tsv to the specific format needed by PrimeQA indexer/retriever.\n",
    "from primeqa.ir.util.corpus_reader import DocumentCollection\n",
    "doc_class = DocumentCollection(\"input.tsv\")\n",
    "doc_class.write_corpus_tsv(\"output.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c03fd03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "198cfd78",
   "metadata": {},
   "source": [
    "## Initializing the Indexer\n",
    "\n",
    "We initialize a ColBERT indexer which will be used for indexing the embeddings created for each document (passage) in the collection. It takes a passage_embedding_model to create the embedding vectors and a vector_db specification where it stores the embedding vectors to search later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "346c9aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from primeqa.components.DocumentStore.dense import ColBERTDocumentStore\n",
    "colbert= ColBERTDocumentStore (ctx_encoder_model_checkpoint = \"/dccstor/colbert-ir/bsiyer/PQLL/experiments/xor_squad_04182023/2023-04/22/17.23.31/checkpoints/colbert.dnn.batch_17524.model\", vector_db='FAISS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c82a4580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[Jun 10, 18:17:43] #> Creating directory index_root/index_name \n",
      "\n",
      "\n",
      "#> Starting...\n",
      "No CUDA runtime is found, using CUDA_HOME='/opt/share/cuda-11.1/x86_64'\n",
      "{\"time\":\"2023-06-10 18:17:49,684\", \"name\": \"faiss.loader\", \"level\": \"INFO\", \"message\": \"Loading faiss.\"}\n",
      "{\"time\":\"2023-06-10 18:17:49,758\", \"name\": \"faiss.loader\", \"level\": \"INFO\", \"message\": \"Successfully loaded faiss.\"}\n",
      "{\n",
      "    \"ncells\": null,\n",
      "    \"centroid_score_threshold\": null,\n",
      "    \"ndocs\": null,\n",
      "    \"index_path\": \"index_root\\/index_name\",\n",
      "    \"index_location\": null,\n",
      "    \"nbits\": 1,\n",
      "    \"kmeans_niters\": 4,\n",
      "    \"num_partitions_max\": 10000000,\n",
      "    \"similarity\": \"cosine\",\n",
      "    \"bsize\": 64,\n",
      "    \"accumsteps\": 1,\n",
      "    \"lr\": 3e-6,\n",
      "    \"maxsteps\": 500000,\n",
      "    \"save_every\": null,\n",
      "    \"resume\": false,\n",
      "    \"resume_optimizer\": false,\n",
      "    \"warmup\": null,\n",
      "    \"warmup_bert\": null,\n",
      "    \"relu\": false,\n",
      "    \"nway\": 2,\n",
      "    \"use_ib_negatives\": false,\n",
      "    \"reranker\": false,\n",
      "    \"distillation_alpha\": 1.0,\n",
      "    \"ignore_scores\": false,\n",
      "    \"shuffle_every_epoch\": false,\n",
      "    \"save_steps\": 2000,\n",
      "    \"save_epochs\": -1,\n",
      "    \"epochs\": 10,\n",
      "    \"input_arguments\": {},\n",
      "    \"model_type\": \"roberta\",\n",
      "    \"init_from_lm\": null,\n",
      "    \"local_models_repository\": null,\n",
      "    \"ranks_fn\": null,\n",
      "    \"output_dir\": null,\n",
      "    \"topK\": 100,\n",
      "    \"student_teacher_temperature\": 1.0,\n",
      "    \"student_teacher_top_loss_weight\": 0.5,\n",
      "    \"teacher_model_type\": \"bert\",\n",
      "    \"teacher_doc_maxlen\": 180,\n",
      "    \"distill_query_passage_separately\": false,\n",
      "    \"query_only\": false,\n",
      "    \"loss_function\": null,\n",
      "    \"query_weight\": 0.5,\n",
      "    \"rng_seed\": 12345,\n",
      "    \"query_maxlen\": 32,\n",
      "    \"attend_to_mask_tokens\": false,\n",
      "    \"interaction\": \"colbert\",\n",
      "    \"dim\": 128,\n",
      "    \"doc_maxlen\": 180,\n",
      "    \"mask_punctuation\": true,\n",
      "    \"checkpoint\": \"\\/dccstor\\/colbert-ir\\/bsiyer\\/PQLL\\/experiments\\/xor_squad_04182023\\/2023-04\\/22\\/17.23.31\\/checkpoints\\/colbert.dnn.batch_17524.model\",\n",
      "    \"teacher_checkpoint\": null,\n",
      "    \"triples\": null,\n",
      "    \"teacher_triples\": null,\n",
      "    \"collection\": \"output.tsv\",\n",
      "    \"queries\": null,\n",
      "    \"index_name\": \"index_name\",\n",
      "    \"overwrite\": false,\n",
      "    \"root\": \"\\/dccstor\\/irl-tableqa\\/jaydeep\\/primeqa\\/docs\\/tutorials\\/experiments\",\n",
      "    \"experiment\": \"default\",\n",
      "    \"index_root\": null,\n",
      "    \"name\": \"2023-06\\/10\\/18.14.03\",\n",
      "    \"rank\": 0,\n",
      "    \"nranks\": 1,\n",
      "    \"amp\": true,\n",
      "    \"gpus\": 0\n",
      "}\n",
      "[Jun 10, 18:17:49] #> Loading collection...\n",
      "0M \n",
      "[Jun 10, 18:17:49] #>>>>> at ColBERT name (model type) : /dccstor/colbert-ir/bsiyer/PQLL/experiments/xor_squad_04182023/2023-04/22/17.23.31/checkpoints/colbert.dnn.batch_17524.model\n",
      "[Jun 10, 18:17:49] #>>>>> at BaseColBERT name (model type) : /dccstor/colbert-ir/bsiyer/PQLL/experiments/xor_squad_04182023/2023-04/22/17.23.31/checkpoints/colbert.dnn.batch_17524.model\n",
      "[Jun 10, 18:17:56] factory model type: roberta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 482/482 [00:00<00:00, 231kB/s]\n",
      "Downloading: 100%|██████████| 1.43G/1.43G [00:32<00:00, 43.7MB/s]\n",
      "Downloading: 100%|██████████| 899k/899k [00:00<00:00, 16.5MB/s]\n",
      "Downloading: 100%|██████████| 456k/456k [00:00<00:00, 9.08MB/s]\n",
      "Downloading: 100%|██████████| 1.36M/1.36M [00:00<00:00, 20.5MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jun 10, 18:18:39] Loading segmented_maxsim_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "[Jun 10, 18:19:12] factory model type: roberta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 481/481 [00:00<00:00, 564kB/s]\n",
      "Downloading: 100%|██████████| 899k/899k [00:00<00:00, 15.5MB/s]\n",
      "Downloading: 100%|██████████| 456k/456k [00:00<00:00, 10.3MB/s]\n",
      "Downloading: 100%|██████████| 1.36M/1.36M [00:00<00:00, 17.9MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jun 10, 18:19:15] factory model type: roberta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dccstor/irl-tableqa/jaydeep/tu4.24.0/lib/python3.7/site-packages/torch/cuda/amp/grad_scaler.py:115: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n",
      "/dccstor/irl-tableqa/jaydeep/tu4.24.0/lib/python3.7/site-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jun 10, 18:19:20] [0] \t\t # of sampled PIDs = 76 \t sampled_pids[:3] = [53, 1, 38]\n",
      "[Jun 10, 18:19:20] [0] \t\t #> Encoding 76 passages..\n",
      "[Jun 10, 18:19:20] #> checkpoint, docFromText, Input: title | text, \t\t 64\n",
      "[Jun 10, 18:19:20] #> Roberta DocTokenizer.tensorize(batch_text[0], batch_background[0], bsize) ==\n",
      "[Jun 10, 18:19:20] #> Input: $ title | text, \t\t 64\n",
      "[Jun 10, 18:19:20] #> Output IDs: torch.Size([177]), tensor([    0, 50262,  1270,  1721,  2788,     2,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1])\n",
      "[Jun 10, 18:19:20] #> Output Mask: torch.Size([177]), tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "[Jun 10, 18:19:20] #> checkpoint, docFromText, Output IDs: (tensor([[    0, 50262,  1270,  ...,     1,     1,     1],\n",
      "        [    0, 50262,    22,  ...,     1,     1,     1],\n",
      "        [    0, 50262,    22,  ...,     1,     1,     1],\n",
      "        ...,\n",
      "        [    0, 50262,    22,  ...,     1,     1,     1],\n",
      "        [    0, 50262,    22,  ...,     1,     1,     1],\n",
      "        [    0, 50262,    22,  ...,     1,     1,     1]]), tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]))\n",
      "[Jun 10, 18:19:20] #>>>> colbert doc ==\n",
      "[Jun 10, 18:19:20] #>>>>> input_ids: torch.Size([177]), tensor([    0, 50262,  1270,  1721,  2788,     2,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1])\n",
      "[Jun 10, 18:19:40] #>>>> before linear doc ==\n",
      "[Jun 10, 18:19:40] #>>>>> D: torch.Size([177, 1024]), tensor([[-0.6161, -0.7938, -1.0386,  ...,  0.2401,  0.9345,  0.5298],\n",
      "        [ 0.2749, -1.1598, -0.7067,  ...,  0.5915,  1.3987,  1.2713],\n",
      "        [-0.0284, -1.3848, -0.3329,  ...,  0.3906,  1.7883,  0.5797],\n",
      "        ...,\n",
      "        [ 0.2049, -0.9493, -0.4944,  ...,  0.6183,  1.2296,  1.2579],\n",
      "        [ 0.2049, -0.9493, -0.4944,  ...,  0.6183,  1.2296,  1.2579],\n",
      "        [ 0.2049, -0.9493, -0.4944,  ...,  0.6183,  1.2296,  1.2579]])\n",
      "[Jun 10, 18:19:40] #>>>>> self.linear doc : Parameter containing:\n",
      "tensor([[-3.0738e-02,  2.1602e-03,  3.6676e-02,  ..., -2.8078e-03,\n",
      "          2.0939e-02,  1.6086e-02],\n",
      "        [ 1.5858e-02, -1.4224e-02, -1.4469e-05,  ...,  2.9249e-02,\n",
      "         -8.7473e-04, -2.0210e-02],\n",
      "        [ 1.5264e-02,  2.7762e-03, -6.8552e-03,  ...,  8.7342e-03,\n",
      "          6.7920e-03,  3.1651e-03],\n",
      "        ...,\n",
      "        [-2.3147e-03,  3.5463e-02, -3.9315e-03,  ..., -6.7647e-03,\n",
      "          9.7542e-03, -5.3362e-02],\n",
      "        [-2.0770e-02, -2.8881e-02, -1.6047e-02,  ..., -1.5005e-02,\n",
      "          1.6194e-02, -6.9083e-03],\n",
      "        [ 3.1504e-02, -8.3395e-03,  1.1636e-03,  ..., -2.6675e-02,\n",
      "          1.1555e-02,  2.7825e-02]], requires_grad=True)\n",
      "[Jun 10, 18:19:40] #>>>> colbert doc ==\n",
      "[Jun 10, 18:19:40] #>>>>> D: torch.Size([177, 128]), tensor([[-0.7747,  0.6183,  0.3650,  ...,  0.7927, -0.4596,  0.8091],\n",
      "        [-0.5594,  0.7684,  0.6692,  ...,  0.7130, -0.8679,  0.5439],\n",
      "        [-0.4377,  0.4801,  0.6009,  ...,  0.3446, -0.9019,  0.5193],\n",
      "        ...,\n",
      "        [-0.7949,  0.6527,  0.5152,  ...,  0.6576, -0.9798,  0.5175],\n",
      "        [-0.7949,  0.6527,  0.5152,  ...,  0.6576, -0.9798,  0.5175],\n",
      "        [-0.7949,  0.6527,  0.5152,  ...,  0.6576, -0.9798,  0.5175]])\n",
      "[Jun 10, 18:19:46] [0] \t\t avg_doclen_est = 162.42105102539062 \t len(local_sample) = 76\n",
      "[Jun 10, 18:19:46] >> num_partitions_multiplier = 8, self.num_partitions = 512\n",
      "[Jun 10, 18:19:46] [0] \t\t Creaing 512 partitions.\n",
      "[Jun 10, 18:19:46] [0] \t\t *Estimated* 12,343 embeddings.\n",
      "[Jun 10, 18:19:46] [0] \t\t #> Saving the indexing plan to index_root/index_name/plan.json ..\n",
      "Clustering 11727 points in 128D to 512 clusters, redo 1 times, 4 iterations\n",
      "  Preprocessing in 0.00 s\n",
      "  Iteration 1 (0.14 s, search 0.13 s): objective=1841.45 imbalance=1.859 nsplit=4        \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING clustering 11727 points to 512 centroids: please provide at least 19968 training points\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Iteration 3 (0.27 s, search 0.25 s): objective=1605.67 imbalance=1.780 nsplit=0       \n",
      "[0.024, 0.025, 0.023, 0.025, 0.026, 0.022, 0.026, 0.022, 0.024, 0.023, 0.025, 0.024, 0.025, 0.024, 0.023, 0.023, 0.025, 0.024, 0.026, 0.025, 0.026, 0.024, 0.027, 0.026, 0.025, 0.023, 0.022, 0.025, 0.026, 0.022, 0.027, 0.025, 0.025, 0.024, 0.024, 0.023, 0.026, 0.023, 0.024, 0.026, 0.026, 0.024, 0.024, 0.025, 0.025, 0.024, 0.022, 0.025, 0.025, 0.024, 0.024, 0.024, 0.024, 0.024, 0.025, 0.021, 0.025, 0.024, 0.025, 0.021, 0.025, 0.023, 0.023, 0.023, 0.024, 0.025, 0.024, 0.025, 0.021, 0.026, 0.025, 0.026, 0.025, 0.024, 0.026, 0.026, 0.023, 0.023, 0.023, 0.023, 0.024, 0.025, 0.023, 0.025, 0.024, 0.028, 0.026, 0.027, 0.024, 0.027, 0.024, 0.025, 0.026, 0.025, 0.023, 0.026, 0.025, 0.026, 0.025, 0.025, 0.027, 0.025, 0.024, 0.026, 0.026, 0.023, 0.024, 0.024, 0.025, 0.025, 0.025, 0.025, 0.026, 0.024, 0.028, 0.023, 0.025, 0.025, 0.026, 0.024, 0.025, 0.027, 0.025, 0.028, 0.028, 0.025, 0.027, 0.025]\n",
      "[Jun 10, 18:19:46] #> Got bucket_cutoffs_quantiles = tensor([0.5000]) and bucket_weights_quantiles = tensor([0.2500, 0.7500])\n",
      "[Jun 10, 18:19:46] #> Got bucket_cutoffs = tensor([0.]) and bucket_weights = tensor([-0.0159,  0.0160])\n",
      "[Jun 10, 18:19:46] avg_residual = 0.02461785450577736\n",
      "[Jun 10, 18:19:46] #> base_config.py from_path index_root/index_name/metadata.json\n",
      "[Jun 10, 18:19:46] #> base_config.py from_path index_root/index_name/plan.json\n",
      "[Jun 10, 18:19:46] #> base_config.py from_path args loaded! \n",
      "[Jun 10, 18:19:46] #> base_config.py from_path args replaced ! \n",
      "[Jun 10, 18:19:46] [0] \t\t #> Encoding 76 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jun 10, 18:20:12] [0] \t\t #> Saving chunk 0: \t 76 passages and 12,344 embeddings. From #0 onward.\n",
      "[Jun 10, 18:20:12] offset: 0\n",
      "[Jun 10, 18:20:12] chunk codes size(0): 12344\n",
      "[Jun 10, 18:20:12] codes size(0): 12344\n",
      "[Jun 10, 18:20:12] codes size(): torch.Size([12344])\n",
      "[Jun 10, 18:20:12] >>>>partition.size(0): 512\n",
      "[Jun 10, 18:20:12] >>>>num_partition: 512\n",
      "[Jun 10, 18:20:12] #> Optimizing IVF to store map from centroids to list of pids..\n",
      "[Jun 10, 18:20:12] #> Building the emb2pid mapping..\n",
      "[Jun 10, 18:20:12] len(emb2pid) = 12344\n",
      "[Jun 10, 18:20:12] #> Saved optimized IVF to index_root/index_name/ivf.pid.pt\n",
      "[Jun 10, 18:20:12] [0] \t\t #> Saving the indexing metadata to index_root/index_name/metadata.json ..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:25, 25.39s/it]\n",
      "100%|██████████| 512/512 [00:00<00:00, 84155.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#> Joined...\n"
     ]
    }
   ],
   "source": [
    "colbert.index(\"output.tsv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "20b409ad",
   "metadata": {},
   "source": [
    "## Initializing the Retriever\n",
    "\n",
    "We initialize a ColBERT retriever to search documents from the indexed document corpus.  Note: since we will retrieve the documents based on questions so we need to embed the questions too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eacb6a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexer: get collection returned as : output.tsv\n",
      "self._config ColBERTConfig(ncells=None, centroid_score_threshold=None, ndocs=None, index_path='index_root/index_name', index_location=None, nbits=1, kmeans_niters=20, num_partitions_max=10000000, similarity='cosine', bsize=32, accumsteps=1, lr=3e-06, maxsteps=500000, save_every=None, resume=False, resume_optimizer=False, warmup=None, warmup_bert=None, relu=False, nway=2, use_ib_negatives=False, reranker=False, distillation_alpha=1.0, ignore_scores=False, shuffle_every_epoch=False, save_steps=2000, save_epochs=-1, epochs=10, input_arguments={}, model_type='roberta', init_from_lm=None, local_models_repository=None, ranks_fn=None, output_dir=None, topK=100, student_teacher_temperature=1.0, student_teacher_top_loss_weight=0.5, teacher_model_type='bert', teacher_doc_maxlen=180, distill_query_passage_separately=False, query_only=False, loss_function=None, query_weight=0.5, rng_seed=12345, query_maxlen=32, attend_to_mask_tokens=False, interaction='colbert', dim=128, doc_maxlen=180, mask_punctuation=True, checkpoint=None, teacher_checkpoint=None, triples=None, teacher_triples=None, collection=None, queries=None, index_name='index_name', overwrite=False, root='/dccstor/irl-tableqa/jaydeep/primeqa/docs/tutorials/experiments', experiment='default', index_root='index_root', name='2023-06/10/18.14.03', rank=0, nranks=1, amp=True, gpus=0)\n",
      "[Jun 10, 18:20:19] #> base_config.py from_path index_root/index_name/metadata.json\n",
      "[Jun 10, 18:20:19] #> base_config.py from_path args loaded! \n",
      "[Jun 10, 18:20:19] #> base_config.py from_path args replaced ! \n",
      "[Jun 10, 18:20:24] #>>>>> at ColBERT name (model type) : /dccstor/colbert-ir/bsiyer/PQLL/experiments/xor_squad_04182023/2023-04/22/17.23.31/checkpoints/colbert.dnn.batch_17524.model\n",
      "[Jun 10, 18:20:24] #>>>>> at BaseColBERT name (model type) : /dccstor/colbert-ir/bsiyer/PQLL/experiments/xor_squad_04182023/2023-04/22/17.23.31/checkpoints/colbert.dnn.batch_17524.model\n",
      "[Jun 10, 18:20:27] factory model type: roberta\n",
      "[Jun 10, 18:20:36] Loading segmented_maxsim_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "[Jun 10, 18:20:42] factory model type: roberta\n",
      "[Jun 10, 18:20:44] factory model type: roberta\n",
      "[Jun 10, 18:20:46] #> Loading codec...\n",
      "[Jun 10, 18:20:46] #> base_config.py from_path index_root/index_name/metadata.json\n",
      "[Jun 10, 18:20:46] #> base_config.py from_path args loaded! \n",
      "[Jun 10, 18:20:46] #> base_config.py from_path args replaced ! \n",
      "[Jun 10, 18:20:46] #> Loading IVF...\n",
      "[Jun 10, 18:20:46] Loading segmented_lookup_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dccstor/irl-tableqa/jaydeep/tu4.24.0/lib/python3.7/site-packages/torch/cuda/amp/grad_scaler.py:115: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jun 10, 18:21:13] Loading filter_pids_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "[Jun 10, 18:21:40] Loading decompress_residuals_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "76it [00:00, 39741.57it/s]\n"
     ]
    }
   ],
   "source": [
    "from primeqa.components.retriever.dense import ColBERTRetriever\n",
    "retriever = ColBERTRetriever(document_store=colbert,\n",
    "                      query_encoder_model_checkpoint = \"/dccstor/colbert-ir/bsiyer/PQLL/experiments/xor_squad_04182023/2023-04/22/17.23.31/checkpoints/colbert.dnn.batch_17524.model\"\n",
    "                       )\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ab15199a",
   "metadata": {},
   "source": [
    "## Start asking Questions\n",
    "\n",
    "We're now ready to query the index we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26e6d79a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jun 10, 18:22:14] #> Roberta QueryTokenizer.tensorize(batch_text[0], batch_background[0], bsize) ==\n",
      "[Jun 10, 18:22:14] #> Input: $ What are some famous inventions by Einstein, \t\t True, \t\t None\n",
      "[Jun 10, 18:22:14] #> Output IDs: torch.Size([32]), tensor([    0, 50261,   653,    32,   103,  3395, 39232,    30, 27648,     2,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1])\n",
      "[Jun 10, 18:22:14] #> Output Mask: torch.Size([32]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "[Jun 10, 18:22:14] #>>>> colbert query ==\n",
      "[Jun 10, 18:22:14] #>>>>> input_ids: torch.Size([32]), tensor([    0, 50261,   653,    32,   103,  3395, 39232,    30, 27648,     2,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dccstor/irl-tableqa/jaydeep/tu4.24.0/lib/python3.7/site-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Jun 10, 18:22:16] #>>>> before linear query ==\n",
      "[Jun 10, 18:22:16] #>>>>> Q: torch.Size([32, 1024]), tensor([[-1.0246,  1.1656, -0.0771,  ...,  0.1875, -0.0456,  1.3663],\n",
      "        [-0.1284,  1.6476, -0.0994,  ..., -0.4682,  0.0463,  0.7251],\n",
      "        [-0.1178,  0.7274, -0.2226,  ..., -0.7760,  1.3722,  0.8347],\n",
      "        ...,\n",
      "        [-0.3671,  1.4453,  0.1219,  ..., -0.6817,  0.2066,  0.7360],\n",
      "        [-0.3671,  1.4453,  0.1219,  ..., -0.6817,  0.2066,  0.7360],\n",
      "        [-0.3671,  1.4453,  0.1219,  ..., -0.6817,  0.2066,  0.7360]])\n",
      "[Jun 10, 18:22:16] #>>>>> self.linear query : Parameter containing:\n",
      "tensor([[-3.0738e-02,  2.1602e-03,  3.6676e-02,  ..., -2.8078e-03,\n",
      "          2.0939e-02,  1.6086e-02],\n",
      "        [ 1.5858e-02, -1.4224e-02, -1.4469e-05,  ...,  2.9249e-02,\n",
      "         -8.7473e-04, -2.0210e-02],\n",
      "        [ 1.5264e-02,  2.7762e-03, -6.8552e-03,  ...,  8.7342e-03,\n",
      "          6.7920e-03,  3.1651e-03],\n",
      "        ...,\n",
      "        [-2.3147e-03,  3.5463e-02, -3.9315e-03,  ..., -6.7647e-03,\n",
      "          9.7542e-03, -5.3362e-02],\n",
      "        [-2.0770e-02, -2.8881e-02, -1.6047e-02,  ..., -1.5005e-02,\n",
      "          1.6194e-02, -6.9083e-03],\n",
      "        [ 3.1504e-02, -8.3395e-03,  1.1636e-03,  ..., -2.6675e-02,\n",
      "          1.1555e-02,  2.7825e-02]], requires_grad=True)\n",
      "[Jun 10, 18:22:16] #>>>> colbert query ==\n",
      "[Jun 10, 18:22:16] #>>>>> Q: torch.Size([32, 128]), tensor([[-1.2451, -0.8905, -0.3496,  ...,  0.1711, -0.2652, -0.1756],\n",
      "        [-1.0020, -0.7162, -0.2673,  ...,  0.4556, -0.3770, -0.3972],\n",
      "        [-0.8076, -1.4477,  0.2029,  ..., -0.8410,  0.9393, -0.0741],\n",
      "        ...,\n",
      "        [-1.1015, -0.7807, -0.2007,  ...,  0.3039, -0.1857, -0.3444],\n",
      "        [-1.1015, -0.7807, -0.2007,  ...,  0.3039, -0.1857, -0.3444],\n",
      "        [-1.1015, -0.7807, -0.2007,  ...,  0.3039, -0.1857, -0.3444]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 41.20it/s]\n"
     ]
    }
   ],
   "source": [
    "question = ['What are some famous inventions by Einstein', \"When did Aple introduce iPhone 7\"]\n",
    "retrieved_doc_ids, passages = retriever.predict(input_texts = question, mode = 'query_list',return_passages=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f7d3c2d5",
   "metadata": {},
   "source": [
    "Here are the retrived results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed63e82b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    [\n",
      "        \"\\\"\\\"\\\"Albert Einstein\\\"\\\"\\\"\\n \\\"Albert Einstein Albert Einstein (; ; 14 March 1879 \\u2013 18 April 1955) was a German-born theoretical physicist who developed the theory of relativity, one of the two pillars of modern physics (alongside quantum mechanics). His work is also known for its influence on the philosophy of science. He is best known to the general public for his mass\\u2013energy equivalence formula , which has been dubbed \\\"\\\"the world's most famous equation\\\"\\\". He received the 1921 Nobel Prize in Physics \\\"\\\"for his services to theoretical physics, and especially for his discovery of the law of the photoelectric effect\\\"\\\", a pivotal step\\\"\",\n",
      "        \"\\\"\\\"\\\"Albert Einstein\\\"\\\"\\\"\\n \\\"model for depictions of mad scientists and absent-minded professors; his expressive face and distinctive hairstyle have been widely copied and exaggerated. \\\"\\\"Time\\\"\\\" magazine's Frederic Golden wrote that Einstein was \\\"\\\"a cartoonist's dream come true\\\"\\\". Many popular quotations are often misattributed to him. Einstein received numerous awards and honors and in 1922 he was awarded the 1921 Nobel Prize in Physics \\\"\\\"for his services to Theoretical Physics, and especially for his discovery of the law of the photoelectric effect\\\"\\\". None of the nominations in 1921 met the criteria set by Alfred Nobel, so the 1921 prize was carried forward and awarded\\\"\",\n",
      "        \"\\\"\\\"\\\"Albert Einstein\\\"\\\"\\\"\\n \\\"to Einstein in 1922. Footnotes Citations Albert Einstein Albert Einstein (; ; 14 March 1879 \\u2013 18 April 1955) was a German-born theoretical physicist who developed the theory of relativity, one of the two pillars of modern physics (alongside quantum mechanics). His work is also known for its influence on the philosophy of science. He is best known to the general public for his mass\\u2013energy equivalence formula , which has been dubbed \\\"\\\"the world's most famous equation\\\"\\\". He received the 1921 Nobel Prize in Physics \\\"\\\"for his services to theoretical physics, and especially for his discovery of the law of\\\"\"\n",
      "    ],\n",
      "    [\n",
      "        \"\\\"\\\"\\\"Apple Inc.\\\"\\\"\\\"\\n 2016, Apple introduced the iPhone 7 and the iPhone 7 Plus, which feature improved system and graphics performance, add water resistance, a new rear dual-camera system on the 7 Plus model, and, controversially, remove the 3.5 mm headphone jack. On September 12, 2017, Apple introduced the iPhone 8 and iPhone 8 Plus, standing as evolutionary updates to its previous phones with a faster processor, improved display technology, upgraded camera systems and wireless charging. The company also announced iPhone X, which radically changes the hardware of the iPhone lineup, removing the home button in favor of facial recognition technology and featuring\",\n",
      "        \"\\\"\\\"\\\"Apple Inc.\\\"\\\"\\\"\\n A mid-October 2013 announcement revealed that Burberry executive Angela Ahrendts will commence as a senior vice president at Apple in mid-2014. Ahrendts oversaw Burberry's digital strategy for almost eight years and, during her tenure, sales increased to about US$3.2 billion and shares gained more than threefold. Alongside Google vice-president Vint Cerf and AT&T CEO Randall Stephenson, Cook attended a closed-door summit held by President Obama on August 8, 2013, in regard to government surveillance and the Internet in the wake of the Edward Snowden NSA incident. On February 4, 2014, Cook met with Abdullah G\\u00fcl, the President of Turkey, in\",\n",
      "        \"\\\"\\\"\\\"America the Beautiful\\\"\\\"\\\"\\n \\\"broadcast of the Tiangong-1 launch. The song is often included in songbooks in a wide variety of religious congregations in the United States. Bing Crosby included the song in a medley on his album \\\"\\\"101 Gang Songs\\\"\\\" (1961). In 1976, while the United States celebrated its bicentennial, a soulful version popularized by Ray Charles peaked at number 98 on the US R&B Charts. Ray Charles did this again in 1984 to re-elect Ronald Reagan. Ray Charles did this yet again in Miami, Florida in 1999. Three different renditions of the song have entered the Hot Country Songs charts. The first\\\"\"\n",
      "    ]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "print(json.dumps(passages, indent = 4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
