{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from primeqa.pipelines.components.reader.extractive import ExtractiveReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"time\":\"2022-12-06 17:44:18,895\", \"name\": \"ExtractiveQAHead\", \"level\": \"INFO\", \"message\": \"Loading dropout value 0.1 from config attribute 'hidden_dropout_prob'\"}\n",
      "{\"time\":\"2022-12-06 17:44:19,563\", \"name\": \"XLMRobertaModelForDownstreamTasks\", \"level\": \"INFO\", \"message\": \"Setting task head for first time to 'None'\"}\n"
     ]
    }
   ],
   "source": [
    "# load the extractive TyDi QA model that has been initialized with XLM-Roberta.\n",
    "reader = ExtractiveReader(model=\"PrimeQA/nq_tydi_sq1-reader-xlmr_large-20221110\")\n",
    "reader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ab2040df1b24173a39f0fbb4c0e8ec7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on eval dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"time\":\"2022-12-06 17:44:37,854\", \"name\": \"primeqa.mrc.trainers.mrc\", \"level\": \"INFO\", \"message\": \"The following columns in the evaluation set  don't have a corresponding argument in `XLMRobertaModelForDownstreamTasks.forward` and have been ignored: example_id, context_idx, offset_mapping, example_idx.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 150.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"0\": [\n",
      "        {\n",
      "            \"example_id\": \"0\",\n",
      "            \"passage_index\": 0,\n",
      "            \"span_answer_text\": \"Bob\",\n",
      "            \"span_answer\": {\n",
      "                \"start_position\": 21,\n",
      "                \"end_position\": 24\n",
      "            },\n",
      "            \"span_answer_score\": 7.202423095703125,\n",
      "            \"confidence_score\": 0.6905364059480282\n",
      "        },\n",
      "        {\n",
      "            \"example_id\": \"0\",\n",
      "            \"passage_index\": 0,\n",
      "            \"span_answer_text\": \"Bob walks the dog\",\n",
      "            \"span_answer\": {\n",
      "                \"start_position\": 21,\n",
      "                \"end_position\": 38\n",
      "            },\n",
      "            \"span_answer_score\": 5.9897801876068115,\n",
      "            \"confidence_score\": 0.20537257885731638\n",
      "        },\n",
      "        {\n",
      "            \"example_id\": \"0\",\n",
      "            \"passage_index\": 0,\n",
      "            \"span_answer_text\": \"Alice walks the cat. Bob\",\n",
      "            \"span_answer\": {\n",
      "                \"start_position\": 0,\n",
      "                \"end_position\": 24\n",
      "            },\n",
      "            \"span_answer_score\": 5.3102200627326965,\n",
      "            \"confidence_score\": 0.10409101519465531\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "questions = [\"¿Quién paseó al perro?\"]\n",
    "contexts = [[\"Alice walks the cat. Bob walks the dog\"]]\n",
    "answers = reader.predict(questions,contexts)  \n",
    "print(json.dumps(answers, indent=4))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea50b4215c3d4056b04f53538c94d8d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on eval dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"time\":\"2022-12-06 17:45:06,071\", \"name\": \"primeqa.mrc.trainers.mrc\", \"level\": \"INFO\", \"message\": \"The following columns in the evaluation set  don't have a corresponding argument in `XLMRobertaModelForDownstreamTasks.forward` and have been ignored: example_id, context_idx, offset_mapping, example_idx.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 505.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"0\": [\n",
      "        {\n",
      "            \"example_id\": \"0\",\n",
      "            \"passage_index\": 0,\n",
      "            \"span_answer_text\": \"Australia\",\n",
      "            \"span_answer\": {\n",
      "                \"start_position\": 32,\n",
      "                \"end_position\": 41\n",
      "            },\n",
      "            \"span_answer_score\": 14.109326839447021,\n",
      "            \"confidence_score\": 0.6732346778531001\n",
      "        },\n",
      "        {\n",
      "            \"example_id\": \"0\",\n",
      "            \"passage_index\": 0,\n",
      "            \"span_answer_text\": \"Australia. \\nFounded following the federation of the colonies of Australia\",\n",
      "            \"span_answer\": {\n",
      "                \"start_position\": 32,\n",
      "                \"end_position\": 105\n",
      "            },\n",
      "            \"span_answer_score\": 12.882871329784393,\n",
      "            \"confidence_score\": 0.1974802270822016\n",
      "        },\n",
      "        {\n",
      "            \"example_id\": \"0\",\n",
      "            \"passage_index\": 0,\n",
      "            \"span_answer_text\": \"Australia. \\nFounded following the federation of the colonies of Australia \\nas the seat of government for the new nation, it is Australia\",\n",
      "            \"span_answer\": {\n",
      "                \"start_position\": 32,\n",
      "                \"end_position\": 168\n",
      "            },\n",
      "            \"span_answer_score\": 12.459252871572971,\n",
      "            \"confidence_score\": 0.12928509506469837\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "question = [\"Which country is Canberra located in?\"]\n",
    "context = [[\"\"\"Canberra is the capital city of Australia. \n",
    "Founded following the federation of the colonies of Australia \n",
    "as the seat of government for the new nation, it is Australia's \n",
    "largest inland city\"\"\"]]\n",
    "answers = reader.predict(question,context)  \n",
    "print(json.dumps(answers, indent=4))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "https://huggingface.co/PrimeQA/tydiqa-ft-listqa_nq-task-xlm-roberta-large/resolve/main/config.json not found in cache or force_download set to True, downloading to /u/bsiyer/.cache/huggingface/transformers/tmpcxogfj2a\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "054f95436206454faf0e632f8860f05b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/837 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/PrimeQA/tydiqa-ft-listqa_nq-task-xlm-roberta-large/resolve/main/config.json in cache at /u/bsiyer/.cache/huggingface/transformers/9bc845a9562743047a594fb8e1e9d520f13abf7eea6f126a1b1759c8d4f38de9.b1ca3c6dd971a4817fecef76104f8bed8bfc2db98900cdd1cc038669fd879ce2\n",
      "creating metadata file for /u/bsiyer/.cache/huggingface/transformers/9bc845a9562743047a594fb8e1e9d520f13abf7eea6f126a1b1759c8d4f38de9.b1ca3c6dd971a4817fecef76104f8bed8bfc2db98900cdd1cc038669fd879ce2\n",
      "loading configuration file https://huggingface.co/PrimeQA/tydiqa-ft-listqa_nq-task-xlm-roberta-large/resolve/main/config.json from cache at /u/bsiyer/.cache/huggingface/transformers/9bc845a9562743047a594fb8e1e9d520f13abf7eea6f126a1b1759c8d4f38de9.b1ca3c6dd971a4817fecef76104f8bed8bfc2db98900cdd1cc038669fd879ce2\n",
      "Model config XLMRobertaConfig {\n",
      "  \"_name_or_path\": \"PrimeQA/tydiqa-ft-listqa_nq-task-xlm-roberta-large\",\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaModelForDownstreamTasks\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"decoding_times_with_dropout\": 5,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_dropout_rate\": 0.25,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"sep_token_id\": 2,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "https://huggingface.co/PrimeQA/tydiqa-ft-listqa_nq-task-xlm-roberta-large/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /u/bsiyer/.cache/huggingface/transformers/tmphhznx10o\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16c370f5a37c4f3c825b091c6ecba536",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/427 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/PrimeQA/tydiqa-ft-listqa_nq-task-xlm-roberta-large/resolve/main/tokenizer_config.json in cache at /u/bsiyer/.cache/huggingface/transformers/140370205f988f472b6d6232170dff5ba75fbfff316b6bf72a697c7132a2bbe0.ff7d9cc37d61711de7a2f7ebc2f240c39f4c5e75c5ed8b9bf32fac57acaa855b\n",
      "creating metadata file for /u/bsiyer/.cache/huggingface/transformers/140370205f988f472b6d6232170dff5ba75fbfff316b6bf72a697c7132a2bbe0.ff7d9cc37d61711de7a2f7ebc2f240c39f4c5e75c5ed8b9bf32fac57acaa855b\n",
      "https://huggingface.co/PrimeQA/tydiqa-ft-listqa_nq-task-xlm-roberta-large/resolve/main/sentencepiece.bpe.model not found in cache or force_download set to True, downloading to /u/bsiyer/.cache/huggingface/transformers/tmpdbo1st7y\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37ff9008e83c42bb8c8521a163c46d4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/4.83M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/PrimeQA/tydiqa-ft-listqa_nq-task-xlm-roberta-large/resolve/main/sentencepiece.bpe.model in cache at /u/bsiyer/.cache/huggingface/transformers/da8fdf9d32b3e07a0a15ecfc4c0847453f295887e60a8cca0ee915888d9d763b.71e50b08dbe7e5375398e165096cacc3d2086119d6a449364490da6908de655e\n",
      "creating metadata file for /u/bsiyer/.cache/huggingface/transformers/da8fdf9d32b3e07a0a15ecfc4c0847453f295887e60a8cca0ee915888d9d763b.71e50b08dbe7e5375398e165096cacc3d2086119d6a449364490da6908de655e\n",
      "https://huggingface.co/PrimeQA/tydiqa-ft-listqa_nq-task-xlm-roberta-large/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /u/bsiyer/.cache/huggingface/transformers/tmpnz79o125\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bc6a061b93b4a64b742b23811390139",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/16.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/PrimeQA/tydiqa-ft-listqa_nq-task-xlm-roberta-large/resolve/main/tokenizer.json in cache at /u/bsiyer/.cache/huggingface/transformers/65667933bf75faad8d563b0ff9a60b5fcce879191d67b74583bd35a10a8215f7.ee8124cbaffa5062e0e2c74a858021aaa326e114c8fd886df3cdae3cb314265f\n",
      "creating metadata file for /u/bsiyer/.cache/huggingface/transformers/65667933bf75faad8d563b0ff9a60b5fcce879191d67b74583bd35a10a8215f7.ee8124cbaffa5062e0e2c74a858021aaa326e114c8fd886df3cdae3cb314265f\n",
      "https://huggingface.co/PrimeQA/tydiqa-ft-listqa_nq-task-xlm-roberta-large/resolve/main/special_tokens_map.json not found in cache or force_download set to True, downloading to /u/bsiyer/.cache/huggingface/transformers/tmp_8b29w1n\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "177c09b6690642ba9799c4fe45318134",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/PrimeQA/tydiqa-ft-listqa_nq-task-xlm-roberta-large/resolve/main/special_tokens_map.json in cache at /u/bsiyer/.cache/huggingface/transformers/351c05ec2eb5e478a478f65a2fa63233dbd08472d70192f4eb7d2ed50aef5a61.a11ebb04664c067c8fe5ef8f8068b0f721263414a26058692f7b2e4ba2a1b342\n",
      "creating metadata file for /u/bsiyer/.cache/huggingface/transformers/351c05ec2eb5e478a478f65a2fa63233dbd08472d70192f4eb7d2ed50aef5a61.a11ebb04664c067c8fe5ef8f8068b0f721263414a26058692f7b2e4ba2a1b342\n",
      "loading file https://huggingface.co/PrimeQA/tydiqa-ft-listqa_nq-task-xlm-roberta-large/resolve/main/sentencepiece.bpe.model from cache at /u/bsiyer/.cache/huggingface/transformers/da8fdf9d32b3e07a0a15ecfc4c0847453f295887e60a8cca0ee915888d9d763b.71e50b08dbe7e5375398e165096cacc3d2086119d6a449364490da6908de655e\n",
      "loading file https://huggingface.co/PrimeQA/tydiqa-ft-listqa_nq-task-xlm-roberta-large/resolve/main/tokenizer.json from cache at /u/bsiyer/.cache/huggingface/transformers/65667933bf75faad8d563b0ff9a60b5fcce879191d67b74583bd35a10a8215f7.ee8124cbaffa5062e0e2c74a858021aaa326e114c8fd886df3cdae3cb314265f\n",
      "loading file https://huggingface.co/PrimeQA/tydiqa-ft-listqa_nq-task-xlm-roberta-large/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/PrimeQA/tydiqa-ft-listqa_nq-task-xlm-roberta-large/resolve/main/special_tokens_map.json from cache at /u/bsiyer/.cache/huggingface/transformers/351c05ec2eb5e478a478f65a2fa63233dbd08472d70192f4eb7d2ed50aef5a61.a11ebb04664c067c8fe5ef8f8068b0f721263414a26058692f7b2e4ba2a1b342\n",
      "loading file https://huggingface.co/PrimeQA/tydiqa-ft-listqa_nq-task-xlm-roberta-large/resolve/main/tokenizer_config.json from cache at /u/bsiyer/.cache/huggingface/transformers/140370205f988f472b6d6232170dff5ba75fbfff316b6bf72a697c7132a2bbe0.ff7d9cc37d61711de7a2f7ebc2f240c39f4c5e75c5ed8b9bf32fac57acaa855b\n",
      "https://huggingface.co/PrimeQA/tydiqa-ft-listqa_nq-task-xlm-roberta-large/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /u/bsiyer/.cache/huggingface/transformers/tmps0msfach\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6681bc9d12b4d8798a0147a873a189d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.09G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://huggingface.co/PrimeQA/tydiqa-ft-listqa_nq-task-xlm-roberta-large/resolve/main/pytorch_model.bin in cache at /u/bsiyer/.cache/huggingface/transformers/6314d822873e437fb5d4645a25e82038affa9ee5e0a2188811b8f2a4d72b587f.99a909aafd00980e2c4884d43d991a0376fc064dc5a95d0cafa4e528dc851ecd\n",
      "creating metadata file for /u/bsiyer/.cache/huggingface/transformers/6314d822873e437fb5d4645a25e82038affa9ee5e0a2188811b8f2a4d72b587f.99a909aafd00980e2c4884d43d991a0376fc064dc5a95d0cafa4e528dc851ecd\n",
      "loading weights file https://huggingface.co/PrimeQA/tydiqa-ft-listqa_nq-task-xlm-roberta-large/resolve/main/pytorch_model.bin from cache at /u/bsiyer/.cache/huggingface/transformers/6314d822873e437fb5d4645a25e82038affa9ee5e0a2188811b8f2a4d72b587f.99a909aafd00980e2c4884d43d991a0376fc064dc5a95d0cafa4e528dc851ecd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"time\":\"2022-12-06 17:46:27,702\", \"name\": \"ExtractiveQAHead\", \"level\": \"INFO\", \"message\": \"Loading dropout value 0.1 from config attribute 'hidden_dropout_prob'\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing XLMRobertaModelForDownstreamTasks.\n",
      "\n",
      "All the weights of XLMRobertaModelForDownstreamTasks were initialized from the model checkpoint at PrimeQA/tydiqa-ft-listqa_nq-task-xlm-roberta-large.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaModelForDownstreamTasks for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"time\":\"2022-12-06 17:46:28,404\", \"name\": \"XLMRobertaModelForDownstreamTasks\", \"level\": \"INFO\", \"message\": \"Setting task head for first time to 'None'\"}\n"
     ]
    }
   ],
   "source": [
    "# load the fine-tuned Natural Questions List QA model that has been initialized with the TyDi model\n",
    "list_reader = ExtractiveReader(model=\"PrimeQA/tydiqa-ft-listqa_nq-task-xlm-roberta-large\")\n",
    "list_reader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "357cef92181e4e948cff18a5e600e7ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on eval dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"time\":\"2022-12-06 21:18:43,908\", \"name\": \"primeqa.mrc.trainers.mrc\", \"level\": \"INFO\", \"message\": \"The following columns in the evaluation set  don't have a corresponding argument in `XLMRobertaModelForDownstreamTasks.forward` and have been ignored: example_id, context_idx, offset_mapping, example_idx.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 129.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"0\": [\n",
      "        {\n",
      "            \"example_id\": \"0\",\n",
      "            \"passage_index\": 0,\n",
      "            \"span_answer_text\": \"* Andaman and Nicobar Islands -- Port Blair * Chandigarh -- Chandigarh * Dadra and Nagar Haveli -- Silvassa * Daman and Diu -- Daman * Lakshwadweep -- Kavaratti * National Capital Territory -- New Delhi * Puducherry -- Pondicherry\",\n",
      "            \"span_answer\": {\n",
      "                \"start_position\": 192,\n",
      "                \"end_position\": 422\n",
      "            },\n",
      "            \"span_answer_score\": 4.881481051445007,\n",
      "            \"confidence_score\": 0.47059373601539756\n",
      "        },\n",
      "        {\n",
      "            \"example_id\": \"0\",\n",
      "            \"passage_index\": 0,\n",
      "            \"span_answer_text\": \"* Andaman and Nicobar Islands -- Port Blair * Chandigarh -- Chandigarh * Dadra and Nagar Haveli -- Silvassa * Daman and Diu -- Daman * Lakshwadweep -- Kavaratti * National Capital Territory -- New Delhi * Puducherry -- Pondicherry * * * State and Union Territory capitals of India * Agartala * Aizawl * Amaravati ( de facto ) * Bangalore * Bhopal * Bhubaneswar * Chandigarh * Chennai * Daman * Dehradun ( interim ) * New Delhi * Dispur * Gandhinagar * Gangtok * Hyderabad * Imphal * Itanagar * Jaipur * Jammu ( in winter ) * Kavaratti * Kohima * Kolkata * Lucknow * Mumbai * Panaji * Patna * Pondicherry * Port Blair * Raipur * Ranchi * Shillong * Shimla * Silvassa * Srinagar ( in summer ) * Thiruvananthapuram * * * States and union territories of India States * Arunachal Pradesh * Andhra Pradesh * Assam * Bihar * Chhattisgarh * Goa * Gujarat * Haryana * Himachal Pradesh * Jammu and Kashmir * Jharkhand * Karnataka * Kerala * Madhya Pradesh * Maharashtra * Manipur * Meghalaya * Mizoram * Nagaland * Odisha * Punjab * Rajasthan * Sikkim * Tamil Nadu * Telangana * Tripura * Uttar Pradesh * Uttarakhand * West Bengal Union Territories * Andaman and Nicobar Islands * Chandigarh * Dadra and Nagar Haveli * National Capital Territory of Delhi * Daman and Diu * Lakshadweep * Puducherry * Capitals in India * Proposed states and territories * Historical Regions * British Provinces\",\n",
      "            \"span_answer\": {\n",
      "                \"start_position\": 192,\n",
      "                \"end_position\": 1574\n",
      "            },\n",
      "            \"span_answer_score\": 4.541947185993195,\n",
      "            \"confidence_score\": 0.3351108257060536\n",
      "        },\n",
      "        {\n",
      "            \"example_id\": \"0\",\n",
      "            \"passage_index\": 0,\n",
      "            \"span_answer_text\": \"* Andaman and Nicobar Islands -- Port Blair * Chandigarh -- Chandigarh * Dadra and Nagar Haveli -- Silvassa * Daman and Diu -- Daman * Lakshwadweep -- Kavaratti * National Capital Territory -- New Delhi * Puducherry -- Pondicherry * * * State and Union Territory capitals of India * Agartala * Aizawl * Amaravati ( de facto ) * Bangalore * Bhopal * Bhubaneswar * Chandigarh * Chennai * Daman * Dehradun ( interim ) * New Delhi * Dispur * Gandhinagar * Gangtok * Hyderabad * Imphal * Itanagar * Jaipur * Jammu ( in winter ) * Kavaratti * Kohima * Kolkata * Lucknow * Mumbai * Panaji * Patna * Pondicherry * Port Blair * Raipur * Ranchi * Shillong * Shimla * Silvassa * Srinagar ( in summer ) * Thiruvananthapuram * * * States and union territories of India States * Arunachal Pradesh * Andhra Pradesh * Assam * Bihar * Chhattisgarh * Goa * Gujarat * Haryana * Himachal Pradesh * Jammu and Kashmir * Jharkhand * Karnataka * Kerala * Madhya Pradesh * Maharashtra * Manipur * Meghalaya * Mizoram * Nagaland * Odisha * Punjab * Rajasthan * Sikkim * Tamil Nadu * Telangana * Tripura * Uttar Pradesh * Uttarakhand * West Bengal\",\n",
      "            \"span_answer\": {\n",
      "                \"start_position\": 192,\n",
      "                \"end_position\": 1312\n",
      "            },\n",
      "            \"span_answer_score\": 3.996865764260292,\n",
      "            \"confidence_score\": 0.19429543827854873\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "question = [\"seven union territories of india and their capital\"]\n",
    "context = [[\"Category : indian Union Territory capitals - wikipedia Help Category : indian Union Territory capitals Jump to : navigation , search * The 7 Union Territories of India and their capitals is : * Andaman and Nicobar Islands -- Port Blair * Chandigarh -- Chandigarh * Dadra and Nagar Haveli -- Silvassa * Daman and Diu -- Daman * Lakshwadweep -- Kavaratti * National Capital Territory -- New Delhi * Puducherry -- Pondicherry * * * State and Union Territory capitals of India * Agartala * Aizawl * Amaravati ( de facto ) * Bangalore * Bhopal * Bhubaneswar * Chandigarh * Chennai * Daman * Dehradun ( interim ) * New Delhi * Dispur * Gandhinagar * Gangtok * Hyderabad * Imphal * Itanagar * Jaipur * Jammu ( in winter ) * Kavaratti * Kohima * Kolkata * Lucknow * Mumbai * Panaji * Patna * Pondicherry * Port Blair * Raipur * Ranchi * Shillong * Shimla * Silvassa * Srinagar ( in summer ) * Thiruvananthapuram * * * States and union territories of India States * Arunachal Pradesh * Andhra Pradesh * Assam * Bihar * Chhattisgarh * Goa * Gujarat * Haryana * Himachal Pradesh * Jammu and Kashmir * Jharkhand * Karnataka * Kerala * Madhya Pradesh * Maharashtra * Manipur * Meghalaya * Mizoram * Nagaland * Odisha * Punjab * Rajasthan * Sikkim * Tamil Nadu * Telangana * Tripura * Uttar Pradesh * Uttarakhand * West Bengal Union Territories * Andaman and Nicobar Islands * Chandigarh * Dadra and Nagar Haveli * National Capital Territory of Delhi * Daman and Diu * Lakshadweep * Puducherry * Capitals in India * Proposed states and territories * Historical Regions * British Provinces This category has the following 7 subcategories , out of 7 total . * \\u25ba Chandigarh \\u200e ( 18 C , 7 P ) * \\u25ba Daman , Daman and Diu \\u200e ( 3 P ) * \\u25ba Kavaratti \\u200e ( 1 P ) * \\u25ba New Delhi \\u200e ( 6 C , 85 P ) * \\u25ba Pondicherry ( city ) \\u200e ( 3 C , 3 P ) * \\u25ba Port Blair \\u200e ( 1 C , 12 P ) * \\u25ba Silvassa \\u200e ( 1 C , 2 P ) Pages in category `` indian Union Territory capitals '' The following 7 pages are in this category , out of 7 total . This list may not reflect recent changes ( learn more ) . * Chandigarh * Daman , Daman and Diu * Kavaratti * New Delhi * Pondicherry * Port Blair * Silvassa Retrieved from `` https://en.wikipedia.org/w/index.php?title=Category:Indian_Union_Territory_capitals&oldid=836071111 '' Categories : * Indian capital cities * Union Territories of India * Cities and towns in India * * Talk * * * * * * * * * * * * * * * * Help * About Wikipedia * * * * * * * * * * * * * * \\u09ac\\u09be\\u0982\\u09b2\\u09be * \\u092e\\u0948\\u0925\\u093f\\u0932\\u0940 * \\u0d2e\\u0d32\\u0d2f\\u0d3e\\u0d33\\u0d02 Edit links * This page was last edited on 12 April 2018 , at 14 : 33 . * * * About Wikipedia * * * * * * * * \"]]\n",
    "answers = list_reader.predict(question,context)  \n",
    "print(json.dumps(answers, indent=4))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Can't instantiate abstract class GenerativeFiDReader with abstract method predict",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [9], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjson\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mprimeqa\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpipelines\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcomponents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mreader\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgenerative\u001b[39;00m \u001b[39mimport\u001b[39;00m GenerativeFiDReader\n\u001b[0;32m----> 4\u001b[0m fid_reader \u001b[39m=\u001b[39m GenerativeFiDReader(model\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mPrimeQA/eli5-fid-bart-large-with-colbert-passages\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      5\u001b[0m fid_reader\u001b[39m.\u001b[39mload()\n",
      "\u001b[0;31mTypeError\u001b[0m: Can't instantiate abstract class GenerativeFiDReader with abstract method predict"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from primeqa.pipelines.components.reader.generative import GenerativeFiDReader\n",
    "\n",
    "fid_reader = GenerativeFiDReader(model='PrimeQA/eli5-fid-bart-large-with-colbert-passages')\n",
    "fid_reader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "859ccd983fef4583a419baeb6cc52789",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on eval dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `BartFiDModelForDownstreamTasks.forward` and have been ignored: example_id. If example_id are not expected by `BartFiDModelForDownstreamTasks.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"example_id\": \"0\",\n",
      "        \"text\": \"The water vapor from the exhaust of the jet is condensing on the particles of the surrounding air. \\n\\nThe water is so cold that it freezes and forms a thin layer of ice.  \\nThe ice is so thin that it can't escape the jet, so it forms a trail behind the jet.\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "question = [\"What causes the trail behind jets at high altitude?\"]\n",
    "context = [[\"\"\"Chemtrail conspiracy theory The chemtrail conspiracy theory is based \n",
    "            on the erroneous belief that long-lasting condensation trails are \n",
    "            \\\"chemtrails\\\" consisting of chemical or biological agents left in the \n",
    "            sky by high-flying aircraft, sprayed for nefarious purposes undisclosed \n",
    "            to the general public. Believers in this conspiracy theory say that while \n",
    "            normal contrails dissipate relatively quickly, contrails that linger must \n",
    "            contain additional substances. Those who subscribe to the theory speculate \n",
    "            that the purpose of the chemical release may be solar radiation management,\n",
    "            weather modification, psychological manipulation, human population control, \n",
    "            or biological or chemical warfare, and that the trails are causing \n",
    "            respiratory illnesses\"\"\",\n",
    "            \"\"\"Associated with jet streams is a phenomenon known as clear-air turbulence \n",
    "            (CAT), caused by vertical and horizontal wind shear caused by jet streams. \n",
    "            The CAT is strongest on the cold air side of the jet, next to and just under \n",
    "            the axis of the jet. Clear-air turbulence can cause aircraft to plunge and so \n",
    "            present a passenger safety hazard that has caused fatal accidents, such as the \n",
    "            death of one passenger on United Airlines Flight 826. \n",
    "            Section: Uses.:Possible future power generation.\"\"\",\n",
    "            \"\"\"Contrails are a manmade type of cirrus cloud formed when water vapor from \n",
    "            the exhaust of a jet engine condenses on particles, which come from either the \n",
    "            surrounding air or the exhaust itself, and freezes, leaving behind a visible trail. \n",
    "            The exhaust can also trigger the formation of cirrus by providing ice nuclei \n",
    "            when there is an insufficient naturally-occurring supply in the atmosphere. \n",
    "            One of the environmental impacts of aviation is that persistent contrails can \n",
    "            form into large mats of cirrus, and increased air traffic has been implicated \n",
    "            as one possible cause of the increasing frequency and amount of cirrus\"\"\"]]\n",
    "answers = fid_reader.predict(question,context)  \n",
    "print(json.dumps(answers, indent=4)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('primeqa')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c2ac43764ba8ae0952fa1403ba32aab1290221796a17e39b9a47976d46f1b90e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
