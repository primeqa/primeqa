{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59a2970f",
   "metadata": {},
   "source": [
    "## In this tutorial we will learn how to use QLORA [Dettmers23] to fine-tune a LLM for QA tasks.\n",
    "\n",
    "The notebook shows an example with Falcon-7B. In practice you can also try larger LLMs e.g. GPT-NeoX-20B etc.\n",
    "\n",
    "### Step 0: Prepare a Colab Environment to run this tutorial on GPUs\n",
    "Make sure to \"Enable GPU Runtime\" by following this [url](https://drive.google.com/file/d/1jhE8CkieQXoW0gvz9IherTDdJY54Q4Yz/view?usp=sharing). This step will make sure the tutorial runs faster.\n",
    "\n",
    "### Step 1: Do all the necessary pip installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844c0f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "!pip install scipy\n",
    "!pip install -q -U bitsandbytes\n",
    "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
    "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "!pip install -q datasets\n",
    "!pip install einops  # needed for falcon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e805917",
   "metadata": {},
   "source": [
    "### Step 2: Do the necessary imports and instantiate a model from the HuggingFace model hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3934689",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import bitsandbytes\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37645a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_id = \"EleutherAI/gpt-neox-20b\"\n",
    "model_id=\"ybelkada/falcon-7b-sharded-bf16\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map=\"auto\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b331b5",
   "metadata": {},
   "source": [
    "### Step3: Initialize PEFT based QLORA training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750e6d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "\n",
    "    \n",
    "from peft import LoraConfig, get_peft_model\n",
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"query_key_value\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2799e8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53e814c",
   "metadata": {},
   "source": [
    "### Step 4: Get a QA dataset. \n",
    "\n",
    "Here we get the SQuAD v2 with answerable questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836eff6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "qa_dataset = load_dataset(\"squad_v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7980bb3",
   "metadata": {},
   "source": [
    "### Step 5: Create a prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ebaf9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(context, question, answer):\n",
    "  if len(answer[\"text\"]) < 1:\n",
    "    answer = \"Cannot Find Answer\"\n",
    "  else:\n",
    "    answer = answer[\"text\"][0]\n",
    "  prompt_template = f\"### CONTEXT\\n{context}\\n\\n### QUESTION\\n{question}\\n\\n### ANSWER\\n{answer}</s>\"\n",
    "  return prompt_template\n",
    "\n",
    "mapped_qa_dataset = qa_dataset.map(lambda samples: tokenizer(create_prompt(samples['context'], samples['question'], samples['answers'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d59e330",
   "metadata": {},
   "source": [
    "### Step 6: Start QLORA fine-tuning.\n",
    "\n",
    "We only show an example here for 100 steps. You can run this for longer to get a stable QA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c92783e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "# needed for gpt-neo-x tokenizer\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=mapped_qa_dataset[\"train\"],\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=100,\n",
    "        max_steps=100,\n",
    "        learning_rate=1e-3,\n",
    "        fp16=True,\n",
    "        logging_steps=1,\n",
    "        output_dir=\"~/path_to_some_output/qlora/outputs\",\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        report_to=\"none\"  # turns off wandb\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1906f9f0",
   "metadata": {},
   "source": [
    "### Step 7: Do inference with the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2014f88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "def make_inference(context, question):\n",
    "  batch = tokenizer(f\"### CONTEXT\\n{context}\\n\\n### QUESTION\\n{question}\\n\\n### ANSWER\\n\", return_tensors='pt', return_token_type_ids=False)\n",
    "\n",
    "  with torch.cuda.amp.autocast():\n",
    "    output_tokens = model.generate(**batch, max_new_tokens=50)\n",
    "    \n",
    "  display(Markdown((tokenizer.decode(output_tokens[0], skip_special_tokens=True))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06aaf45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is an example of \"answerable\" question given a context\n",
    "\n",
    "context = \"Cheese is the best food.\"\n",
    "question = \"What is the best food?\"\n",
    "\n",
    "make_inference(context, question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5063f009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is an example of \"unanswerable\" question given a context\n",
    "\n",
    "context = \"Cheese is the best food.\"\n",
    "question = \"How far away is the Moon from the Earth?\"\n",
    "\n",
    "make_inference(context, question) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ea9450",
   "metadata": {},
   "source": [
    "Congratulations ðŸŽ‰âœ¨ðŸŽŠðŸ¥³ !! You can now fine-tune a LLM with PrimeQA and QLORA."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
