{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c771c295",
   "metadata": {},
   "source": [
    "# Confidence Calibration\n",
    "\n",
    "In this notebook, we will see how to train a confidence calibration model for MRC using the TyDiQA dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5349c2d7",
   "metadata": {},
   "source": [
    "# Dependencies\n",
    "\n",
    "If not already done, make sure to install PrimeQA with notebooks extras before getting started.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b573272a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want CUDA 11 uncomment and run this (for CUDA 10 or CPU you can ignore this line).\n",
    "#! pip install 'torch~=1.11.0' --extra-index-url https://download.pytorch.org/whl/cu113\n",
    "\n",
    "# Uncomment to install PrimeQA from source (pypi package pending).\n",
    "# The path should be the project root (e.g. '.' below).\n",
    "#! pip install .[notebooks]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f2176e",
   "metadata": {},
   "source": [
    "# Configuration\n",
    "\n",
    "We start by setting some parameters to configure the process. Note that depending on the GPU being used you may need to tune the batch size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c80327c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These need to be filled in.\n",
    "output_dir = 'FILL_ME_IN'               # Save the mrc model here.  Will overwrite if directory already exists.\n",
    "confidence_model_dir = 'FILL_ME_IN'     # Save the confidence model here.  Will overwrite if diirectory already exists.\n",
    "confidence_dataset_dir = 'FILL_ME_IN'   # Save the confidence model training data here.  Will overwrite if directory already exists.\n",
    "\n",
    "# Parameters for mrc model training (feel free to leave as default).\n",
    "model_name = 'xlm-roberta-base'  # Set this to select the LM.  Since this is a multi-lingual dataset, we use the XLM-Roberta model.\n",
    "cache_dir = None                 # Set this if you have a cache directory for transformers.  Alternatively set the HF_HOME env var.\n",
    "train_batch_size = 8             # Set this to change the number of features per batch during training.\n",
    "eval_batch_size = 8              # Set this to change the number of features per batch during evaluation.\n",
    "gradient_accumulation_steps = 8  # Set this to effectively increase training batch size.\n",
    "max_train_samples = 300          # Set this to use a subset of the training data (or None for all).\n",
    "max_eval_samples = 50            # Set this to use a subset of the evaluation data (or None for all).\n",
    "num_train_epochs = 1             # Set this to change the number of training epochs.\n",
    "fp16 = False                     # Set this to true to enable fp16 (hardware support required).\n",
    "\n",
    "# Parameters for coonfidence model training (feel free to leave as default).\n",
    "relative_confidence_train_size = 0.1          # Set this to change the relative size of confidence model train set split from original train set.\n",
    "max_iter_of_confidence_model_training = 100   # Set this to change the maximum number of iterations for confidence model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8547da04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from transformers.trainer_utils import set_seed\n",
    "\n",
    "seed = 42\n",
    "set_seed(seed)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=train_batch_size,\n",
    "    per_device_eval_batch_size=eval_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    evaluation_strategy='no',\n",
    "    learning_rate=4e-05,\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.1,\n",
    "    save_steps=50000,\n",
    "    fp16=fp16,\n",
    "    seed=seed,\n",
    ")\n",
    "\n",
    "task_args = dict(\n",
    "    confidence_model_dir=confidence_model_dir,\n",
    "    confidence_dataset_dir=confidence_dataset_dir,\n",
    "    relative_confidence_train_size=relative_confidence_train_size,\n",
    "    max_iter_of_confidence_model_training=max_iter_of_confidence_model_training,    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d57558f",
   "metadata": {},
   "source": [
    "# Loading Data\n",
    "\n",
    "Here we load the TyDiQA dataset using Huggingface's datasets library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e0f2601",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset tydiqa (/dccstor/zhrong-nmt/QA/primeqa/exp/cache/tydiqa/primary_task/1.0.0/b8a6c4c0db10bf5703d7b36645e5dbae821b8c0e902dac9daeecd459a8337148)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5720cbc4cffd49d78cfe74e958bd67e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "raw_datasets = datasets.load_dataset(\n",
    "    'tydiqa',\n",
    "    'primary_task',\n",
    "    cache_dir=cache_dir,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a29555d",
   "metadata": {},
   "source": [
    "# Splitting Data\n",
    "\n",
    "Here we split the train set of the raw_datasets into mrc_train set and confidence_train set with the ratio specified by 'relative_confidence_train_size'. For example, 'relative_confidence_train_size=0.1' means 10% of the original train data is used for confidence model training, and 90% is used for MRC model training. The new datasets, including the two new train sets and original validation set are saved to the directory specifid by 'confidence_dataset_dir'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c689196",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at /dccstor/zhrong-nmt/QA/primeqa/exp/cache/tydiqa/primary_task/1.0.0/b8a6c4c0db10bf5703d7b36645e5dbae821b8c0e902dac9daeecd459a8337148/cache-4d5134431da553f5.arrow and /dccstor/zhrong-nmt/QA/primeqa/exp/cache/tydiqa/primary_task/1.0.0/b8a6c4c0db10bf5703d7b36645e5dbae821b8c0e902dac9daeecd459a8337148/cache-46529ac46b6a5efb.arrow\n",
      "Loading cached processed dataset at /dccstor/zhrong-nmt/QA/primeqa/exp/cache/tydiqa/primary_task/1.0.0/b8a6c4c0db10bf5703d7b36645e5dbae821b8c0e902dac9daeecd459a8337148/cache-8548cd041ab057a6.arrow\n",
      "Loading cached processed dataset at /dccstor/zhrong-nmt/QA/primeqa/exp/cache/tydiqa/primary_task/1.0.0/b8a6c4c0db10bf5703d7b36645e5dbae821b8c0e902dac9daeecd459a8337148/cache-dc77c82f5ac6aaa8.arrow\n"
     ]
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "import os\n",
    "\n",
    "original_train_set = raw_datasets[\"train\"]\n",
    "split_train_set = original_train_set.train_test_split(test_size=task_args['relative_confidence_train_size'])\n",
    "mrc_train_set = split_train_set[\"train\"]\n",
    "confidence_train_set = split_train_set[\"test\"]\n",
    "validation_set = raw_datasets[\"validation\"]\n",
    "\n",
    "confidence_datasets = DatasetDict({\n",
    "    \"mrc_train\": mrc_train_set,\n",
    "    \"confidence_train\": confidence_train_set,\n",
    "    \"validation\": validation_set,\n",
    "})\n",
    "\n",
    "# save new datasets\n",
    "os.makedirs(task_args['confidence_dataset_dir'], exist_ok=True)\n",
    "confidence_datasets.save_to_disk(task_args['confidence_dataset_dir'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a62ad7",
   "metadata": {},
   "source": [
    "# Building MRC Model\n",
    "\n",
    "Before the building of a confidence calibration model, we need to train a MRC model which will be used later to generate the features for confidence model training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b6707e",
   "metadata": {},
   "source": [
    "## Loading Language Model\n",
    "\n",
    "The first step of MRC model training is to load the LM and tokenizer based on the model_name parameter set above. We also set the task head to EXTRACTIVE_WITH_CONFIDENCE_HEAD which supports confidence calibration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55e940f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaModelForDownstreamTasks were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['task_heads.qa_head.classifier.dense.bias', 'task_heads.qa_head.classifier.out_proj.bias', 'task_heads.qa_head.classifier.out_proj.weight', 'task_heads.qa_head.qa_outputs.weight', 'task_heads.qa_head.classifier.dense.weight', 'task_heads.qa_head.qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XLMRobertaModelForDownstreamTasks(\n",
      "  (roberta): XLMRobertaModel(\n",
      "    (embeddings): RobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
      "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): RobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): RobertaPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (lm_head): RobertaLMHead(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (decoder): Linear(in_features=768, out_features=250002, bias=True)\n",
      "  )\n",
      "  (task_heads): ModuleDict(\n",
      "    (qa_head): ExtractiveQAWithConfidenceHead(\n",
      "      (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
      "      (classifier): RobertaClassificationHead(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (out_proj): Linear(in_features=768, out_features=5, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig, AutoTokenizer\n",
    "from primeqa.mrc.models.heads.extractive import EXTRACTIVE_WITH_CONFIDENCE_HEAD\n",
    "from primeqa.mrc.models.task_model import ModelForDownstreamTasks\n",
    "\n",
    "from primeqa.mrc.trainers.mrc import MRCTrainer\n",
    "\n",
    "task_heads = EXTRACTIVE_WITH_CONFIDENCE_HEAD\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_name,\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    cache_dir=cache_dir,\n",
    "    use_fast=True,\n",
    "    config=config,\n",
    ")\n",
    "config.sep_token_id = tokenizer.convert_tokens_to_ids(tokenizer.sep_token)\n",
    "config.output_dropout_rate = 0.25\n",
    "config.decoding_times_with_dropout = 5\n",
    "model = ModelForDownstreamTasks.from_config(\n",
    "    config,\n",
    "    model_name,\n",
    "    task_heads=task_heads,\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "model.set_task_head(next(iter(task_heads)))\n",
    "\n",
    "print(model)  # Examine the model structure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95743033",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Next we preprocess the data of mrc_train set which can be given to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "837656a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function TyDiQAPreprocessor._rename_examples at 0x7f8fb1b8d7a0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "Loading cached processed dataset at /dccstor/zhrong-nmt/QA/primeqa/exp/cache/tydiqa/primary_task/1.0.0/b8a6c4c0db10bf5703d7b36645e5dbae821b8c0e902dac9daeecd459a8337148/cache-1c80317fa3b1799d.arrow\n",
      "Loading cached processed dataset at /dccstor/zhrong-nmt/QA/primeqa/exp/cache/tydiqa/primary_task/1.0.0/b8a6c4c0db10bf5703d7b36645e5dbae821b8c0e902dac9daeecd459a8337148/cache-bdd640fb06671ad1.arrow\n",
      "Loading cached processed dataset at /dccstor/zhrong-nmt/QA/primeqa/exp/cache/tydiqa/primary_task/1.0.0/b8a6c4c0db10bf5703d7b36645e5dbae821b8c0e902dac9daeecd459a8337148/cache-3eb13b9046685257.arrow\n",
      "Loading cached processed dataset at /dccstor/zhrong-nmt/QA/primeqa/exp/cache/tydiqa/primary_task/1.0.0/b8a6c4c0db10bf5703d7b36645e5dbae821b8c0e902dac9daeecd459a8337148/cache-23b8c1e9392456de.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing produced 447 train features from 500 examples.\n"
     ]
    }
   ],
   "source": [
    "from primeqa.mrc.processors.preprocessors.tydiqa import TyDiQAPreprocessor\n",
    "\n",
    "preprocessor = TyDiQAPreprocessor(\n",
    "    stride=128,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "train_dataset = confidence_datasets[\"mrc_train\"]\n",
    "if max_train_samples is not None:\n",
    "    # We will select sample from whole data if argument is specified\n",
    "    train_dataset = train_dataset.select(range(max_train_samples))\n",
    "# Train Feature Creation\n",
    "with training_args.main_process_first(desc=\"train dataset map pre-processing\"):\n",
    "    train_examples, train_dataset = preprocessor.process_train(train_dataset)\n",
    "print(f\"Preprocessing produced {train_dataset.num_rows} train features from {train_examples.num_rows} examples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6b0e4e",
   "metadata": {},
   "source": [
    "## Fine-tuning\n",
    "\n",
    "Here we fine-tune the MRC model on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7f8edd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dccstor/zhrong-nmt/anaconda3/envs/primeqa/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 447\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 7\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7/7 08:32, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to FILL_ME_IN\n",
      "Configuration saved in FILL_ME_IN/config.json\n",
      "Model weights saved in FILL_ME_IN/pytorch_model.bin\n",
      "tokenizer config file saved in FILL_ME_IN/tokenizer_config.json\n",
      "Special tokens file saved in FILL_ME_IN/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  total_flos               =   111370GF\n",
      "  train_loss               =      4.608\n",
      "  train_runtime            = 0:09:58.81\n",
      "  train_samples            =        447\n",
      "  train_samples_per_second =      0.746\n",
      "  train_steps_per_second   =      0.012\n"
     ]
    }
   ],
   "source": [
    "from operator import attrgetter\n",
    "from transformers import DataCollatorWithPadding\n",
    "from primeqa.mrc.data_models.eval_prediction_with_processing import EvalPredictionWithProcessing\n",
    "from primeqa.mrc.metrics.tydi_f1.tydi_f1 import TyDiF1\n",
    "from primeqa.mrc.processors.postprocessors.extractive import ExtractivePostProcessor\n",
    "from primeqa.mrc.processors.postprocessors.scorers import SupportedSpanScorers\n",
    "\n",
    "# If using mixed precision we pad for efficient hardware acceleration\n",
    "using_mixed_precision = any(attrgetter('fp16', 'bf16')(training_args))\n",
    "data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=64 if using_mixed_precision else None)\n",
    "\n",
    "# noinspection PyProtectedMember\n",
    "postprocessor = ExtractivePostProcessor(\n",
    "    k=3,\n",
    "    n_best_size=20,\n",
    "    max_answer_length=30,\n",
    "    scorer_type=SupportedSpanScorers.WEIGHTED_SUM_TARGET_TYPE_AND_SCORE_DIFF,\n",
    "    single_context_multiple_passages=preprocessor._single_context_multiple_passages,\n",
    "    output_confidence_feature = True,\n",
    ")\n",
    "\n",
    "def compute_metrics(p: EvalPredictionWithProcessing):\n",
    "    return TyDiF1().compute(predictions=p.processed_predictions, references=p.label_ids)\n",
    "\n",
    "trainer = MRCTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset if training_args.do_train else None,\n",
    "    eval_dataset=None,\n",
    "    eval_examples=None,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    post_process_function=postprocessor.process_references_and_predictions,  # see QATrainer in Huggingface\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "train_result = trainer.train()\n",
    "trainer.save_model()  # Saves the tokenizer too for easy upload\n",
    "\n",
    "metrics = train_result.metrics\n",
    "max_train_samples = max_train_samples or len(train_dataset)\n",
    "metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n",
    "\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "trainer.save_metrics(\"train\", metrics)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fde2ca2",
   "metadata": {},
   "source": [
    "# Building Confidence Calibration Model\n",
    "\n",
    "Here we start to build the confidence calibration model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ede4e7",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "We first preprocess the data of confidence_train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d1c4dea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "116f64716ae24befb3ac31328e8c5f56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7b3747040864a879ece777a09b7890b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eabfd4e81694fd6a5b62a23336593cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "753bed8cdd2c4d6292d8d401bd64dee9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on eval dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing produced 1475 train features from 100 examples.\n"
     ]
    }
   ],
   "source": [
    "conf_examples = confidence_datasets[\"confidence_train\"]\n",
    "if max_eval_samples is not None:\n",
    "    # We will select sample from whole data\n",
    "    conf_examples = conf_examples.select(range(max_eval_samples))\n",
    "# Feature Creation\n",
    "with training_args.main_process_first(desc=\"confidence dataset map pre-processing\"):\n",
    "    conf_examples, conf_dataset = preprocessor.process_eval(conf_examples)\n",
    "print(f\"Preprocessing produced {conf_dataset.num_rows} train features from {conf_examples.num_rows} examples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3e0d0e",
   "metadata": {},
   "source": [
    "## Generating Confidence Features\n",
    "\n",
    "Here we run the MRC model obtained in previous step on the processed confidence_train set. The prediction file contains the information that can be used to generate confidence features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f0273ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1475\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='321' max='185' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [185/185 22:35]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 100/100 [00:05<00:00, 18.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passage & english & \\fpr{0.0}{0.0}{0.0}\n",
      "Minimal Answer & english & \\fpr{0.0}{0.0}{0.0}\n",
      "********************\n",
      "english\n",
      "Language: english (3)\n",
      "********************\n",
      "PASSAGE ANSWER R@P TABLE:\n",
      "Optimal threshold: 0.0\n",
      " F1     /  P      /  R\n",
      "  0.00% /   0.00% /   0.00%\n",
      "R@P=0.5: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "R@P=0.75: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "R@P=0.9: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "********************\n",
      "MINIMAL ANSWER R@P TABLE:\n",
      "Optimal threshold: 0.0\n",
      " F1     /  P      /  R\n",
      "  0.00% /   0.00% /   0.00%\n",
      "R@P=0.5: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "R@P=0.75: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "R@P=0.9: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "Passage & arabic & \\fpr{0.0}{0.0}{0.0}\n",
      "Minimal Answer & arabic & \\fpr{0.0}{0.0}{0.0}\n",
      "********************\n",
      "arabic\n",
      "Language: arabic (14)\n",
      "********************\n",
      "PASSAGE ANSWER R@P TABLE:\n",
      "Optimal threshold: 0.0\n",
      " F1     /  P      /  R\n",
      "  0.00% /   0.00% /   0.00%\n",
      "R@P=0.5: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "R@P=0.75: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "R@P=0.9: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "********************\n",
      "MINIMAL ANSWER R@P TABLE:\n",
      "Optimal threshold: 0.0\n",
      " F1     /  P      /  R\n",
      "  0.00% /   0.00% /   0.00%\n",
      "R@P=0.5: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "R@P=0.75: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "R@P=0.9: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "Passage & bengali & \\fpr{0.0}{0.0}{0.0}\n",
      "Minimal Answer & bengali & \\fpr{0.0}{0.0}{0.0}\n",
      "********************\n",
      "bengali\n",
      "Language: bengali (4)\n",
      "********************\n",
      "PASSAGE ANSWER R@P TABLE:\n",
      "Optimal threshold: 0.0\n",
      " F1     /  P      /  R\n",
      "  0.00% /   0.00% /   0.00%\n",
      "R@P=0.5: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "R@P=0.75: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "R@P=0.9: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "********************\n",
      "MINIMAL ANSWER R@P TABLE:\n",
      "Optimal threshold: 0.0\n",
      " F1     /  P      /  R\n",
      "  0.00% /   0.00% /   0.00%\n",
      "R@P=0.5: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "R@P=0.75: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "R@P=0.9: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "Passage & finnish & \\fpr{0.0}{0.0}{0.0}\n",
      "Minimal Answer & finnish & \\fpr{0.0}{0.0}{0.0}\n",
      "********************\n",
      "finnish\n",
      "Language: finnish (9)\n",
      "********************\n",
      "PASSAGE ANSWER R@P TABLE:\n",
      "Optimal threshold: 0.0\n",
      " F1     /  P      /  R\n",
      "  0.00% /   0.00% /   0.00%\n",
      "R@P=0.5: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "R@P=0.75: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "R@P=0.9: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "********************\n",
      "MINIMAL ANSWER R@P TABLE:\n",
      "Optimal threshold: 0.0\n",
      " F1     /  P      /  R\n",
      "  0.00% /   0.00% /   0.00%\n",
      "R@P=0.5: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "R@P=0.75: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "R@P=0.9: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "Passage & indonesian & \\fpr{0.0}{0.0}{0.0}\n",
      "Minimal Answer & indonesian & \\fpr{0.0}{0.0}{0.0}\n",
      "********************\n",
      "indonesian\n",
      "Language: indonesian (7)\n",
      "********************\n",
      "PASSAGE ANSWER R@P TABLE:\n",
      "Optimal threshold: 0.0\n",
      " F1     /  P      /  R\n",
      "  0.00% /   0.00% /   0.00%\n",
      "R@P=0.5: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "R@P=0.75: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "R@P=0.9: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "********************\n",
      "MINIMAL ANSWER R@P TABLE:\n",
      "Optimal threshold: 0.0\n",
      " F1     /  P      /  R\n",
      "  0.00% /   0.00% /   0.00%\n",
      "R@P=0.5: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "R@P=0.75: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "R@P=0.9: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "Passage & japanese & \\fpr{0.0}{0.0}{0.0}\n",
      "Minimal Answer & japanese & \\fpr{0.0}{0.0}{0.0}\n",
      "********************\n",
      "japanese\n",
      "Language: japanese (9)\n",
      "********************\n",
      "PASSAGE ANSWER R@P TABLE:\n",
      "Optimal threshold: 0.0\n",
      " F1     /  P      /  R\n",
      "  0.00% /   0.00% /   0.00%\n",
      "R@P=0.5: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "R@P=0.75: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "R@P=0.9: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "********************\n",
      "MINIMAL ANSWER R@P TABLE:\n",
      "Optimal threshold: 0.0\n",
      " F1     /  P      /  R\n",
      "  0.00% /   0.00% /   0.00%\n",
      "R@P=0.5: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "R@P=0.75: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "R@P=0.9: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "Passage & swahili & \\fpr{0.0}{0.0}{0.0}\n",
      "Minimal Answer & swahili & \\fpr{0.0}{0.0}{0.0}\n",
      "********************\n",
      "swahili\n",
      "Language: swahili (13)\n",
      "********************\n",
      "PASSAGE ANSWER R@P TABLE:\n",
      "Optimal threshold: 0.0\n",
      " F1     /  P      /  R\n",
      "  0.00% /   0.00% /   0.00%\n",
      "R@P=0.5: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "R@P=0.75: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "R@P=0.9: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "********************\n",
      "MINIMAL ANSWER R@P TABLE:\n",
      "Optimal threshold: 0.0\n",
      " F1     /  P      /  R\n",
      "  0.00% /   0.00% /   0.00%\n",
      "R@P=0.5: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "R@P=0.75: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "R@P=0.9: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "Passage & korean & \\fpr{0.0}{0.0}{0.0}\n",
      "Minimal Answer & korean & \\fpr{0.0}{0.0}{0.0}\n",
      "********************\n",
      "korean\n",
      "Language: korean (5)\n",
      "********************\n",
      "PASSAGE ANSWER R@P TABLE:\n",
      "Optimal threshold: 0.0\n",
      " F1     /  P      /  R\n",
      "  0.00% /   0.00% /   0.00%\n",
      "R@P=0.5: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "R@P=0.75: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "R@P=0.9: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "********************\n",
      "MINIMAL ANSWER R@P TABLE:\n",
      "Optimal threshold: 0.0\n",
      " F1     /  P      /  R\n",
      "  0.00% /   0.00% /   0.00%\n",
      "R@P=0.5: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "R@P=0.75: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "R@P=0.9: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "Passage & russian & \\fpr{0.0}{0.0}{0.0}\n",
      "Minimal Answer & russian & \\fpr{0.0}{0.0}{0.0}\n",
      "********************\n",
      "russian\n",
      "Language: russian (10)\n",
      "********************\n",
      "PASSAGE ANSWER R@P TABLE:\n",
      "Optimal threshold: 0.0\n",
      " F1     /  P      /  R\n",
      "  0.00% /   0.00% /   0.00%\n",
      "R@P=0.5: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "R@P=0.75: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "R@P=0.9: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "********************\n",
      "MINIMAL ANSWER R@P TABLE:\n",
      "Optimal threshold: 0.0\n",
      " F1     /  P      /  R\n",
      "  0.00% /   0.00% /   0.00%\n",
      "R@P=0.5: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "R@P=0.75: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "R@P=0.9: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "Passage & telugu & \\fpr{0.0}{0.0}{0.0}\n",
      "Minimal Answer & telugu & \\fpr{0.0}{0.0}{0.0}\n",
      "********************\n",
      "telugu\n",
      "Language: telugu (18)\n",
      "********************\n",
      "PASSAGE ANSWER R@P TABLE:\n",
      "Optimal threshold: 0.0\n",
      " F1     /  P      /  R\n",
      "  0.00% /   0.00% /   0.00%\n",
      "R@P=0.5: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "R@P=0.75: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "R@P=0.9: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "********************\n",
      "MINIMAL ANSWER R@P TABLE:\n",
      "Optimal threshold: 0.0\n",
      " F1     /  P      /  R\n",
      "  0.00% /   0.00% /   0.00%\n",
      "R@P=0.5: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "R@P=0.75: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "R@P=0.9: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "Passage & thai & \\fpr{0.0}{0.0}{0.0}\n",
      "Minimal Answer & thai & \\fpr{0.0}{0.0}{0.0}\n",
      "********************\n",
      "thai\n",
      "Language: thai (8)\n",
      "********************\n",
      "PASSAGE ANSWER R@P TABLE:\n",
      "Optimal threshold: 0.0\n",
      " F1     /  P      /  R\n",
      "  0.00% /   0.00% /   0.00%\n",
      "R@P=0.5: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "R@P=0.75: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "R@P=0.9: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "********************\n",
      "MINIMAL ANSWER R@P TABLE:\n",
      "Optimal threshold: 0.0\n",
      " F1     /  P      /  R\n",
      "  0.00% /   0.00% /   0.00%\n",
      "R@P=0.5: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "R@P=0.75: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "R@P=0.9: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "Total # examples in gold: 100, # ex. in pred: 100 (including english)\n",
      "*** Macro Over 10 Languages, excluding English **\n",
      "Passage F1:0.000 P:0.000 R:0.000000\n",
      "\\fpr{0.0}{0.0}{0.0}\n",
      "Minimal F1:0.000 P:0.000 R:0.000000\n",
      "\\fpr{0.0}{0.0}{0.0}\n",
      "*** / Aggregate Scores ****\n",
      "{\"avg_passage_f1\": 0.0, \"avg_passage_recall\": 0.0, \"avg_passage_precision\": 0.0, \"avg_minimal_f1\": 0.0, \"avg_minimal_recall\": 0.0, \"avg_minimal_precision\": 0.0}\n",
      "***** eval metrics *****\n",
      "  confidence_samples         = 100\n",
      "  epoch                      = 1.0\n",
      "  eval_avg_minimal_f1        = 0.0\n",
      "  eval_avg_minimal_precision = 0.0\n",
      "  eval_avg_minimal_recall    = 0.0\n",
      "  eval_avg_passage_f1        = 0.0\n",
      "  eval_avg_passage_precision = 0.0\n",
      "  eval_avg_passage_recall    = 0.0\n"
     ]
    }
   ],
   "source": [
    "metrics = trainer.evaluate(eval_dataset=conf_dataset, eval_examples=conf_examples)\n",
    "max_conf_samples = max_eval_samples if max_eval_samples else len(conf_dataset)\n",
    "metrics[\"confidence_samples\"] = min(max_conf_samples, len(conf_dataset))\n",
    "trainer.log_metrics(\"eval\", metrics)\n",
    "trainer.save_metrics(\"eval\", metrics)\n",
    "\n",
    "for fn in [\"eval_predictions.json\", \"eval_references.json\", \"eval_predictions_processed.json\"]:\n",
    "    if not os.path.exists(os.path.join(training_args.output_dir, fn)):\n",
    "        raise ValueError(f\"Unable to find eval result file {fn} from {training_args.output_dir}.\")\n",
    "\n",
    "os.replace(os.path.join(training_args.output_dir, 'eval_predictions.json'),\n",
    "           os.path.join(training_args.output_dir, 'conf_predictions.json'))\n",
    "os.replace(os.path.join(training_args.output_dir, 'eval_references.json'),\n",
    "           os.path.join(training_args.output_dir, 'conf_references.json'))\n",
    "confidence_set_prediction_file = os.path.join(training_args.output_dir, \"conf_predictions.json\")\n",
    "confidence_set_reference_file = os.path.join(training_args.output_dir, \"conf_references.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd392c5d",
   "metadata": {},
   "source": [
    "## Training Confidence Model\n",
    "\n",
    "Here we train the confidence calibration model and save it to the directory specified in 'confidence_model_dir'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d3ad670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training confidence model ...\n",
      "Iteration 1, loss = 0.51296729\n",
      "Iteration 2, loss = 0.37961850\n",
      "Iteration 3, loss = 0.27984038\n",
      "Iteration 4, loss = 0.20783396\n",
      "Iteration 5, loss = 0.15726690\n",
      "Iteration 6, loss = 0.12240877\n",
      "Iteration 7, loss = 0.09866111\n",
      "Iteration 8, loss = 0.08260248\n",
      "Iteration 9, loss = 0.07180332\n",
      "Iteration 10, loss = 0.06458435\n",
      "Iteration 11, loss = 0.05980054\n",
      "Iteration 12, loss = 0.05667476\n",
      "Iteration 13, loss = 0.05467909\n",
      "Iteration 14, loss = 0.05345336\n",
      "Iteration 15, loss = 0.05275034\n",
      "Iteration 16, loss = 0.05239923\n",
      "Iteration 17, loss = 0.05228125\n",
      "Iteration 18, loss = 0.05231340\n",
      "Iteration 19, loss = 0.05243743\n",
      "Iteration 20, loss = 0.05261242\n",
      "Iteration 21, loss = 0.05280964\n",
      "Iteration 22, loss = 0.05300907\n",
      "Iteration 23, loss = 0.05319686\n",
      "Iteration 24, loss = 0.05336367\n",
      "Iteration 25, loss = 0.05350336\n",
      "Iteration 26, loss = 0.05361218\n",
      "Iteration 27, loss = 0.05368806\n",
      "Iteration 28, loss = 0.05373018\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['FILL_ME_IN/confidence_model.bin']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "from joblib import dump, load\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from primeqa.calibration.confidence_scorer import ConfidenceScorer\n",
    "\n",
    "confidence_model = MLPClassifier(random_state = 1, activation = 'tanh',\n",
    "                                 hidden_layer_sizes=(100,100),\n",
    "                                 max_iter=task_args['max_iter_of_confidence_model_training'],\n",
    "                                 verbose=1)\n",
    "X, Y = ConfidenceScorer.make_training_data(confidence_set_prediction_file, confidence_set_reference_file)\n",
    "\n",
    "print(\"Training confidence model ...\")\n",
    "confidence_model.fit(X, Y)\n",
    "\n",
    "confidence_model_file = os.path.join(task_args['confidence_model_dir'], 'confidence_model.bin')\n",
    "dump(confidence_model, confidence_model_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f3e296",
   "metadata": {},
   "source": [
    "# Running MRC with Confidence Calibration\n",
    "\n",
    "Here we evaluate the MRC model on the validation set, and apply the confidence calibration model obtained in previous step to the prediction result. Each answer will be assigned a confidence score to show how reliable it is.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba66bca9",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Here we preprocess the data of validation set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dfcb17eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bc96ea2d49a4c179d871377f20c91f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f76e49df4094bd08f0497976a972327",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c47f6b45db1c4b2bb84c4839c012e8fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83790a50ea744cc88eabfb841f26feca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on eval dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing produced 1084 train features from 100 examples.\n"
     ]
    }
   ],
   "source": [
    "eval_examples = confidence_datasets[\"validation\"]\n",
    "if max_eval_samples is not None:\n",
    "    # We will select sample from whole data if argument is specified\n",
    "    eval_examples = eval_examples.select(range(max_eval_samples))\n",
    "# Validation Feature Creation\n",
    "with training_args.main_process_first(desc=\"validation dataset map pre-processing\"):\n",
    "    eval_examples, eval_dataset = preprocessor.process_eval(eval_examples)\n",
    "print(f\"Preprocessing produced {eval_dataset.num_rows} train features from {eval_examples.num_rows} examples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168bbe80",
   "metadata": {},
   "source": [
    "## Predicting Answer and Assigning Confidence Score\n",
    "\n",
    "Here we run the MRC model to predict answer for each question and apply the confidence calibration model to get the confidence score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6152170e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1084\n",
      "  Batch size = 8\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 100/100 [00:03<00:00, 25.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passage & english & \\fpr{0.0}{0.0}{0.0}\n",
      "Minimal Answer & english & \\fpr{0.0}{0.0}{0.0}\n",
      "********************\n",
      "english\n",
      "Language: english (1)\n",
      "********************\n",
      "PASSAGE ANSWER R@P TABLE:\n",
      "Optimal threshold: 0.0\n",
      " F1     /  P      /  R\n",
      "  0.00% /   0.00% /   0.00%\n",
      "R@P=0.5: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "R@P=0.75: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "R@P=0.9: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "********************\n",
      "MINIMAL ANSWER R@P TABLE:\n",
      "Optimal threshold: 0.0\n",
      " F1     /  P      /  R\n",
      "  0.00% /   0.00% /   0.00%\n",
      "R@P=0.5: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "R@P=0.75: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "R@P=0.9: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "Passage & arabic & \\fpr{0.0}{0.0}{0.0}\n",
      "Minimal Answer & arabic & \\fpr{0.0}{0.0}{0.0}\n",
      "********************\n",
      "arabic\n",
      "Language: arabic (8)\n",
      "********************\n",
      "PASSAGE ANSWER R@P TABLE:\n",
      "Optimal threshold: 0.0\n",
      " F1     /  P      /  R\n",
      "  0.00% /   0.00% /   0.00%\n",
      "R@P=0.5: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "R@P=0.75: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "R@P=0.9: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "********************\n",
      "MINIMAL ANSWER R@P TABLE:\n",
      "Optimal threshold: 0.0\n",
      " F1     /  P      /  R\n",
      "  0.00% /   0.00% /   0.00%\n",
      "R@P=0.5: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "R@P=0.75: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "R@P=0.9: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "Passage & bengali & \\fpr{0.0}{0.0}{0.0}\n",
      "Minimal Answer & bengali & \\fpr{0.0}{0.0}{0.0}\n",
      "********************\n",
      "bengali\n",
      "Language: bengali (4)\n",
      "********************\n",
      "PASSAGE ANSWER R@P TABLE:\n",
      "Optimal threshold: 0.0\n",
      " F1     /  P      /  R\n",
      "  0.00% /   0.00% /   0.00%\n",
      "R@P=0.5: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "R@P=0.75: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "R@P=0.9: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "********************\n",
      "MINIMAL ANSWER R@P TABLE:\n",
      "Optimal threshold: 0.0\n",
      " F1     /  P      /  R\n",
      "  0.00% /   0.00% /   0.00%\n",
      "R@P=0.5: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "R@P=0.75: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "R@P=0.9: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "Passage & finnish & \\fpr{0.0}{0.0}{0.0}\n",
      "Minimal Answer & finnish & \\fpr{0.0}{0.0}{0.0}\n",
      "********************\n",
      "finnish\n",
      "Language: finnish (14)\n",
      "********************\n",
      "PASSAGE ANSWER R@P TABLE:\n",
      "Optimal threshold: 0.0\n",
      " F1     /  P      /  R\n",
      "  0.00% /   0.00% /   0.00%\n",
      "R@P=0.5: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "R@P=0.75: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "R@P=0.9: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "********************\n",
      "MINIMAL ANSWER R@P TABLE:\n",
      "Optimal threshold: 0.0\n",
      " F1     /  P      /  R\n",
      "  0.00% /   0.00% /   0.00%\n",
      "R@P=0.5: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "R@P=0.75: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "R@P=0.9: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "Passage & indonesian & \\fpr{0.0}{0.0}{0.0}\n",
      "Minimal Answer & indonesian & \\fpr{0.0}{0.0}{0.0}\n",
      "********************\n",
      "indonesian\n",
      "Language: indonesian (10)\n",
      "********************\n",
      "PASSAGE ANSWER R@P TABLE:\n",
      "Optimal threshold: 0.0\n",
      " F1     /  P      /  R\n",
      "  0.00% /   0.00% /   0.00%\n",
      "R@P=0.5: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "R@P=0.75: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "R@P=0.9: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "********************\n",
      "MINIMAL ANSWER R@P TABLE:\n",
      "Optimal threshold: 0.0\n",
      " F1     /  P      /  R\n",
      "  0.00% /   0.00% /   0.00%\n",
      "R@P=0.5: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "R@P=0.75: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "R@P=0.9: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "Passage & japanese & \\fpr{0.0}{0.0}{0.0}\n",
      "Minimal Answer & japanese & \\fpr{0.0}{0.0}{0.0}\n",
      "********************\n",
      "japanese\n",
      "Language: japanese (9)\n",
      "********************\n",
      "PASSAGE ANSWER R@P TABLE:\n",
      "Optimal threshold: 0.0\n",
      " F1     /  P      /  R\n",
      "  0.00% /   0.00% /   0.00%\n",
      "R@P=0.5: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "R@P=0.75: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "R@P=0.9: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "********************\n",
      "MINIMAL ANSWER R@P TABLE:\n",
      "Optimal threshold: 0.0\n",
      " F1     /  P      /  R\n",
      "  0.00% /   0.00% /   0.00%\n",
      "R@P=0.5: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "R@P=0.75: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "R@P=0.9: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "Passage & swahili & \\fpr{0.0}{0.0}{0.0}\n",
      "Minimal Answer & swahili & \\fpr{0.0}{0.0}{0.0}\n",
      "********************\n",
      "swahili\n",
      "Language: swahili (13)\n",
      "********************\n",
      "PASSAGE ANSWER R@P TABLE:\n",
      "Optimal threshold: 0.0\n",
      " F1     /  P      /  R\n",
      "  0.00% /   0.00% /   0.00%\n",
      "R@P=0.5: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "R@P=0.75: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "R@P=0.9: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "********************\n",
      "MINIMAL ANSWER R@P TABLE:\n",
      "Optimal threshold: 0.0\n",
      " F1     /  P      /  R\n",
      "  0.00% /   0.00% /   0.00%\n",
      "R@P=0.5: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "R@P=0.75: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "R@P=0.9: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "Passage & korean & \\fpr{0.0}{0.0}{0.0}\n",
      "Minimal Answer & korean & \\fpr{0.0}{0.0}{0.0}\n",
      "********************\n",
      "korean\n",
      "Language: korean (10)\n",
      "********************\n",
      "PASSAGE ANSWER R@P TABLE:\n",
      "Optimal threshold: 0.0\n",
      " F1     /  P      /  R\n",
      "  0.00% /   0.00% /   0.00%\n",
      "R@P=0.5: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "R@P=0.75: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "R@P=0.9: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "********************\n",
      "MINIMAL ANSWER R@P TABLE:\n",
      "Optimal threshold: 0.0\n",
      " F1     /  P      /  R\n",
      "  0.00% /   0.00% /   0.00%\n",
      "R@P=0.5: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "R@P=0.75: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "R@P=0.9: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "Passage & russian & \\fpr{0.0}{0.0}{0.0}\n",
      "Minimal Answer & russian & \\fpr{0.0}{0.0}{0.0}\n",
      "********************\n",
      "russian\n",
      "Language: russian (8)\n",
      "********************\n",
      "PASSAGE ANSWER R@P TABLE:\n",
      "Optimal threshold: 0.0\n",
      " F1     /  P      /  R\n",
      "  0.00% /   0.00% /   0.00%\n",
      "R@P=0.5: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "R@P=0.75: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "R@P=0.9: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "********************\n",
      "MINIMAL ANSWER R@P TABLE:\n",
      "Optimal threshold: 0.0\n",
      " F1     /  P      /  R\n",
      "  0.00% /   0.00% /   0.00%\n",
      "R@P=0.5: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "R@P=0.75: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "R@P=0.9: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "Passage & telugu & \\fpr{0.0}{0.0}{0.0}\n",
      "Minimal Answer & telugu & \\fpr{0.0}{0.0}{0.0}\n",
      "********************\n",
      "telugu\n",
      "Language: telugu (12)\n",
      "********************\n",
      "PASSAGE ANSWER R@P TABLE:\n",
      "Optimal threshold: 0.0\n",
      " F1     /  P      /  R\n",
      "  0.00% /   0.00% /   0.00%\n",
      "R@P=0.5: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "R@P=0.75: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "R@P=0.9: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "********************\n",
      "MINIMAL ANSWER R@P TABLE:\n",
      "Optimal threshold: 0.0\n",
      " F1     /  P      /  R\n",
      "  0.00% /   0.00% /   0.00%\n",
      "R@P=0.5: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "R@P=0.75: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "R@P=0.9: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "Passage & thai & \\fpr{18.2}{16.7}{20.0}\n",
      "Minimal Answer & thai & \\fpr{15.6}{14.3}{17.1}\n",
      "********************\n",
      "thai\n",
      "Language: thai (11)\n",
      "********************\n",
      "PASSAGE ANSWER R@P TABLE:\n",
      "Optimal threshold: -0.039717\n",
      " F1     /  P      /  R\n",
      " 18.18% /  16.67% /  20.00%\n",
      "R@P=0.5: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "R@P=0.75: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "R@P=0.9: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "********************\n",
      "MINIMAL ANSWER R@P TABLE:\n",
      "Optimal threshold: -0.039717\n",
      " F1     /  P      /  R\n",
      " 15.58% /  14.29% /  17.14%\n",
      "R@P=0.5: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "R@P=0.75: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "R@P=0.9: 0.00% (actual p=0.00%, score threshold=0.0)\n",
      "Total # examples in gold: 100, # ex. in pred: 100 (including english)\n",
      "*** Macro Over 10 Languages, excluding English **\n",
      "Passage F1:0.018 P:0.017 R:0.020000\n",
      "\\fpr{1.8}{1.7}{2.0}\n",
      "Minimal F1:0.016 P:0.014 R:0.017143\n",
      "\\fpr{1.6}{1.4}{1.7}\n",
      "*** / Aggregate Scores ****\n",
      "{\"avg_passage_f1\": 0.01818181818181818, \"avg_passage_recall\": 0.02, \"avg_passage_precision\": 0.016666666666666666, \"avg_minimal_f1\": 0.015584415584415584, \"avg_minimal_recall\": 0.017142857142857144, \"avg_minimal_precision\": 0.014285714285714285}\n",
      "***** eval metrics *****\n",
      "  epoch                      =    1.0\n",
      "  eval_avg_minimal_f1        = 0.0156\n",
      "  eval_avg_minimal_precision = 0.0143\n",
      "  eval_avg_minimal_recall    = 0.0171\n",
      "  eval_avg_passage_f1        = 0.0182\n",
      "  eval_avg_passage_precision = 0.0167\n",
      "  eval_avg_passage_recall    =   0.02\n",
      "  eval_samples               =    100\n",
      "Saved rescored validation predictions to FILL_ME_IN/eval_predictions.rescored.json.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "metrics = trainer.evaluate(eval_dataset=eval_dataset, eval_examples=eval_examples)\n",
    "max_eval_samples = max_eval_samples if max_eval_samples else len(eval_dataset)\n",
    "metrics[\"eval_samples\"] = min(max_eval_samples, len(eval_dataset))\n",
    "trainer.log_metrics(\"eval\", metrics)\n",
    "trainer.save_metrics(\"eval\", metrics)\n",
    "\n",
    "for fn in [\"eval_predictions.json\", \"eval_references.json\", \"eval_predictions_processed.json\"]:\n",
    "    if not os.path.exists(os.path.join(training_args.output_dir, fn)):\n",
    "        raise ValueError(f\"Unable to find eval result file {fn} from {training_args.output_dir}.\")\n",
    "\n",
    "confidence_scorer = ConfidenceScorer(task_args['confidence_model_dir'])\n",
    "validation_set_prediction_file = os.path.join(training_args.output_dir, \"eval_predictions.json\")\n",
    "with open(validation_set_prediction_file, 'r') as f:\n",
    "    validation_predictions = json.load(f)\n",
    "\n",
    "for example_id in validation_predictions:\n",
    "    scores = confidence_scorer.predict_scores(validation_predictions[example_id])\n",
    "    for i in range(len(validation_predictions[example_id])):\n",
    "        validation_predictions[example_id][i][\"confidence_score\"] = scores[i]\n",
    "\n",
    "rescored_prediction_file = os.path.join(training_args.output_dir, \"eval_predictions.rescored.json\")\n",
    "with open(rescored_prediction_file, 'w') as f:\n",
    "    json.dump(validation_predictions, f, indent=4)\n",
    "print(f\"Saved rescored validation predictions to {rescored_prediction_file}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb81a266",
   "metadata": {},
   "source": [
    "## Printing Output\n",
    "\n",
    "Here we print out the answers with confidence scores. Please be noted that this notebook is only for the purpose of showing how to train a confidence calibration model. So we select a very small number of examples for both MRC and confidence model training. This leads to the result that most of the confidence scores are low. Users are welcome to modify this notebook to use a larger training set in order to obtain more reasonable confiidence scores. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8fb6ffe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'example_id': 'd309e832-e871-41de-8e40-5b78701c71f4', 'cls_score': -0.13252444937825203, 'start_logit': -0.1879718154668808, 'end_logit': 0.05548543855547905, 'span_answer': {'start_position': 8081, 'end_position': 8089}, 'span_answer_score': 0.15447045117616653, 'start_index': 183, 'end_index': 183, 'passage_index': 16, 'target_type_logits': [0.45754021406173706, 0.3089028298854828, 0.02573591284453869, -0.1720322072505951, -0.5589603781700134], 'span_answer_text': 'ในที่สุด', 'yes_no_answer': 0, 'start_stdev': 0.06726226210594177, 'end_stdev': 0.12430277466773987, 'query_passage_similarity': 1.976719856262207, 'normalized_span_answer_score': 0.3443146476218905, 'confidence_score': 0.006224607667931191}\n",
      "{'example_id': '8711e3e6-b978-45ac-bca8-ffd37b39df72', 'cls_score': 1.1598286777734756, 'start_logit': -0.2032819539308548, 'end_logit': -0.17536720633506775, 'span_answer': {'start_position': 1123, 'end_position': 1126}, 'span_answer_score': -0.46588316559791565, 'start_index': 339, 'end_index': 339, 'passage_index': 3, 'target_type_logits': [0.5697859525680542, 0.6067115068435669, -0.058271367102861404, -0.3915261924266815, -0.5946668982505798], 'span_answer_text': 'hai', 'yes_no_answer': 0, 'start_stdev': 0.04243913292884827, 'end_stdev': 0.0428016223013401, 'query_passage_similarity': 1.971705675125122, 'normalized_span_answer_score': 0.3366914389185114, 'confidence_score': 0.0017059487155688415}\n",
      "{'example_id': '417bf088-917b-4c0a-b11c-1bfd29e3aa78', 'cls_score': -0.04665015637874603, 'start_logit': -0.2423243671655655, 'end_logit': -0.1708858758211136, 'span_answer': {'start_position': 995, 'end_position': 997}, 'span_answer_score': -0.030599884688854218, 'start_index': 34, 'end_index': 34, 'passage_index': 3, 'target_type_logits': [0.47126567363739014, 0.3053603172302246, -0.000588270544540137, -0.22068455815315247, -0.6097366213798523], 'span_answer_text': '80', 'yes_no_answer': 0, 'start_stdev': 0.19806070625782013, 'end_stdev': 0.14382994174957275, 'query_passage_similarity': 1.976982593536377, 'normalized_span_answer_score': 0.334578370801344, 'confidence_score': 0.005257806839521112}\n",
      "{'example_id': '4e0c37c4-8e4f-4520-8055-554e0f3f4878', 'cls_score': 0.2130223661661148, 'start_logit': -0.1676768660545349, 'end_logit': -0.14409320056438446, 'span_answer': {'start_position': 16513, 'end_position': 16517}, 'span_answer_score': -0.03935757279396057, 'start_index': 261, 'end_index': 261, 'passage_index': 48, 'target_type_logits': [0.5215031504631042, 0.44607728719711304, 0.01254031527787447, -0.2821575105190277, -0.6071687340736389], 'span_answer_text': 'rave', 'yes_no_answer': 0, 'start_stdev': 0.12878166139125824, 'end_stdev': 0.11498729139566422, 'query_passage_similarity': 1.9740766286849976, 'normalized_span_answer_score': 0.3355571350112211, 'confidence_score': 0.004473857746316182}\n",
      "{'example_id': '55a4525b-63fd-458d-a075-ac6384dba97f', 'cls_score': 0.9522079527378082, 'start_logit': 0.29996824264526367, 'end_logit': 0.9450535178184509, 'span_answer': {'start_position': 404, 'end_position': 405}, 'span_answer_score': 0.4443587213754654, 'start_index': 119, 'end_index': 119, 'passage_index': 0, 'target_type_logits': [0.723188042640686, 0.5959036350250244, -0.1327202022075653, -0.469977468252182, -0.6050500273704529], 'span_answer_text': '।', 'yes_no_answer': 0, 'start_stdev': 0.2139652967453003, 'end_stdev': 0.2281518578529358, 'query_passage_similarity': 1.9748700857162476, 'normalized_span_answer_score': 0.3400895098782056, 'confidence_score': 0.003146623823792805}\n",
      "{'example_id': '5e4b3daa-16b3-4a98-aa8f-7a96d787a5e7', 'cls_score': 0.15428520739078522, 'start_logit': -0.2061581313610077, 'end_logit': -0.1355806291103363, 'span_answer': {'start_position': 26213, 'end_position': 26287}, 'span_answer_score': -0.05605895072221756, 'start_index': 63, 'end_index': 89, 'passage_index': 48, 'target_type_logits': [0.5042004585266113, 0.3839060664176941, 0.0406869575381279, -0.25371721386909485, -0.5887975096702576], 'span_answer_text': 'кандидоз, гистоплазмоз, криптококкоз, злокачественные опухоли (саркома Кап', 'yes_no_answer': 0, 'start_stdev': 0.09550031274557114, 'end_stdev': 0.1403791308403015, 'query_passage_similarity': 1.973114013671875, 'normalized_span_answer_score': 0.3350493914773396, 'confidence_score': 0.00472906382661663}\n",
      "{'example_id': '10519928-625d-4827-ad16-cf5458eba159', 'cls_score': 0.582994855940342, 'start_logit': -0.0921158641576767, 'end_logit': -0.2009209543466568, 'span_answer': {'start_position': 1634, 'end_position': 1641}, 'span_answer_score': -0.20719443634152412, 'start_index': 270, 'end_index': 272, 'passage_index': 16, 'target_type_logits': [0.5996860265731812, 0.4616428017616272, -0.033828869462013245, -0.3382960259914398, -0.6419457197189331], 'span_answer_text': '378-018', 'yes_no_answer': 0, 'start_stdev': 0.14085669815540314, 'end_stdev': 0.09608014672994614, 'query_passage_similarity': 1.972785234451294, 'normalized_span_answer_score': 0.3374153677448533, 'confidence_score': 0.0034487200397811555}\n",
      "{'example_id': '00ebbb62-3a8d-4bec-9ff8-5eed2a0726f2', 'cls_score': 0.17684802412986755, 'start_logit': -0.20136302709579468, 'end_logit': -0.14034318923950195, 'span_answer': {'start_position': 7090, 'end_position': 7103}, 'span_answer_score': 0.09661771357059479, 'start_index': 206, 'end_index': 211, 'passage_index': 13, 'target_type_logits': [0.6534494161605835, 0.7117896676063538, -0.07534558326005936, -0.42008331418037415, -0.5704401135444641], 'span_answer_text': 'ในปี พ.ศ. 250', 'yes_no_answer': 0, 'start_stdev': 0.21152129769325256, 'end_stdev': 0.1369227170944214, 'query_passage_similarity': 1.9717844724655151, 'normalized_span_answer_score': 0.34282602699629366, 'confidence_score': 0.0027204027089744913}\n",
      "{'example_id': '926a5087-862c-4e77-a65c-59355971c708', 'cls_score': -0.03541959822177887, 'start_logit': -0.14300163090229034, 'end_logit': -0.23507805168628693, 'span_answer': {'start_position': 139, 'end_position': 143}, 'span_answer_score': 0.010046519339084625, 'start_index': 57, 'end_index': 57, 'passage_index': 0, 'target_type_logits': [0.4721713364124298, 0.36275312304496765, 0.03399767354130745, -0.2332189828157425, -0.628407895565033], 'span_answer_text': '3000', 'yes_no_answer': 0, 'start_stdev': 0.03371221944689751, 'end_stdev': 0.01585473120212555, 'query_passage_similarity': 1.9804351329803467, 'normalized_span_answer_score': 0.334510329336885, 'confidence_score': 0.006028581595697015}\n",
      "{'example_id': '3a82f698-da01-4484-ad52-280ab30d94bf', 'cls_score': 0.1505829393863678, 'start_logit': -0.13340501487255096, 'end_logit': -0.17066341638565063, 'span_answer': {'start_position': 693, 'end_position': 698}, 'span_answer_score': -0.056629739701747894, 'start_index': 194, 'end_index': 194, 'passage_index': 2, 'target_type_logits': [0.5193266272544861, 0.3413918912410736, 0.04088452458381653, -0.24098367989063263, -0.5897200703620911], 'span_answer_text': 'kirja', 'yes_no_answer': 0, 'start_stdev': 0.19147805869579315, 'end_stdev': 0.10211571305990219, 'query_passage_similarity': 1.9713815450668335, 'normalized_span_answer_score': 0.3340776307539364, 'confidence_score': 0.0046439359661336315}\n",
      "{'example_id': '8688cb6e-173a-4c26-b737-947ee362e3ca', 'cls_score': 0.872747465968132, 'start_logit': -0.18780691921710968, 'end_logit': -0.14462631940841675, 'span_answer': {'start_position': 4950, 'end_position': 4985}, 'span_answer_score': -0.2474333941936493, 'start_index': 279, 'end_index': 290, 'passage_index': 8, 'target_type_logits': [0.6339309811592102, 0.7103139162063599, -0.1429702192544937, -0.4846093952655792, -0.5873103737831116], 'span_answer_text': 'الدقيقة 70، 73 و78)، وهو أول هاتريك', 'yes_no_answer': 0, 'start_stdev': 0.0725446492433548, 'end_stdev': 0.1482936441898346, 'query_passage_similarity': 1.9730725288391113, 'normalized_span_answer_score': 0.33436677153656225, 'confidence_score': 0.0018622217685113054}\n",
      "{'example_id': '7acea3a2-7210-4ef5-8d02-e2f9ea5f5649', 'cls_score': 0.8342375159263611, 'start_logit': -0.22468842566013336, 'end_logit': -0.08512241393327713, 'span_answer': {'start_position': 122, 'end_position': 169}, 'span_answer_score': -0.30872587487101555, 'start_index': 45, 'end_index': 55, 'passage_index': 1, 'target_type_logits': [0.6524236798286438, 0.5265966057777405, -0.09502659738063812, -0.3143152892589569, -0.572206437587738], 'span_answer_text': 'III, (inayojulikana kama Bill Gates) mwaka 1975', 'yes_no_answer': 0, 'start_stdev': 0.12714485824108124, 'end_stdev': 0.044241294264793396, 'query_passage_similarity': 1.9796801805496216, 'normalized_span_answer_score': 0.33487046154503886, 'confidence_score': 0.0028233602469207525}\n",
      "{'example_id': 'e806b39f-c86a-4b1c-b27d-7d11b18ebf34', 'cls_score': 0.8679958879947662, 'start_logit': -0.17754201591014862, 'end_logit': -0.1565588265657425, 'span_answer': {'start_position': 211, 'end_position': 243}, 'span_answer_score': -0.3248273879289627, 'start_index': 61, 'end_index': 66, 'passage_index': 0, 'target_type_logits': [0.636605441570282, 0.5524419546127319, -0.08112793415784836, -0.31027138233184814, -0.6435977220535278], 'span_answer_text': '1492. Perjalanan tersebut didana', 'yes_no_answer': 0, 'start_stdev': 0.16006237268447876, 'end_stdev': 0.04949251934885979, 'query_passage_similarity': 1.9733835458755493, 'normalized_span_answer_score': 0.3360151102883371, 'confidence_score': 0.002770908571126341}\n",
      "{'example_id': '0b8350cc-9da3-45a5-8bbd-3d23a0683dd6', 'cls_score': -0.0010800063610076904, 'start_logit': -0.17640481889247894, 'end_logit': -0.2684200406074524, 'span_answer': {'start_position': 4087, 'end_position': 4129}, 'span_answer_score': -0.08867361396551132, 'start_index': 319, 'end_index': 344, 'passage_index': 18, 'target_type_logits': [0.48701971769332886, 0.266397625207901, 0.018426379188895226, -0.16185221076011658, -0.5702910423278809], 'span_answer_text': '인터뷰에서 지존파, 온보현, 유영철 등의 연쇄살인범들의 배경에는 뒤틀린 심리', 'yes_no_answer': 0, 'start_stdev': 0.15005965530872345, 'end_stdev': 0.1345374435186386, 'query_passage_similarity': 1.9777390956878662, 'normalized_span_answer_score': 0.3347546477535173, 'confidence_score': 0.005362301137676697}\n",
      "{'example_id': 'f57c6e18-7f63-46d9-aae8-6f13d7d94d5f', 'cls_score': 0.28218962252140045, 'start_logit': -0.08146612346172333, 'end_logit': -0.17823956906795502, 'span_answer': {'start_position': 17874, 'end_position': 17935}, 'span_answer_score': -0.10302124172449112, 'start_index': 472, 'end_index': 489, 'passage_index': 47, 'target_type_logits': [0.5052830576896667, 0.33585283160209656, -0.00619552144780755, -0.2644665837287903, -0.583211362361908], 'span_answer_text': 'namespace. Use {{lang-en}} or {{en icon}} instead.\\nJokke Saha', 'yes_no_answer': 0, 'start_stdev': 0.1722831428050995, 'end_stdev': 0.17900756001472473, 'query_passage_similarity': 1.962996006011963, 'normalized_span_answer_score': 0.34203236252193464, 'confidence_score': 0.004323407635058062}\n",
      "{'example_id': '265745bc-c43d-49dc-9635-b5703bc4edda', 'cls_score': -0.021118327975273132, 'start_logit': -0.19567959010601044, 'end_logit': -0.12547971308231354, 'span_answer': {'start_position': 7307, 'end_position': 7400}, 'span_answer_score': -0.015455268323421478, 'start_index': 124, 'end_index': 146, 'passage_index': 10, 'target_type_logits': [0.4005970358848572, 0.2691304385662079, 0.05623661354184151, -0.1529119908809662, -0.5360077023506165], 'span_answer_text': 'применении водорастворимых красителей «кубовой» природы являются разработанные в СССР в 1980-', 'yes_no_answer': 0, 'start_stdev': 0.1614636480808258, 'end_stdev': 0.15862852334976196, 'query_passage_similarity': 1.9726500511169434, 'normalized_span_answer_score': 0.33862916047547603, 'confidence_score': 0.005444168321526129}\n",
      "{'example_id': 'f2665946-e6e0-47c9-ae1c-1624a335d735', 'cls_score': 0.6598522514104843, 'start_logit': -0.1618855893611908, 'end_logit': -0.1988346129655838, 'span_answer': {'start_position': 941, 'end_position': 1007}, 'span_answer_score': -0.2631895989179611, 'start_index': 257, 'end_index': 277, 'passage_index': 4, 'target_type_logits': [0.6491990685462952, 0.49419325590133667, -0.08083286136388779, -0.29885512590408325, -0.5884626507759094], 'span_answer_text': 'รินเตอร์อิงค์เจ็ต เลเซอร์พรินเตอร์ รวมถึงสแกนเนอร์ เมื่อ ค.ศ. 1984', 'yes_no_answer': 0, 'start_stdev': 0.13558349013328552, 'end_stdev': 0.12667980790138245, 'query_passage_similarity': 1.9755034446716309, 'normalized_span_answer_score': 0.33486445300876705, 'confidence_score': 0.00320738178974852}\n",
      "{'example_id': '3359def7-a33f-4221-9bcc-a391c337a492', 'cls_score': 0.6350763738155365, 'start_logit': -0.15531255304813385, 'end_logit': -0.12892785668373108, 'span_answer': {'start_position': 607, 'end_position': 621}, 'span_answer_score': -0.1975146308541298, 'start_index': 17, 'end_index': 26, 'passage_index': 3, 'target_type_logits': [0.684544026851654, 0.5242875218391418, -0.026882627978920937, -0.37753045558929443, -0.6183912754058838], 'span_answer_text': 'くに第二次世界大戦後は、米ソ', 'yes_no_answer': 0, 'start_stdev': 0.1906595677137375, 'end_stdev': 0.12181728333234787, 'query_passage_similarity': 1.9746609926223755, 'normalized_span_answer_score': 0.34168148128084164, 'confidence_score': 0.0030770994216480067}\n",
      "{'example_id': '31a51659-4a04-45f9-9b35-3e4da43a71dc', 'cls_score': 0.4479604586958885, 'start_logit': -0.21268479526042938, 'end_logit': -0.26816701889038086, 'span_answer': {'start_position': 11647, 'end_position': 11678}, 'span_answer_score': -0.09397799149155617, 'start_index': 14, 'end_index': 21, 'passage_index': 29, 'target_type_logits': [0.6325287222862244, 0.7408562898635864, -0.07131221145391464, -0.4167902171611786, -0.5527201890945435], 'span_answer_text': 'ائل عام 2009 أنها ستحاول التحول', 'yes_no_answer': 0, 'start_stdev': 0.05080670118331909, 'end_stdev': 0.12031910568475723, 'query_passage_similarity': 1.9684009552001953, 'normalized_span_answer_score': 0.3359618813693299, 'confidence_score': 0.0024905353483530247}\n",
      "{'example_id': '845c686a-0319-4f24-b053-39958da79658', 'cls_score': -0.181381955742836, 'start_logit': -0.2029242366552353, 'end_logit': -0.23724734783172607, 'span_answer': {'start_position': 1404, 'end_position': 1447}, 'span_answer_score': 0.008348613977432251, 'start_index': 447, 'end_index': 458, 'passage_index': 2, 'target_type_logits': [0.43712902069091797, 0.27548685669898987, 0.03732962906360626, -0.18666407465934753, -0.5656370520591736], 'span_answer_text': 'కుటుంబం వారు బొడ్డుచెర్ల గ్రామానికి ఉత్తరది', 'yes_no_answer': 0, 'start_stdev': 0.02484164945781231, 'end_stdev': 0.12269618362188339, 'query_passage_similarity': 1.9809775352478027, 'normalized_span_answer_score': 0.33595064135610975, 'confidence_score': 0.006708960666395413}\n",
      "{'example_id': '552e7963-44e7-4ffe-bf03-b0ebe1ab0e6d', 'cls_score': 0.20897546038031578, 'start_logit': -0.14030204713344574, 'end_logit': -0.09804832935333252, 'span_answer': {'start_position': 5154, 'end_position': 5273}, 'span_answer_score': -0.03952648304402828, 'start_index': 257, 'end_index': 283, 'passage_index': -1, 'target_type_logits': [0.5391961336135864, 0.3682728707790375, 0.004080539103597403, -0.25420400500297546, -0.6378421187400818], 'span_answer_text': 'yhtye itse ja ensimmäisen Miikka Lommi.\\nHell of a Collection -kokoelma-albumi (2001)\\nInton menestyksen myötä The Rasmus', 'yes_no_answer': 0, 'start_stdev': 0.17728005349636078, 'end_stdev': 0.07294851541519165, 'query_passage_similarity': 1.9721752405166626, 'normalized_span_answer_score': 0.33545239315490744, 'confidence_score': 0.004431902828661582}\n",
      "{'example_id': '7dab0e28-9e7e-4528-9aa8-b30cb1fb2c7e', 'cls_score': 0.8089719414710999, 'start_logit': -0.14614880084991455, 'end_logit': -0.11693479865789413, 'span_answer': {'start_position': 292, 'end_position': 411}, 'span_answer_score': -0.24283812567591667, 'start_index': 78, 'end_index': 101, 'passage_index': -1, 'target_type_logits': [0.6928963661193848, 0.5863792896270752, -0.13898147642612457, -0.39507636427879333, -0.6220007538795471], 'span_answer_text': '837, wameutumia uwanja huu tangu klabu ilivyoanzishwa.\\nChelsea, kwa mara ya kwanza ilipata mafanikio makubwa mwaka 1955', 'yes_no_answer': 0, 'start_stdev': 0.13182929158210754, 'end_stdev': 0.11683323234319687, 'query_passage_similarity': 1.9733701944351196, 'normalized_span_answer_score': 0.33709851382150724, 'confidence_score': 0.0028508992192352767}\n",
      "{'example_id': '74334648-ac41-4d2f-98a0-cda3bb75c516', 'cls_score': 0.7780117057263851, 'start_logit': 0.2247537523508072, 'end_logit': 1.1477742195129395, 'span_answer': {'start_position': 5872, 'end_position': 5876}, 'span_answer_score': 0.5827940646559, 'start_index': 108, 'end_index': 108, 'passage_index': 5, 'target_type_logits': [0.6536763310432434, 0.5710718631744385, -0.05674181133508682, -0.31467029452323914, -0.6283876299858093], 'span_answer_text': '[32]', 'yes_no_answer': 0, 'start_stdev': 0.32439953088760376, 'end_stdev': 0.20803984999656677, 'query_passage_similarity': 1.969970703125, 'normalized_span_answer_score': 0.43842798598540983, 'confidence_score': 0.0033782009491411065}\n",
      "{'example_id': '8dda8975-ea62-4fe3-ae8b-2192662af720', 'cls_score': 0.38305288925766945, 'start_logit': -0.13947558403015137, 'end_logit': -0.14609235525131226, 'span_answer': {'start_position': 138, 'end_position': 142}, 'span_answer_score': -0.133632006123662, 'start_index': 55, 'end_index': 55, 'passage_index': 0, 'target_type_logits': [0.5605286955833435, 0.4013568162918091, -0.02525629661977291, -0.29093965888023376, -0.6653935313224792], 'span_answer_text': '1963', 'yes_no_answer': 0, 'start_stdev': 0.1634020209312439, 'end_stdev': 0.09796058386564255, 'query_passage_similarity': 1.9738798141479492, 'normalized_span_answer_score': 0.3388065354744971, 'confidence_score': 0.003886383220042976}\n",
      "{'example_id': '3f9fe7f6-a515-4de3-8c12-1707543fbeba', 'cls_score': -0.0013629794120788574, 'start_logit': -0.14919571578502655, 'end_logit': 0.28615880012512207, 'span_answer': {'start_position': 8813, 'end_position': 8814}, 'span_answer_score': 0.22176074236631393, 'start_index': 310, 'end_index': 310, 'passage_index': 21, 'target_type_logits': [0.46915704011917114, 0.3051954209804535, -0.001442084787413478, -0.1920212209224701, -0.5725768804550171], 'span_answer_text': 'ภ', 'yes_no_answer': 0, 'start_stdev': 0.2331610769033432, 'end_stdev': 0.1176430955529213, 'query_passage_similarity': 1.9757522344589233, 'normalized_span_answer_score': 0.36438847806522895, 'confidence_score': 0.005152781701685747}\n",
      "{'example_id': 'dea4fa14-01f7-42d8-b8df-c4278573c0d4', 'cls_score': 0.43837738782167435, 'start_logit': 0.047649405896663666, 'end_logit': 0.3907444477081299, 'span_answer': {'start_position': 2983, 'end_position': 2984}, 'span_answer_score': 0.18481039255857468, 'start_index': 253, 'end_index': 253, 'passage_index': 10, 'target_type_logits': [0.515014111995697, 0.36960431933403015, -0.011648419313132763, -0.27452537417411804, -0.6234394907951355], 'span_answer_text': 'ョ', 'yes_no_answer': 0, 'start_stdev': 0.10193432122468948, 'end_stdev': 0.06301937252283096, 'query_passage_similarity': 1.9710121154785156, 'normalized_span_answer_score': 0.385027211413307, 'confidence_score': 0.004202012497326542}\n",
      "{'example_id': 'aae79851-78aa-4d7e-a204-72b049b28ffe', 'cls_score': 0.1790880411863327, 'start_logit': -0.143996462225914, 'end_logit': -0.18936681747436523, 'span_answer': {'start_position': 41497, 'end_position': 41516}, 'span_answer_score': -0.024155616760253906, 'start_index': 352, 'end_index': 357, 'passage_index': -1, 'target_type_logits': [0.56056809425354, 0.4641400873661041, -0.021901438012719154, -0.29123154282569885, -0.6128155589103699], 'span_answer_text': '53]\\nالجمعة 8/4/2011', 'yes_no_answer': 0, 'start_stdev': 0.04056373983621597, 'end_stdev': 0.05060403421521187, 'query_passage_similarity': 1.9721989631652832, 'normalized_span_answer_score': 0.3360990758266211, 'confidence_score': 0.0048856465192852975}\n",
      "{'example_id': '73bfa10d-37b4-4397-936b-94e0faf83681', 'cls_score': -0.20820394516340457, 'start_logit': -0.12217611819505692, 'end_logit': -0.2483244091272354, 'span_answer': {'start_position': 10022, 'end_position': 10079}, 'span_answer_score': 0.01723507598217111, 'start_index': 18, 'end_index': 37, 'passage_index': -1, 'target_type_logits': [0.3970170319080353, 0.19676673412322998, 0.04760611429810524, -0.12618282437324524, -0.5451856851577759], 'span_answer_text': 'శ్రమ భారము తగ్గింది. \\nఈ ఆవిష్కరణపై అభిప్రాయాలు\\nచారి ఫార్మ', 'yes_no_answer': 0, 'start_stdev': 0.12098279595375061, 'end_stdev': 0.14994694292545319, 'query_passage_similarity': 1.9738502502441406, 'normalized_span_answer_score': 0.33550933232746366, 'confidence_score': 0.00681931041713158}\n",
      "{'example_id': 'e0660e6a-1721-4d74-9001-111b14132822', 'cls_score': 0.26419465988874435, 'start_logit': -0.11370867490768433, 'end_logit': -0.1806640625, 'span_answer': {'start_position': 3309, 'end_position': 3313}, 'span_answer_score': -0.0461474172770977, 'start_index': 102, 'end_index': 102, 'passage_index': 7, 'target_type_logits': [0.599522054195404, 0.4662725627422333, 0.02713041566312313, -0.30562636256217957, -0.599168598651886], 'span_answer_text': 'hine', 'yes_no_answer': 0, 'start_stdev': 0.13286162912845612, 'end_stdev': 0.10808549076318741, 'query_passage_similarity': 1.9741199016571045, 'normalized_span_answer_score': 0.33631640421732223, 'confidence_score': 0.004292904773245258}\n",
      "{'example_id': '842aff3e-f0a5-476b-a147-6601851c82da', 'cls_score': 0.7361651957035065, 'start_logit': -0.16762571036815643, 'end_logit': -0.1726813167333603, 'span_answer': {'start_position': 1954, 'end_position': 2068}, 'span_answer_score': -0.30739931762218475, 'start_index': 142, 'end_index': 165, 'passage_index': -1, 'target_type_logits': [0.6135383248329163, 0.4616735875606537, -0.023709939792752266, -0.3066232204437256, -0.6092126965522766], 'span_answer_text': \"template. See the documentation for available templates.\\n\\n at Prison Break's official site\\n\\nJamii:Waliozaliwa 1977\", 'yes_no_answer': 0, 'start_stdev': 0.2172478437423706, 'end_stdev': 0.10775381326675415, 'query_passage_similarity': 1.9724836349487305, 'normalized_span_answer_score': 0.3354347032720835, 'confidence_score': 0.002933369404784488}\n",
      "{'example_id': '47f1054f-f909-4abc-b757-487fc76a694a', 'cls_score': 1.3103211671113968, 'start_logit': -0.07655902206897736, 'end_logit': -0.04565904662013054, 'span_answer': {'start_position': 44, 'end_position': 48}, 'span_answer_score': -0.3542454894632101, 'start_index': 23, 'end_index': 23, 'passage_index': 0, 'target_type_logits': [0.7107779383659363, 0.7240482568740845, -0.12853692471981049, -0.4063705503940582, -0.5920934081077576], 'span_answer_text': '1890', 'yes_no_answer': 0, 'start_stdev': 0.1871616691350937, 'end_stdev': 0.1574217975139618, 'query_passage_similarity': 1.9723176956176758, 'normalized_span_answer_score': 0.34061578201001436, 'confidence_score': 0.0014876869746974625}\n",
      "{'example_id': 'c4521b21-4dff-46b3-afd0-473c8ef117ec', 'cls_score': 0.05737452208995819, 'start_logit': -0.126859650015831, 'end_logit': -0.1604817658662796, 'span_answer': {'start_position': 869, 'end_position': 925}, 'span_answer_score': -0.07307015359401703, 'start_index': 327, 'end_index': 348, 'passage_index': -1, 'target_type_logits': [0.45367053151130676, 0.19857563078403473, 0.015352924354374409, -0.1466754674911499, -0.5438522696495056], 'span_answer_text': 'চিত্রগ্রহণ\\nপরবর্তী-প্রযোজনা\\nসঙ্গীত\\nবিপণন\\nমুক্তি\\nথিয়েটার', 'yes_no_answer': 0, 'start_stdev': 0.13199247419834137, 'end_stdev': 0.08812432736158371, 'query_passage_similarity': 1.9775253534317017, 'normalized_span_answer_score': 0.34360394662168514, 'confidence_score': 0.005314367029539461}\n",
      "{'example_id': '3ec12ed9-f961-4cd7-83ad-4683e8e80dd0', 'cls_score': 0.20148985087871552, 'start_logit': -0.16605795919895172, 'end_logit': -0.21210578083992004, 'span_answer': {'start_position': 4046, 'end_position': 4091}, 'span_answer_score': -0.1395864486694336, 'start_index': 458, 'end_index': 484, 'passage_index': 17, 'target_type_logits': [0.48984086513519287, 0.3004806935787201, 0.04400057718157768, -0.22130315005779266, -0.6024855971336365], 'span_answer_text': 'RNAはDNAに比べて不安定」である。両者の安定の度合いの違いが、DNAは静的でRNAは動', 'yes_no_answer': 0, 'start_stdev': 0.11321225762367249, 'end_stdev': 0.0558452233672142, 'query_passage_similarity': 1.970852017402649, 'normalized_span_answer_score': 0.33826512011489246, 'confidence_score': 0.004735915407055218}\n",
      "{'example_id': '6de2d246-0976-46c7-9e5b-71962f74989a', 'cls_score': -0.12740372121334076, 'start_logit': -0.2542421817779541, 'end_logit': -0.17321829497814178, 'span_answer': {'start_position': 16596, 'end_position': 16655}, 'span_answer_score': -0.010504037141799927, 'start_index': 308, 'end_index': 324, 'passage_index': 30, 'target_type_logits': [0.4577885866165161, 0.2790486812591553, 0.022248802706599236, -0.15157026052474976, -0.5318469405174255], 'span_answer_text': 'นานหลังจากที่ผู้รับเหมาติดตั้งระบบ SCADA ในเดือนมกราคม 2000', 'yes_no_answer': 0, 'start_stdev': 0.20017434656620026, 'end_stdev': 0.025809332728385925, 'query_passage_similarity': 1.975723147392273, 'normalized_span_answer_score': 0.33904790554356923, 'confidence_score': 0.005757337218839798}\n",
      "{'example_id': 'a63d6e33-75e4-4f6b-920d-6fed81718c16', 'cls_score': 0.09990309178829193, 'start_logit': -0.21510854363441467, 'end_logit': -0.2528725266456604, 'span_answer': {'start_position': 6637, 'end_position': 6658}, 'span_answer_score': -0.08943849056959152, 'start_index': 318, 'end_index': 323, 'passage_index': 22, 'target_type_logits': [0.498746395111084, 0.38900718092918396, 0.0369856096804142, -0.32126304507255554, -0.6181672215461731], 'span_answer_text': 'Stuart Godfrey. 2003.', 'yes_no_answer': 0, 'start_stdev': 0.16497699916362762, 'end_stdev': 0.14364396035671234, 'query_passage_similarity': 1.977576494216919, 'normalized_span_answer_score': 0.33378736986286656, 'confidence_score': 0.004754096835652394}\n",
      "{'example_id': '457fc29a-0f70-4685-9b9c-fbb2e279536d', 'cls_score': 0.037251368165016174, 'start_logit': -0.18179820477962494, 'end_logit': -0.22040636837482452, 'span_answer': {'start_position': 42948, 'end_position': 43047}, 'span_answer_score': -0.05340879410505295, 'start_index': 99, 'end_index': 120, 'passage_index': -1, 'target_type_logits': [0.4690646529197693, 0.33263835310935974, 0.046359483152627945, -0.22835390269756317, -0.582570493221283], 'span_answer_text': 'Painotusalat\\nBio-, terveys- ja ympäristötieteet sekä niihin liittyvä teknologia (erityisesti sensor', 'yes_no_answer': 0, 'start_stdev': 0.14231085777282715, 'end_stdev': 0.1059986874461174, 'query_passage_similarity': 1.9752137660980225, 'normalized_span_answer_score': 0.3386640207732333, 'confidence_score': 0.005207962138622825}\n",
      "{'example_id': '6c8c0fac-7b26-4892-a6e0-1a63eee5929d', 'cls_score': 0.7434437721967697, 'start_logit': -0.19264601171016693, 'end_logit': -0.18007947504520416, 'span_answer': {'start_position': 1288, 'end_position': 1324}, 'span_answer_score': -0.1507030948996544, 'start_index': 45, 'end_index': 55, 'passage_index': 1, 'target_type_logits': [0.6515344381332397, 0.814763069152832, -0.05895257368683815, -0.5019306540489197, -0.526472270488739], 'span_answer_text': 'المعادن القليلة التي تفوقه في الصلاب', 'yes_no_answer': 0, 'start_stdev': 0.1606624275445938, 'end_stdev': 0.12291757762432098, 'query_passage_similarity': 1.9712276458740234, 'normalized_span_answer_score': 0.33639907830743576, 'confidence_score': 0.0019102656578958283}\n",
      "{'example_id': '28850b64-bc71-4f45-ad89-dae29a7bcfaf', 'cls_score': 1.122559055685997, 'start_logit': -0.2189585417509079, 'end_logit': -0.17816905677318573, 'span_answer': {'start_position': 2238, 'end_position': 2263}, 'span_answer_score': -0.43548179417848587, 'start_index': 220, 'end_index': 225, 'passage_index': 9, 'target_type_logits': [0.6163217425346375, 0.6487230658531189, -0.09472733736038208, -0.4651685059070587, -0.532999575138092], 'span_answer_text': 'nne 1999 Collins (en chap', 'yes_no_answer': 0, 'start_stdev': 0.058195456862449646, 'end_stdev': 0.1434423327445984, 'query_passage_similarity': 1.9779243469238281, 'normalized_span_answer_score': 0.33624998096979625, 'confidence_score': 0.00164776432789453}\n",
      "{'example_id': '79ea2530-07bd-4f62-b152-3e047481ce29', 'cls_score': 0.355292484164238, 'start_logit': -0.1829746961593628, 'end_logit': -0.2493823617696762, 'span_answer': {'start_position': 295, 'end_position': 436}, 'span_answer_score': -0.16547523438930511, 'start_index': 74, 'end_index': 98, 'passage_index': -1, 'target_type_logits': [0.5721105933189392, 0.45669907331466675, 0.02584652416408062, -0.33546945452690125, -0.5542472004890442], 'span_answer_text': 'grey water).[1]\\nLimbah padat lebih dikenal sebagai sampah, yang seringkali tidak dikehendaki kehadirannya karena tidak memiliki nilai ekonomi', 'yes_no_answer': 0, 'start_stdev': 0.16680322587490082, 'end_stdev': 0.10434027761220932, 'query_passage_similarity': 1.9727102518081665, 'normalized_span_answer_score': 0.33894621586466234, 'confidence_score': 0.0027449020433120905}\n",
      "{'example_id': '520a4a7d-b356-488b-8bd3-92a7e54571bd', 'cls_score': 0.07843943685293198, 'start_logit': -0.09052463620901108, 'end_logit': -0.26409801840782166, 'span_answer': {'start_position': 15, 'end_position': 19}, 'span_answer_score': -0.0721655860543251, 'start_index': 27, 'end_index': 27, 'passage_index': 0, 'target_type_logits': [0.5108317136764526, 0.2887309193611145, 0.005046339239925146, -0.1764836311340332, -0.5974364876747131], 'span_answer_text': '1946', 'yes_no_answer': 0, 'start_stdev': 0.15668971836566925, 'end_stdev': 0.13637353479862213, 'query_passage_similarity': 1.9761488437652588, 'normalized_span_answer_score': 0.3354857737038913, 'confidence_score': 0.00511221887404559}\n",
      "{'example_id': '6fa86cac-4253-443d-9dec-2da1195b9c69', 'cls_score': 0.1785745620727539, 'start_logit': -0.16009172797203064, 'end_logit': -0.22186599671840668, 'span_answer': {'start_position': 5896, 'end_position': 5907}, 'span_answer_score': -0.13051524013280869, 'start_index': 357, 'end_index': 363, 'passage_index': 18, 'target_type_logits': [0.5367642641067505, 0.29950180649757385, 0.021427223458886147, -0.20496030151844025, -0.6258746385574341], 'span_answer_text': '204년 겨우 31세', 'yes_no_answer': 0, 'start_stdev': 0.08093620836734772, 'end_stdev': 0.09138837456703186, 'query_passage_similarity': 1.9775338172912598, 'normalized_span_answer_score': 0.3374458885198794, 'confidence_score': 0.004783150080206097}\n",
      "{'example_id': 'a7b7d04c-3100-43ee-b773-3f4664e9f545', 'cls_score': 0.17912808060646057, 'start_logit': -0.19118569791316986, 'end_logit': -0.15970855951309204, 'span_answer': {'start_position': 24180, 'end_position': 24183}, 'span_answer_score': -0.09435486048460007, 'start_index': 63, 'end_index': 63, 'passage_index': 47, 'target_type_logits': [0.5195940136909485, 0.34131261706352234, -0.029337817803025246, -0.23721382021903992, -0.6040759682655334], 'span_answer_text': 'XIX', 'yes_no_answer': 0, 'start_stdev': 0.11632867157459259, 'end_stdev': 0.1538608968257904, 'query_passage_similarity': 1.9726301431655884, 'normalized_span_answer_score': 0.3388265651076747, 'confidence_score': 0.004599525631737208}\n",
      "{'example_id': 'c4d951fc-8692-4820-a520-5b2ff9bd8e0d', 'cls_score': 0.03199368715286255, 'start_logit': -0.11920757591724396, 'end_logit': 0.30041006207466125, 'span_answer': {'start_position': 11945, 'end_position': 11946}, 'span_answer_score': 0.19919077306985855, 'start_index': 200, 'end_index': 200, 'passage_index': 28, 'target_type_logits': [0.5139358043670654, 0.24917274713516235, 0.011903380043804646, -0.17410218715667725, -0.6214012503623962], 'span_answer_text': '。', 'yes_no_answer': 0, 'start_stdev': 0.03400430083274841, 'end_stdev': 0.17010998725891113, 'query_passage_similarity': 1.9715306758880615, 'normalized_span_answer_score': 0.36597368157800264, 'confidence_score': 0.005585208322084493}\n",
      "{'example_id': 'd8c06faa-4fba-4369-ae4b-3738002f1efb', 'cls_score': -0.02985122799873352, 'start_logit': -0.14231432974338531, 'end_logit': -0.28396493196487427, 'span_answer': {'start_position': 4190, 'end_position': 4217}, 'span_answer_score': -0.05328542739152908, 'start_index': 456, 'end_index': 463, 'passage_index': 7, 'target_type_logits': [0.4387585520744324, 0.2898571789264679, 0.06551069021224976, -0.16319911181926727, -0.5276175141334534], 'span_answer_text': '3,890 ตัว กระจายทั่วไปใน 13', 'yes_no_answer': 0, 'start_stdev': 0.16461524367332458, 'end_stdev': 0.11809137463569641, 'query_passage_similarity': 1.976351022720337, 'normalized_span_answer_score': 0.3344257327029806, 'confidence_score': 0.005582417472469645}\n",
      "{'example_id': '7c4b01de-32c9-42d4-bd5c-bb8b3a0d4a00', 'cls_score': 0.2945246770977974, 'start_logit': -0.17494051158428192, 'end_logit': -0.20050501823425293, 'span_answer': {'start_position': 6746, 'end_position': 6749}, 'span_answer_score': -0.04816010221838951, 'start_index': 159, 'end_index': 159, 'passage_index': 10, 'target_type_logits': [0.5862521529197693, 0.5736500024795532, 0.004542095120996237, -0.42200925946235657, -0.6273640990257263], 'span_answer_text': '198', 'yes_no_answer': 0, 'start_stdev': 0.19215528666973114, 'end_stdev': 0.10231685638427734, 'query_passage_similarity': 1.9773962497711182, 'normalized_span_answer_score': 0.33566125671070857, 'confidence_score': 0.0039721510870820405}\n",
      "{'example_id': '7ce27bb2-9285-472e-a202-a569dc2a8d49', 'cls_score': 0.7675100564956665, 'start_logit': -0.16514472663402557, 'end_logit': -0.22131629288196564, 'span_answer': {'start_position': 5746, 'end_position': 5837}, 'span_answer_score': -0.2642810046672821, 'start_index': 467, 'end_index': 491, 'passage_index': 11, 'target_type_logits': [0.6220139861106873, 0.6254090666770935, -0.07900568842887878, -0.5051496028900146, -0.5382346510887146], 'span_answer_text': 'yhteyden Viinijärvi–Keretti. Tehorajaksi oli tuotantolaitokselle vuonna 1981 määrätty 6,7MW', 'yes_no_answer': 0, 'start_stdev': 0.17487046122550964, 'end_stdev': 0.14094462990760803, 'query_passage_similarity': 1.9710850715637207, 'normalized_span_answer_score': 0.3345287684456165, 'confidence_score': 0.0018999850030954346}\n",
      "{'example_id': 'aeffa6b1-7992-4ab8-ab10-fe8dfb468636', 'cls_score': 0.46661414206027985, 'start_logit': -0.1589783877134323, 'end_logit': -0.1471101939678192, 'span_answer': {'start_position': 3836, 'end_position': 3872}, 'span_answer_score': -0.096906378865242, 'start_index': 213, 'end_index': 222, 'passage_index': -1, 'target_type_logits': [0.5872231125831604, 0.5788899660110474, -0.08831687271595001, -0.4120281934738159, -0.5996041297912598], 'span_answer_text': 'biji.\\nNchi hizi zimepitiwa na mstari', 'yes_no_answer': 0, 'start_stdev': 0.1308203637599945, 'end_stdev': 0.07669250667095184, 'query_passage_similarity': 1.9750303030014038, 'normalized_span_answer_score': 0.3438972755468561, 'confidence_score': 0.0036651489595349775}\n",
      "{'example_id': 'afa4767c-e508-4ac1-9cf5-547ea5be2f4d', 'cls_score': 1.1757859885692596, 'start_logit': 0.3445588946342468, 'end_logit': 1.0784741640090942, 'span_answer': {'start_position': 817, 'end_position': 818}, 'span_answer_score': 0.4713427871465683, 'start_index': 196, 'end_index': 196, 'passage_index': 3, 'target_type_logits': [0.6541728973388672, 0.6954385042190552, -0.05782506242394447, -0.3893885314464569, -0.5680907368659973], 'span_answer_text': 'P', 'yes_no_answer': 0, 'start_stdev': 0.3012639284133911, 'end_stdev': 0.19767065346240997, 'query_passage_similarity': 1.970974326133728, 'normalized_span_answer_score': 0.48736190613374397, 'confidence_score': 0.0019535372588842044}\n",
      "{'example_id': '66c3133a-da7c-405b-8e83-c22619429b94', 'cls_score': 0.13473562896251678, 'start_logit': -0.2332793027162552, 'end_logit': -0.21855510771274567, 'span_answer': {'start_position': 7181, 'end_position': 7185}, 'span_answer_score': -0.1584489420056343, 'start_index': 39, 'end_index': 39, 'passage_index': 28, 'target_type_logits': [0.4707821011543274, 0.269672155380249, 0.009788871742784977, -0.20754790306091309, -0.550044059753418], 'span_answer_text': '2002', 'yes_no_answer': 0, 'start_stdev': 0.10409414768218994, 'end_stdev': 0.07747364044189453, 'query_passage_similarity': 1.9667733907699585, 'normalized_span_answer_score': 0.33557589307025665, 'confidence_score': 0.004926287144279182}\n",
      "{'example_id': '614dfc0d-8be3-464d-8306-ee3490547d11', 'cls_score': 0.2300143614411354, 'start_logit': -0.2571955919265747, 'end_logit': -0.20053423941135406, 'span_answer': {'start_position': 1069, 'end_position': 1084}, 'span_answer_score': -0.14161380752921104, 'start_index': 264, 'end_index': 270, 'passage_index': 6, 'target_type_logits': [0.5801949501037598, 0.4045165777206421, -0.02049420215189457, -0.28385043144226074, -0.5760557055473328], 'span_answer_text': '3.7-9.2 km)로 이에', 'yes_no_answer': 0, 'start_stdev': 0.15294866263866425, 'end_stdev': 0.1691409796476364, 'query_passage_similarity': 1.980401635169983, 'normalized_span_answer_score': 0.33857916848692016, 'confidence_score': 0.004133937916314551}\n",
      "{'example_id': '20136c9c-6069-4907-937f-30399055fc1c', 'cls_score': 0.19120799750089645, 'start_logit': -0.17658907175064087, 'end_logit': -0.18645374476909637, 'span_answer': {'start_position': 2505, 'end_position': 2508}, 'span_answer_score': -0.07484609261155128, 'start_index': 284, 'end_index': 284, 'passage_index': 9, 'target_type_logits': [0.5168127417564392, 0.40455862879753113, 0.06374607980251312, -0.2633518874645233, -0.5787954926490784], 'span_answer_text': 'kry', 'yes_no_answer': 0, 'start_stdev': 0.033222746104002, 'end_stdev': 0.022706078365445137, 'query_passage_similarity': 1.9715971946716309, 'normalized_span_answer_score': 0.3347785006273036, 'confidence_score': 0.004900850396614854}\n",
      "{'example_id': 'd5183448-baed-47b8-a753-9a453eadd97b', 'cls_score': 1.051070511341095, 'start_logit': 0.39676433801651, 'end_logit': 1.3126187324523926, 'span_answer': {'start_position': 1024, 'end_position': 1029}, 'span_answer_score': 0.6056421995162964, 'start_index': 263, 'end_index': 263, 'passage_index': 2, 'target_type_logits': [0.5728487372398376, 0.5529718399047852, 0.053563810884952545, -0.3596673905849457, -0.5837070941925049], 'span_answer_text': 'There', 'yes_no_answer': 0, 'start_stdev': 0.3750673830509186, 'end_stdev': 0.12789790332317352, 'query_passage_similarity': 1.971407413482666, 'normalized_span_answer_score': 0.3558888401397341, 'confidence_score': 0.0032430570734104636}\n",
      "{'example_id': 'ff425110-e3a3-41b9-8bfc-6ebd9c576b1d', 'cls_score': -0.13393384590744972, 'start_logit': -0.22728490829467773, 'end_logit': -0.21812491118907928, 'span_answer': {'start_position': 1415, 'end_position': 1467}, 'span_answer_score': -0.04215492866933346, 'start_index': 100, 'end_index': 112, 'passage_index': 6, 'target_type_logits': [0.41131022572517395, 0.22716611623764038, 0.04664851352572441, -0.15111032128334045, -0.5588777661323547], 'span_answer_text': 'రింగ్ కాలేజి (కారెపల్లి)\\nప్రభుత్వ డిగ్రీ కళాశాల 1991', 'yes_no_answer': 0, 'start_stdev': 0.18875347077846527, 'end_stdev': 0.11793793737888336, 'query_passage_similarity': 1.983193039894104, 'normalized_span_answer_score': 0.3335889815954428, 'confidence_score': 0.005863378218317353}\n",
      "{'example_id': 'a2842d8e-eefd-468b-b118-3a079d92ef9d', 'cls_score': 0.13419528305530548, 'start_logit': -0.17793671786785126, 'end_logit': -0.09252211451530457, 'span_answer': {'start_position': 34, 'end_position': 47}, 'span_answer_score': -0.05760765075683594, 'start_index': 25, 'end_index': 26, 'passage_index': 0, 'target_type_logits': [0.47241801023483276, 0.28943881392478943, -0.018849192187190056, -0.18884751200675964, -0.58086097240448], 'span_answer_text': 'Septemba 1959', 'yes_no_answer': 0, 'start_stdev': 0.10138693451881409, 'end_stdev': 0.12533754110336304, 'query_passage_similarity': 1.9745116233825684, 'normalized_span_answer_score': 0.3388089323246674, 'confidence_score': 0.004906715667273239}\n",
      "{'example_id': '8dae1434-b683-4b91-b844-1993a9746179', 'cls_score': 0.5051530078053474, 'start_logit': -0.22785601019859314, 'end_logit': -0.15812338888645172, 'span_answer': {'start_position': 9644, 'end_position': 9650}, 'span_answer_score': -0.22915593907237053, 'start_index': 320, 'end_index': 320, 'passage_index': 17, 'target_type_logits': [0.5167137980461121, 0.43282052874565125, -0.05526207759976387, -0.29655584692955017, -0.624236524105072], 'span_answer_text': 'отличи', 'yes_no_answer': 0, 'start_stdev': 0.08912134915590286, 'end_stdev': 0.04312802851200104, 'query_passage_similarity': 1.9735432863235474, 'normalized_span_answer_score': 0.33550173709653475, 'confidence_score': 0.003678434622683886}\n",
      "{'example_id': 'ee2d550e-dbce-44cd-8621-65706270f7e3', 'cls_score': 0.10898364335298538, 'start_logit': -0.07141280174255371, 'end_logit': -0.2980205714702606, 'span_answer': {'start_position': 14683, 'end_position': 14716}, 'span_answer_score': -0.0748567022383213, 'start_index': 48, 'end_index': 61, 'passage_index': 36, 'target_type_logits': [0.5283904671669006, 0.3287036120891571, -0.01619722880423069, -0.19583407044410706, -0.5614584684371948], 'span_answer_text': '199 ซม.\\nน้ำหนัก: 107 กก.\\nพลังหมัด', 'yes_no_answer': 0, 'start_stdev': 0.10573910176753998, 'end_stdev': 0.08059477806091309, 'query_passage_similarity': 1.9765183925628662, 'normalized_span_answer_score': 0.3411290185962687, 'confidence_score': 0.00517483817932922}\n",
      "{'example_id': '801fe5bc-ca98-465f-bc94-486d4988df0e', 'cls_score': 0.11496789753437042, 'start_logit': -0.22450736165046692, 'end_logit': -0.16659735143184662, 'span_answer': {'start_position': 1985, 'end_position': 1997}, 'span_answer_score': -0.10100710391998291, 'start_index': 218, 'end_index': 225, 'passage_index': 8, 'target_type_logits': [0.49902740120887756, 0.30405840277671814, 0.019415272399783134, -0.22521957755088806, -0.606820821762085], 'span_answer_text': '時代は冷遇していた。のち', 'yes_no_answer': 0, 'start_stdev': 0.13627587258815765, 'end_stdev': 0.13563033938407898, 'query_passage_similarity': 1.9752681255340576, 'normalized_span_answer_score': 0.3372113256473362, 'confidence_score': 0.004770763707475666}\n",
      "{'example_id': 'fa1214d9-963d-449c-bde1-fd8197f6fccb', 'cls_score': 1.3468073308467865, 'start_logit': -0.17828460037708282, 'end_logit': -0.17465965449810028, 'span_answer': {'start_position': 7020, 'end_position': 7031}, 'span_answer_score': -0.4050433486700058, 'start_index': 385, 'end_index': 387, 'passage_index': 24, 'target_type_logits': [0.5816989541053772, 0.889664888381958, -0.18217259645462036, -0.5597931742668152, -0.4024174213409424], 'span_answer_text': 'أرض والبراك', 'yes_no_answer': 0, 'start_stdev': 0.18466663360595703, 'end_stdev': 0.09667222201824188, 'query_passage_similarity': 1.9690649509429932, 'normalized_span_answer_score': 0.3349261933522768, 'confidence_score': 0.001499706243683975}\n",
      "{'example_id': '7bbecca3-21c1-4eb4-8dd5-46d585db849a', 'cls_score': -0.04432128369808197, 'start_logit': -0.14349116384983063, 'end_logit': -0.2676498293876648, 'span_answer': {'start_position': 159, 'end_position': 166}, 'span_answer_score': -0.05258622765541077, 'start_index': 53, 'end_index': 54, 'passage_index': -1, 'target_type_logits': [0.4151017665863037, 0.2616472542285919, 0.0633048564195633, -0.18363797664642334, -0.5724905729293823], 'span_answer_text': '39\\n\\nమూల', 'yes_no_answer': 0, 'start_stdev': 0.11316618323326111, 'end_stdev': 0.11528749763965607, 'query_passage_similarity': 1.981276035308838, 'normalized_span_answer_score': 0.33402633316392627, 'confidence_score': 0.005832117469362607}\n",
      "{'example_id': '60466de1-07a1-443a-aba0-f071731cd751', 'cls_score': -0.08291199803352356, 'start_logit': -0.17973875999450684, 'end_logit': -0.2573334574699402, 'span_answer': {'start_position': 11176, 'end_position': 11286}, 'span_answer_score': -0.0042746663093566895, 'start_index': 450, 'end_index': 475, 'passage_index': -1, 'target_type_logits': [0.48459094762802124, 0.3456108868122101, 0.03740357980132103, -0.23837676644325256, -0.5785254240036011], 'span_answer_text': 'peräisin tuottavasta sijoituskohteesta eikä uusilta osallistujilta.\\nNiin kuin huijauksissa yleensä, myös ponzi', 'yes_no_answer': 0, 'start_stdev': 0.1017092764377594, 'end_stdev': 0.06813646852970123, 'query_passage_similarity': 1.9686665534973145, 'normalized_span_answer_score': 0.3355079640086703, 'confidence_score': 0.005977166540835443}\n",
      "{'example_id': '63108a2a-6135-499a-a368-4da3bda05bb4', 'cls_score': 1.1695459932088852, 'start_logit': -0.1926121562719345, 'end_logit': -0.12493613362312317, 'span_answer': {'start_position': 1968, 'end_position': 2015}, 'span_answer_score': -0.4094400405883789, 'start_index': 112, 'end_index': 122, 'passage_index': 5, 'target_type_logits': [0.5392483472824097, 0.6682142019271851, -0.2295074313879013, -0.45368900895118713, -0.4692118167877197], 'span_answer_text': 'Futebol Clube kusini mwa Brazil. Mwaka wa 2001,', 'yes_no_answer': 0, 'start_stdev': 0.11992423236370087, 'end_stdev': 0.07903657108545303, 'query_passage_similarity': 1.9728362560272217, 'normalized_span_answer_score': 0.33685687961147903, 'confidence_score': 0.0016286473960554896}\n",
      "{'example_id': '91d26328-7ebf-4001-b2f8-85b890706121', 'cls_score': 1.277224212884903, 'start_logit': -0.2018776684999466, 'end_logit': -0.17105023562908173, 'span_answer': {'start_position': 547, 'end_position': 550}, 'span_answer_score': -0.47468526661396027, 'start_index': 176, 'end_index': 176, 'passage_index': 1, 'target_type_logits': [0.5996225476264954, 0.7007815837860107, -0.18433482944965363, -0.48751670122146606, -0.5678070187568665], 'span_answer_text': 'ese', 'yes_no_answer': 0, 'start_stdev': 0.06728341430425644, 'end_stdev': 0.09485269337892532, 'query_passage_similarity': 1.9736276865005493, 'normalized_span_answer_score': 0.3361570315098093, 'confidence_score': 0.0015874761379358487}\n",
      "{'example_id': '56589910-8584-4ab1-b2e6-d393d3ca4f08', 'cls_score': 0.7515074014663696, 'start_logit': -0.1812513768672943, 'end_logit': -0.20910482108592987, 'span_answer': {'start_position': 5657, 'end_position': 5762}, 'span_answer_score': -0.2966231480240822, 'start_index': 45, 'end_index': 68, 'passage_index': -1, 'target_type_logits': [0.5850104093551636, 0.5486173033714294, -0.07433030009269714, -0.32424718141555786, -0.5665165781974792], 'span_answer_text': 'команде впервые стала олимпийской чемпионкой.\\nСпустя неделю, фигуристка блестяще исполнила свои программы', 'yes_no_answer': 0, 'start_stdev': 0.09366213530302048, 'end_stdev': 0.15926900506019592, 'query_passage_similarity': 1.9748729467391968, 'normalized_span_answer_score': 0.3354330406761998, 'confidence_score': 0.0030662453804347826}\n",
      "{'example_id': '20c429e5-e814-4148-bb0b-8a61b7328f84', 'cls_score': 0.26079241912520956, 'start_logit': -0.17589744925498962, 'end_logit': -0.24912433326244354, 'span_answer': {'start_position': 9577, 'end_position': 9593}, 'span_answer_score': -0.19465571632463252, 'start_index': 157, 'end_index': 167, 'passage_index': -1, 'target_type_logits': [0.5181775689125061, 0.2965027689933777, 0.0557059682905674, -0.19692039489746094, -0.5556153655052185], 'span_answer_text': '86: 4레벨\\n68K: 2레벌', 'yes_no_answer': 0, 'start_stdev': 0.09288019686937332, 'end_stdev': 0.01708483323454857, 'query_passage_similarity': 1.977149486541748, 'normalized_span_answer_score': 0.33526657167128854, 'confidence_score': 0.004539448797870307}\n",
      "{'example_id': '8cddccef-0843-4ce7-bf9c-842217d7f766', 'cls_score': 0.6248171702027321, 'start_logit': -0.2595961391925812, 'end_logit': -0.1069069653749466, 'span_answer': {'start_position': 1011, 'end_position': 1015}, 'span_answer_score': -0.24759168550372124, 'start_index': 44, 'end_index': 45, 'passage_index': 0, 'target_type_logits': [0.7002372145652771, 0.4961369037628174, -0.11349587142467499, -0.371994286775589, -0.6494185328483582], 'span_answer_text': '১৯১৪', 'yes_no_answer': 0, 'start_stdev': 0.10904204845428467, 'end_stdev': 0.10369563847780228, 'query_passage_similarity': 1.978014349937439, 'normalized_span_answer_score': 0.3359922233621697, 'confidence_score': 0.003130235577593123}\n",
      "{'example_id': '00f08c42-4e8e-4b86-a0e4-d7368175421e', 'cls_score': 0.15224048495292664, 'start_logit': -0.17288896441459656, 'end_logit': -0.18415813148021698, 'span_answer': {'start_position': 1311, 'end_position': 1328}, 'span_answer_score': -0.027150996029376984, 'start_index': 389, 'end_index': 395, 'passage_index': 2, 'target_type_logits': [0.597987949848175, 0.4549855887889862, -0.02441280521452427, -0.32171645760536194, -0.638160765171051], 'span_answer_text': 'พฤษภาคม ค.ศ. 1707', 'yes_no_answer': 0, 'start_stdev': 0.21949012577533722, 'end_stdev': 0.04805217683315277, 'query_passage_similarity': 1.9744417667388916, 'normalized_span_answer_score': 0.3411436451238455, 'confidence_score': 0.00442507889242388}\n",
      "{'example_id': '4d13bb3e-1872-4dbc-9d0f-494b95547ab7', 'cls_score': 0.07723668217658997, 'start_logit': -0.18544505536556244, 'end_logit': -0.22730234265327454, 'span_answer': {'start_position': 4031, 'end_position': 4045}, 'span_answer_score': -0.08060721307992935, 'start_index': 207, 'end_index': 213, 'passage_index': 38, 'target_type_logits': [0.5176440477371216, 0.32876965403556824, -0.0006703610415570438, -0.22113533318042755, -0.5699714422225952], 'span_answer_text': '疑いがない。その後、2005', 'yes_no_answer': 0, 'start_stdev': 0.17695322632789612, 'end_stdev': 0.1373472511768341, 'query_passage_similarity': 1.96546471118927, 'normalized_span_answer_score': 0.3345687060400864, 'confidence_score': 0.0049031258405047105}\n",
      "{'example_id': 'ee5b1fba-93ce-4d21-bee4-38d43cc92385', 'cls_score': 0.8404246566351503, 'start_logit': -0.10453522205352783, 'end_logit': -0.14847157895565033, 'span_answer': {'start_position': 6603, 'end_position': 6704}, 'span_answer_score': -0.12019514280837029, 'start_index': 169, 'end_index': 197, 'passage_index': 21, 'target_type_logits': [0.6000871658325195, 0.8530411720275879, -0.15692637860774994, -0.5786235332489014, -0.3337884843349457], 'span_answer_text': 'دى الطويل. على العكس من ذلك لا يمكن للغلوبيولين المناعي م عبور المشيمة، وهذا هو السبب في أن بعض العدو', 'yes_no_answer': 0, 'start_stdev': 0.1778518110513687, 'end_stdev': 0.02966790832579136, 'query_passage_similarity': 1.9712837934494019, 'normalized_span_answer_score': 0.34050625659479383, 'confidence_score': 0.0019175858856199492}\n",
      "{'example_id': 'acb4031c-ae4a-402b-9e68-93f4eb0ac409', 'cls_score': -0.07831574976444244, 'start_logit': -0.2158074826002121, 'end_logit': -0.22429116070270538, 'span_answer': {'start_position': 1393, 'end_position': 1409}, 'span_answer_score': 0.026886440813541412, 'start_index': 34, 'end_index': 37, 'passage_index': 5, 'target_type_logits': [0.5010812282562256, 0.41555577516555786, -0.026127567514777184, -0.26209744811058044, -0.5974437594413757], 'span_answer_text': '1994లో భారీ తేడా', 'yes_no_answer': 0, 'start_stdev': 0.2123274952173233, 'end_stdev': 0.13406597077846527, 'query_passage_similarity': 1.977634310722351, 'normalized_span_answer_score': 0.33405347984943035, 'confidence_score': 0.005342553438968105}\n",
      "{'example_id': '3cdd4f28-1407-4155-afb6-b73f768b6c79', 'cls_score': 0.8340791612863541, 'start_logit': -0.2726515233516693, 'end_logit': -0.048108313232660294, 'span_answer': {'start_position': 430, 'end_position': 520}, 'span_answer_score': -0.28243223764002323, 'start_index': 116, 'end_index': 135, 'passage_index': 2, 'target_type_logits': [0.6776952147483826, 0.5899745225906372, -0.09499721974134445, -0.3654592037200928, -0.656507134437561], 'span_answer_text': 'Ensimmäinen Napoleonin sodista Ranskan keisarina oli Yhdistyneen kuningaskunnan vuonna 180', 'yes_no_answer': 0, 'start_stdev': 0.1512528508901596, 'end_stdev': 0.03268104046583176, 'query_passage_similarity': 1.9707258939743042, 'normalized_span_answer_score': 0.33657102854909526, 'confidence_score': 0.0027215737359747366}\n",
      "{'example_id': '4ea6a2db-b796-49ba-af8e-fbc6cf593f80', 'cls_score': 0.7404927462339401, 'start_logit': -0.14851351082324982, 'end_logit': -0.0867680087685585, 'span_answer': {'start_position': 49, 'end_position': 53}, 'span_answer_score': -0.24829846993088722, 'start_index': 26, 'end_index': 26, 'passage_index': 0, 'target_type_logits': [0.5893981456756592, 0.479177325963974, -0.05134760960936546, -0.31539157032966614, -0.6365391612052917], 'span_answer_text': '1988', 'yes_no_answer': 0, 'start_stdev': 0.06381341814994812, 'end_stdev': 0.048460885882377625, 'query_passage_similarity': 1.975763201713562, 'normalized_span_answer_score': 0.33832535173094097, 'confidence_score': 0.0031834191070737664}\n",
      "{'example_id': '9e270318-242e-46d4-a81a-182d089104d4', 'cls_score': 0.3669428452849388, 'start_logit': -0.19117747247219086, 'end_logit': -0.1677405834197998, 'span_answer': {'start_position': 36, 'end_position': 40}, 'span_answer_score': -0.2044062800705433, 'start_index': 19, 'end_index': 19, 'passage_index': 0, 'target_type_logits': [0.5082184076309204, 0.3170483410358429, -0.01062233280390501, -0.21717649698257446, -0.6037549376487732], 'span_answer_text': 'Iran', 'yes_no_answer': 0, 'start_stdev': 0.10533840209245682, 'end_stdev': 0.10038138180971146, 'query_passage_similarity': 1.9761468172073364, 'normalized_span_answer_score': 0.3388083002717031, 'confidence_score': 0.004066147535313073}\n",
      "{'example_id': '7438e841-0b3d-4932-a3d3-833f773af943', 'cls_score': 0.7699806839227676, 'start_logit': -0.16800455749034882, 'end_logit': -0.19686757028102875, 'span_answer': {'start_position': 1318, 'end_position': 1330}, 'span_answer_score': -0.2542370781302452, 'start_index': 33, 'end_index': 37, 'passage_index': 6, 'target_type_logits': [0.6693320870399475, 0.6263786554336548, -0.050672415643930435, -0.41001906991004944, -0.6030713319778442], 'span_answer_text': 'عنصراً معمار', 'yes_no_answer': 0, 'start_stdev': 0.08422298729419708, 'end_stdev': 0.1628447324037552, 'query_passage_similarity': 1.974406123161316, 'normalized_span_answer_score': 0.3343229068474251, 'confidence_score': 0.0021562873953944075}\n",
      "{'example_id': 'ff0a1d49-2aaf-44a8-9177-27b105e38600', 'cls_score': 0.2144315242767334, 'start_logit': -0.13544730842113495, 'end_logit': -0.21469880640506744, 'span_answer': {'start_position': 5609, 'end_position': 5643}, 'span_answer_score': -0.11932118237018585, 'start_index': 70, 'end_index': 75, 'passage_index': 14, 'target_type_logits': [0.46665066480636597, 0.3259352743625641, 0.01367576140910387, -0.2084813416004181, -0.550326406955719], 'span_answer_text': 'Parigi Moutong. Dalam perkembangan', 'yes_no_answer': 0, 'start_stdev': 0.1177198737859726, 'end_stdev': 0.10573136806488037, 'query_passage_similarity': 1.9649196863174438, 'normalized_span_answer_score': 0.3363728418870471, 'confidence_score': 0.004754825726218601}\n",
      "{'example_id': 'a8308972-e833-4285-92be-6f49076d2201', 'cls_score': 0.9550120458006859, 'start_logit': -0.2070208042860031, 'end_logit': -0.08778274059295654, 'span_answer': {'start_position': 1372, 'end_position': 1376}, 'span_answer_score': -0.29245251789689064, 'start_index': 320, 'end_index': 320, 'passage_index': 7, 'target_type_logits': [0.6962985396385193, 0.6649105548858643, -0.22222106158733368, -0.4484863877296448, -0.5826525688171387], 'span_answer_text': '1885', 'yes_no_answer': 0, 'start_stdev': 0.05377182736992836, 'end_stdev': 0.038205571472644806, 'query_passage_similarity': 1.972233772277832, 'normalized_span_answer_score': 0.33685787339804363, 'confidence_score': 0.002687163357570207}\n",
      "{'example_id': 'df469e43-5df6-4633-9f1a-ed2251c00f00', 'cls_score': 0.46488965302705765, 'start_logit': -0.06311703473329544, 'end_logit': 0.52800053358078, 'span_answer': {'start_position': 6641, 'end_position': 6645}, 'span_answer_score': 0.22298873215913773, 'start_index': 253, 'end_index': 253, 'passage_index': 17, 'target_type_logits': [0.6686529517173767, 0.4459836184978485, -0.04667074233293533, -0.30363863706588745, -0.670892059803009], 'span_answer_text': '불구하고', 'yes_no_answer': 0, 'start_stdev': 0.19738246500492096, 'end_stdev': 0.16049307584762573, 'query_passage_similarity': 1.976318597793579, 'normalized_span_answer_score': 0.3774391860424851, 'confidence_score': 0.0035985301189396905}\n",
      "{'example_id': '29038dc9-8369-4f33-bdb8-6bd5d008535a', 'cls_score': 0.4811353012919426, 'start_logit': 0.004711523652076721, 'end_logit': -0.15581144392490387, 'span_answer': {'start_position': 114, 'end_position': 130}, 'span_answer_score': -0.08446140214800835, 'start_index': 43, 'end_index': 48, 'passage_index': 0, 'target_type_logits': [0.5624176263809204, 0.46331241726875305, 0.0015215699095278978, -0.30689969658851624, -0.6523681282997131], 'span_answer_text': 'nimenään etunime', 'yes_no_answer': 0, 'start_stdev': 0.2160855084657669, 'end_stdev': 0.15375368297100067, 'query_passage_similarity': 1.9760624170303345, 'normalized_span_answer_score': 0.34494728023304366, 'confidence_score': 0.0036445733473000072}\n",
      "{'example_id': '981b8602-3b85-46fa-83e9-8284ae2e5f10', 'cls_score': 1.3414384424686432, 'start_logit': -0.12873654067516327, 'end_logit': -0.031551096588373184, 'span_answer': {'start_position': 19, 'end_position': 26}, 'span_answer_score': -0.4233237747102976, 'start_index': 19, 'end_index': 22, 'passage_index': 0, 'target_type_logits': [0.655785322189331, 0.6550785303115845, -0.12608948349952698, -0.4014042615890503, -0.548662006855011], 'span_answer_text': 'c. 1096', 'yes_no_answer': 0, 'start_stdev': 0.12118221074342728, 'end_stdev': 0.10694344341754913, 'query_passage_similarity': 1.9726179838180542, 'normalized_span_answer_score': 0.33926052914781984, 'confidence_score': 0.0022193298971118097}\n",
      "{'example_id': '1169f6f4-e781-42f9-89df-732d5132bddf', 'cls_score': 0.4215589463710785, 'start_logit': -0.20103125274181366, 'end_logit': -0.2276170551776886, 'span_answer': {'start_position': 2057, 'end_position': 2061}, 'span_answer_score': -0.22952573746442795, 'start_index': 62, 'end_index': 62, 'passage_index': 6, 'target_type_logits': [0.5949073433876038, 0.39115577936172485, 0.009900224395096302, -0.27512258291244507, -0.6570228934288025], 'span_answer_text': '[16]', 'yes_no_answer': 0, 'start_stdev': 0.09457330405712128, 'end_stdev': 0.0345536433160305, 'query_passage_similarity': 1.9775367975234985, 'normalized_span_answer_score': 0.3346092697220207, 'confidence_score': 0.0038697302447326673}\n",
      "{'example_id': 'c86724f8-25c9-4648-990f-bc606321a6cf', 'cls_score': 0.32950614392757416, 'start_logit': -0.25121304392814636, 'end_logit': -0.17766091227531433, 'span_answer': {'start_position': 3822, 'end_position': 3829}, 'span_answer_score': -0.1728261485695839, 'start_index': 350, 'end_index': 351, 'passage_index': 7, 'target_type_logits': [0.5768911242485046, 0.41272780299186707, -0.015497508458793163, -0.2744673490524292, -0.6279031038284302], 'span_answer_text': '24 июля', 'yes_no_answer': 0, 'start_stdev': 0.07814270257949829, 'end_stdev': 0.12610037624835968, 'query_passage_similarity': 1.9775522947311401, 'normalized_span_answer_score': 0.3361149788326441, 'confidence_score': 0.0040364423659370674}\n",
      "{'example_id': '8cb61917-db7f-452f-97b5-363c1693ac06', 'cls_score': 0.10437649488449097, 'start_logit': -0.15687380731105804, 'end_logit': -0.1979089081287384, 'span_answer': {'start_position': 1992, 'end_position': 1995}, 'span_answer_score': -0.08085853606462479, 'start_index': 73, 'end_index': 73, 'passage_index': 11, 'target_type_logits': [0.4990677833557129, 0.29744213819503784, 0.031189559027552605, -0.20313525199890137, -0.5814221501350403], 'span_answer_text': '228', 'yes_no_answer': 0, 'start_stdev': 0.15040811896324158, 'end_stdev': 0.15563257038593292, 'query_passage_similarity': 1.9654942750930786, 'normalized_span_answer_score': 0.3394974374226807, 'confidence_score': 0.004934009209770471}\n",
      "{'example_id': '5115a5e0-e156-4268-9f39-156d9d866118', 'cls_score': 0.026804745197296143, 'start_logit': -0.261343777179718, 'end_logit': -0.1832931786775589, 'span_answer': {'start_position': 12210, 'end_position': 12334}, 'span_answer_score': -0.06481973081827164, 'start_index': 22, 'end_index': 43, 'passage_index': -1, 'target_type_logits': [0.48518043756484985, 0.3418022394180298, 0.030206920579075813, -0.22703799605369568, -0.5795691609382629], 'span_answer_text': '[30]\\nสหรัฐอเมริกา\\nบทความหลัก (ภาษาอังกฤษ): Greenhouse gas emissions by the United States และ  United States federal register', 'yes_no_answer': 0, 'start_stdev': 0.11020688712596893, 'end_stdev': 0.1384824961423874, 'query_passage_similarity': 1.9755889177322388, 'normalized_span_answer_score': 0.33483983285310787, 'confidence_score': 0.005156511843758531}\n",
      "{'example_id': 'f33be50c-edd9-49f1-85ac-d25dd6f8bc01', 'cls_score': -0.11584915965795517, 'start_logit': -0.21177516877651215, 'end_logit': -0.19610972702503204, 'span_answer': {'start_position': 2137, 'end_position': 2178}, 'span_answer_score': -0.011113438755273819, 'start_index': 16, 'end_index': 32, 'passage_index': 3, 'target_type_logits': [0.43462228775024414, 0.2698088586330414, 0.0051804534159600735, -0.1863126903772354, -0.5804638266563416], 'span_answer_text': 'టోబరు 28న కవిసార్వభౌమ\\nరామచంద్రపురంలో 1982', 'yes_no_answer': 0, 'start_stdev': 0.07899904251098633, 'end_stdev': 0.12062530964612961, 'query_passage_similarity': 1.9776055812835693, 'normalized_span_answer_score': 0.3353264791278145, 'confidence_score': 0.006125058249610499}\n",
      "{'example_id': '7ac7e599-1c87-4712-a0e1-e5799b2cc905', 'cls_score': 0.498475044965744, 'start_logit': -0.14036937057971954, 'end_logit': -0.14989610016345978, 'span_answer': {'start_position': 4223, 'end_position': 4262}, 'span_answer_score': -0.1874144822359085, 'start_index': 38, 'end_index': 47, 'passage_index': 12, 'target_type_logits': [0.5411182045936584, 0.4139115512371063, 0.03728250041604042, -0.3481089770793915, -0.5757942199707031], 'span_answer_text': 'järjestelmään. Meluperäiset kuulovammat', 'yes_no_answer': 0, 'start_stdev': 0.14654138684272766, 'end_stdev': 0.1095329076051712, 'query_passage_similarity': 1.9716119766235352, 'normalized_span_answer_score': 0.3409573612691808, 'confidence_score': 0.003633779368201624}\n",
      "{'example_id': '88173673-8427-42ea-8dd1-184354601c19', 'cls_score': 1.164983406662941, 'start_logit': -0.18210457265377045, 'end_logit': -0.18873022496700287, 'span_answer': {'start_position': 155, 'end_position': 157}, 'span_answer_score': -0.38665761798620224, 'start_index': 57, 'end_index': 57, 'passage_index': 2, 'target_type_logits': [0.6192336678504944, 0.7625029683113098, -0.17833924293518066, -0.49454465508461, -0.41149720549583435], 'span_answer_text': 'na', 'yes_no_answer': 0, 'start_stdev': 0.1725030243396759, 'end_stdev': 0.128980815410614, 'query_passage_similarity': 1.9712287187576294, 'normalized_span_answer_score': 0.3368182517668753, 'confidence_score': 0.0015820408405030487}\n",
      "{'example_id': 'b76284d7-bcfe-44fd-9a74-f1d341010f77', 'cls_score': 0.9890072047710419, 'start_logit': -0.16048096120357513, 'end_logit': -0.16653670370578766, 'span_answer': {'start_position': 1784, 'end_position': 1793}, 'span_answer_score': -0.36189426481723785, 'start_index': 378, 'end_index': 379, 'passage_index': 2, 'target_type_logits': [0.647951066493988, 0.592236340045929, -0.07157783210277557, -0.3759646713733673, -0.593360960483551], 'span_answer_text': 'India kun', 'yes_no_answer': 0, 'start_stdev': 0.07074688374996185, 'end_stdev': 0.15691539645195007, 'query_passage_similarity': 1.9738246202468872, 'normalized_span_answer_score': 0.3338075330356691, 'confidence_score': 0.0026838008198829216}\n",
      "{'example_id': '1968b603-02e7-428a-ae8a-7b59ca6392d0', 'cls_score': 0.10107946395874023, 'start_logit': -0.2150181084871292, 'end_logit': -0.22635142505168915, 'span_answer': {'start_position': 4538, 'end_position': 4588}, 'span_answer_score': -0.10640525817871094, 'start_index': 81, 'end_index': 105, 'passage_index': -1, 'target_type_logits': [0.5642011761665344, 0.3296384811401367, -0.019952060654759407, -0.2263408899307251, -0.6131995320320129], 'span_answer_text': '증거가 있으면 명예 훼손에 대한 방어가 무효화 될 수 있다.\\n호주의 고등법원 판결\\n2002', 'yes_no_answer': 0, 'start_stdev': 0.14539751410484314, 'end_stdev': 0.14640413224697113, 'query_passage_similarity': 1.974363923072815, 'normalized_span_answer_score': 0.336713703245202, 'confidence_score': 0.0047579940358269375}\n",
      "{'example_id': 'c29fbf84-08ff-4807-824f-69d75aac52aa', 'cls_score': -0.14665133506059647, 'start_logit': -0.24605965614318848, 'end_logit': -0.24051757156848907, 'span_answer': {'start_position': 318, 'end_position': 330}, 'span_answer_score': -0.03971685841679573, 'start_index': 75, 'end_index': 80, 'passage_index': 0, 'target_type_logits': [0.44998297095298767, 0.2604921758174896, 0.01901969127357006, -0.1646374613046646, -0.5559430718421936], 'span_answer_text': 'ปี พ.ศ. 2558', 'yes_no_answer': 0, 'start_stdev': 0.13208404183387756, 'end_stdev': 0.01855667121708393, 'query_passage_similarity': 1.9723964929580688, 'normalized_span_answer_score': 0.3345055419281346, 'confidence_score': 0.0061813101453129515}\n",
      "{'example_id': 'f9b8985c-bb39-4b55-9fa2-c618b91911fd', 'cls_score': 0.945884183049202, 'start_logit': -0.1876204013824463, 'end_logit': -0.1186993345618248, 'span_answer': {'start_position': 200, 'end_position': 265}, 'span_answer_score': -0.3104926235973835, 'start_index': 68, 'end_index': 85, 'passage_index': -1, 'target_type_logits': [0.6559941172599792, 0.631218671798706, -0.1257292479276657, -0.43270906805992126, -0.537575364112854], 'span_answer_text': 'fano\\nMaji yakimwagika hayazoleki\\nMayai yaliyooza yananuka sana\\nYa', 'yes_no_answer': 0, 'start_stdev': 0.10192957520484924, 'end_stdev': 0.12512564659118652, 'query_passage_similarity': 1.967233419418335, 'normalized_span_answer_score': 0.3381221433145962, 'confidence_score': 0.002669193460250131}\n",
      "{'example_id': '0a296f9f-5dd5-4827-8101-f2a007e3c4f8', 'cls_score': -0.12659385800361633, 'start_logit': -0.2083895355463028, 'end_logit': -0.14488044381141663, 'span_answer': {'start_position': 500, 'end_position': 549}, 'span_answer_score': 0.06333661824464798, 'start_index': 197, 'end_index': 218, 'passage_index': 1, 'target_type_logits': [0.4763985276222229, 0.35334935784339905, 0.042223747819662094, -0.2238631546497345, -0.5870777368545532], 'span_answer_text': 'విటమిన్\\u200c ఇ, పొటాషియం, ఫోలిక్\\u200c యాసిడ్\\u200c, కెరోటినాయి', 'yes_no_answer': 0, 'start_stdev': 0.05879925191402435, 'end_stdev': 0.10859887301921844, 'query_passage_similarity': 1.9788658618927002, 'normalized_span_answer_score': 0.34285690264324176, 'confidence_score': 0.006142557118667349}\n",
      "{'example_id': '749ddb2b-7726-4a75-ac29-fd1f91f9ce17', 'cls_score': 0.2749990001320839, 'start_logit': -0.12135399878025055, 'end_logit': -0.1505197286605835, 'span_answer': {'start_position': 2745, 'end_position': 2748}, 'span_answer_score': -0.09543570503592491, 'start_index': 359, 'end_index': 359, 'passage_index': 2, 'target_type_logits': [0.5113399624824524, 0.3560013175010681, 0.02603369764983654, -0.2627083361148834, -0.5666877031326294], 'span_answer_text': 'pur', 'yes_no_answer': 0, 'start_stdev': 0.19966185092926025, 'end_stdev': 0.15376803278923035, 'query_passage_similarity': 1.9566349983215332, 'normalized_span_answer_score': 0.3387668293164193, 'confidence_score': 0.004246870328015554}\n",
      "{'example_id': 'cd994345-7659-4881-b6e2-569f3c4930f1', 'cls_score': -0.08236292004585266, 'start_logit': -0.21534837782382965, 'end_logit': -0.07829920947551727, 'span_answer': {'start_position': 28448, 'end_position': 28479}, 'span_answer_score': -0.00024031847715377808, 'start_index': 278, 'end_index': 286, 'passage_index': 17, 'target_type_logits': [0.4110424518585205, 0.2108040302991867, 0.02625654824078083, -0.12462039291858673, -0.5074655413627625], 'span_answer_text': 'лето от Рождества Христова 1576', 'yes_no_answer': 0, 'start_stdev': 0.10885588079690933, 'end_stdev': 0.14005792140960693, 'query_passage_similarity': 1.9734760522842407, 'normalized_span_answer_score': 0.3371791652198142, 'confidence_score': 0.0058833753068693925}\n",
      "{'example_id': '0d124e24-af29-472b-82b1-2620ca2e1d8b', 'cls_score': 0.29447026550769806, 'start_logit': -0.22380812466144562, 'end_logit': -0.18393392860889435, 'span_answer': {'start_position': 11824, 'end_position': 11828}, 'span_answer_score': -0.1508021429181099, 'start_index': 182, 'end_index': 182, 'passage_index': 35, 'target_type_logits': [0.6021908521652222, 0.40060803294181824, 0.00607288395985961, -0.27194446325302124, -0.650018036365509], 'span_answer_text': '1397', 'yes_no_answer': 0, 'start_stdev': 0.07213305681943893, 'end_stdev': 0.09512114524841309, 'query_passage_similarity': 1.97749924659729, 'normalized_span_answer_score': 0.3400901027159843, 'confidence_score': 0.004193852997146236}\n",
      "{'example_id': '74b869d4-05ec-4389-8d4a-5147a9781a72', 'cls_score': -0.04909420758485794, 'start_logit': -0.1856374889612198, 'end_logit': -0.2618599832057953, 'span_answer': {'start_position': 4586, 'end_position': 4614}, 'span_answer_score': -0.0903298668563366, 'start_index': 177, 'end_index': 188, 'passage_index': 6, 'target_type_logits': [0.428783118724823, 0.21774353086948395, -0.004565934184938669, -0.17252083122730255, -0.5405582189559937], 'span_answer_text': 'শক্তি চট্টোপাধ্যায়, শৈলেশ্ব', 'yes_no_answer': 0, 'start_stdev': 0.12733997404575348, 'end_stdev': 0.03422384709119797, 'query_passage_similarity': 1.9790221452713013, 'normalized_span_answer_score': 0.3349826591014957, 'confidence_score': 0.005824438911728715}\n",
      "{'example_id': '4ef95b97-74c9-4068-962a-259c65ce2b84', 'cls_score': 0.13507036864757538, 'start_logit': -0.20726613700389862, 'end_logit': -0.2922581136226654, 'span_answer': {'start_position': 21167, 'end_position': 21267}, 'span_answer_score': -0.09535188972949982, 'start_index': 151, 'end_index': 175, 'passage_index': 38, 'target_type_logits': [0.5598884224891663, 0.44389083981513977, -0.044517289847135544, -0.27151867747306824, -0.6049932241439819], 'span_answer_text': 'คะแนน CSE สูงจะรายงานว่า รู้สึกถึงความสมบูรณ์ทางการงาน-ครอบครัวที่ดีกว่า คือ เหตุการณ์ในบทบาทหน้าที่', 'yes_no_answer': 0, 'start_stdev': 0.19647732377052307, 'end_stdev': 0.07009320706129074, 'query_passage_similarity': 1.9738094806671143, 'normalized_span_answer_score': 0.3348747255209364, 'confidence_score': 0.00456291801237806}\n",
      "{'example_id': 'cc4564a6-af91-4327-bc11-2e07226ff8d3', 'cls_score': 0.4734383597970009, 'start_logit': -0.04094879329204559, 'end_logit': 0.740124523639679, 'span_answer': {'start_position': 21285, 'end_position': 21286}, 'span_answer_score': 0.32111022248864174, 'start_index': 334, 'end_index': 334, 'passage_index': 121, 'target_type_logits': [0.6490380167961121, 0.416483074426651, -0.02716880477964878, -0.3122195899486542, -0.6777611970901489], 'span_answer_text': '第', 'yes_no_answer': 0, 'start_stdev': 0.1859908103942871, 'end_stdev': 0.15331916511058807, 'query_passage_similarity': 1.9668188095092773, 'normalized_span_answer_score': 0.3383340825050984, 'confidence_score': 0.0038346337789129563}\n",
      "{'example_id': 'e4d810fe-6f1a-437a-91ed-41d2a9902afe', 'cls_score': 1.2763093411922455, 'start_logit': 0.3744754493236542, 'end_logit': 0.8990387320518494, 'span_answer': {'start_position': 21516, 'end_position': 21518}, 'span_answer_score': 0.434415727853775, 'start_index': 217, 'end_index': 217, 'passage_index': 51, 'target_type_logits': [0.5773878693580627, 0.871626615524292, -0.11601564288139343, -0.5609339475631714, -0.4461568593978882], 'span_answer_text': 'في', 'yes_no_answer': 0, 'start_stdev': 0.25094854831695557, 'end_stdev': 0.3440949022769928, 'query_passage_similarity': 1.9651124477386475, 'normalized_span_answer_score': 0.46924009521099236, 'confidence_score': 0.0019273825826091196}\n",
      "{'example_id': '91220d07-eb45-4a93-87c5-e0c81a631929', 'cls_score': -0.0017979741096496582, 'start_logit': -0.11598891019821167, 'end_logit': -0.14936459064483643, 'span_answer': {'start_position': 57, 'end_position': 61}, 'span_answer_score': 0.04649144411087036, 'start_index': 33, 'end_index': 33, 'passage_index': 0, 'target_type_logits': [0.47410693764686584, 0.35653841495513916, -0.020374616608023643, -0.20441892743110657, -0.612105131149292], 'span_answer_text': '1908', 'yes_no_answer': 0, 'start_stdev': 0.18435527384281158, 'end_stdev': 0.16215601563453674, 'query_passage_similarity': 1.9797523021697998, 'normalized_span_answer_score': 0.3364361412376652, 'confidence_score': 0.005260320756696365}\n",
      "{'example_id': '9433ca09-b1ff-41ff-bddc-af522799db8c', 'cls_score': 0.3036112040281296, 'start_logit': -0.17008395493030548, 'end_logit': -0.21599243581295013, 'span_answer': {'start_position': 1251, 'end_position': 1264}, 'span_answer_score': -0.1332528367638588, 'start_index': 328, 'end_index': 331, 'passage_index': 3, 'target_type_logits': [0.5742418169975281, 0.4231819212436676, -0.00024363651755265892, -0.3108854591846466, -0.6099100708961487], 'span_answer_text': 'hilloja, make', 'yes_no_answer': 0, 'start_stdev': 0.22957441210746765, 'end_stdev': 0.13950608670711517, 'query_passage_similarity': 1.9696364402770996, 'normalized_span_answer_score': 0.34049469737869814, 'confidence_score': 0.003920050624942281}\n",
      "{'example_id': '3fe83626-ab7d-463c-9902-92cbebe36faf', 'cls_score': 1.1063142716884613, 'start_logit': -0.15824446082115173, 'end_logit': -0.19257675111293793, 'span_answer': {'start_position': 138, 'end_position': 182}, 'span_answer_score': -0.35987909883260727, 'start_index': 53, 'end_index': 66, 'passage_index': 2, 'target_type_logits': [0.557885468006134, 0.7373772859573364, -0.14136497676372528, -0.42662712931632996, -0.3780454099178314], 'span_answer_text': 'Jamii:Miji ya Uholanzi\\nJamii:Mkoa wa Utrecht', 'yes_no_answer': 0, 'start_stdev': 0.15078260004520416, 'end_stdev': 0.14852817356586456, 'query_passage_similarity': 1.9769244194030762, 'normalized_span_answer_score': 0.3340573262162433, 'confidence_score': 0.0016515456489369419}\n"
     ]
    }
   ],
   "source": [
    "for example_id in validation_predictions:\n",
    "    print(validation_predictions[example_id][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d92e562",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
