{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running inference for the pre-trained model on OTTQA datset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"per_device_train_batch_size_rr\":8,\n",
    "    \"per_device_eval_batch_size_rr\":8,\n",
    "    \"rr_model_name\":\"bert-base-uncased\",\n",
    "    \"row_retriever_model_name_path\":\"data/ottqa/row_retriever/rr.bin\",\n",
    "    \"pos_frac_per_epoch\":[0.3, 0.3, 0.1, 0.0001, 0.0001],\n",
    "    \"group_frac_per_epoch\":[0.0, 0.5, 1.0, 1.0, 1.0],\n",
    "    \"max_seq_length\":512,\n",
    "    \"per_gpu_train_batch_size\":8,\n",
    "    \"train_batch_size\":8,\n",
    "    \"per_gpu_eval_batch_size\":8,\n",
    "    \"eval_batch_size\":8,\n",
    "    \"max_query_length\":64,\n",
    "    \"threads\":1,\n",
    "    \"null_score_diff_threshold\":0.0,\n",
    "    \"n_best_size\":20,\n",
    "    \"do_predict_ae\":True,\n",
    "    \"n_gpu\":1,\n",
    "    \"max_answer_length\":30,\n",
    "    \"model_name_or_path_ae\":\"bert-base-uncased\",\n",
    "    \"output_dir\":\"data/ottqa/models/answer_extractor/\",\n",
    "    \"model_type\":\"bert\",\n",
    "    \"doc_stride\":128,\n",
    "    \"pred_ans_file\":\"data/ottqa/predictions/answer_extractor_output_test.json\",\n",
    "    \"eval_file\":\"data/ottqa/ae_input_test.json\",\n",
    "    \"model\":\"gpt2\",\n",
    "    \"top_k\":0,\n",
    "    \"top_p\":0.9,\n",
    "    \"seed_lg\":42,\n",
    "    \"batch_size_lg\":2,\n",
    "    \"linker_model\":\"data/ottqa/models/link_generator/model-ep9.pt\",\n",
    "    \"max_source_len\":32,\n",
    "    \"max_target_len\":16,\n",
    "    \"do_all_lg\":True,\n",
    "    \"data_path_root\":\"data/ottqa/\",\n",
    "    \"dataset_name\":\"ottqa\",\n",
    "    \"test_data_path\":\"data/ottqa/released_data/toy.json\",\n",
    "    \"collections_file\":\"linearized_tables.tsv\",\n",
    "    \"test\":True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dccstor/cssblr/vishwajeet/git/primeqaenv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from primeqa.hybridqa.utils.arguments_utils import HybridQAArguments,LinkPredictorArguments, RRArguments,AEArguments\n",
    "\n",
    "\n",
    "hqa_parser = HfArgumentParser((HybridQAArguments,LinkPredictorArguments, RRArguments,AEArguments))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parse configs from config dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No GPU found. Please add GPU to your notebook\n",
      "{\"time\":\"2022-12-23 05:42:41,777\", \"name\": \"sentence_transformers.SentenceTransformer\", \"level\": \"INFO\", \"message\": \"Load pretrained SentenceTransformer: msmarco-distilbert-base-tas-b\"}\n",
      "{\"time\":\"2022-12-23 05:42:43,593\", \"name\": \"sentence_transformers.SentenceTransformer\", \"level\": \"INFO\", \"message\": \"Use pytorch device: cpu\"}\n",
      "Warning: No GPU found. Please add GPU to your notebook\n",
      "{\"time\":\"2022-12-23 05:42:44,287\", \"name\": \"sentence_transformers.SentenceTransformer\", \"level\": \"INFO\", \"message\": \"Load pretrained SentenceTransformer: msmarco-distilbert-base-tas-b\"}\n",
      "{\"time\":\"2022-12-23 05:42:45,739\", \"name\": \"sentence_transformers.SentenceTransformer\", \"level\": \"INFO\", \"message\": \"Use pytorch device: cpu\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.84it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.64it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.16it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.91it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.19it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.05it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.97it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.23it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.78it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.32it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.25it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.20it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.29it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.13it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.20it/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "from primeqa.hybridqa.utils.model_utils.row_retriever_MITQA import RowRetriever\n",
    "from primeqa.hybridqa.utils.model_utils.reranker import re_rank_ae_output\n",
    "from primeqa.hybridqa.utils.link_predictor import predict_link_for_tables,train_link_generator\n",
    "from primeqa.hybridqa.utils.model_utils.table_retriever import train_table_retriever,predict_table_retriever\n",
    "from primeqa.hybridqa.utils.model_utils.process_row_retriever_output import preprocess_data_using_row_retrieval_scores,create_dataset_for_answer_extractor\n",
    "from primeqa.hybridqa.utils.model_utils.answer_extractor_multi_Answer import run_answer_extractor\n",
    "from primeqa.hybridqa.processors.preprocessors.preprocess_raw_data import preprocess_data\n",
    "import logging\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "from primeqa.hybridqa.utils.arguments_utils import HybridQAArguments,LinkPredictorArguments, RRArguments,AEArguments\n",
    "\n",
    "# Read agruments and load the dataset\n",
    "test=True\n",
    "hqa_args,lp_args,rr_args,ae_args,= hqa_parser.parse_dict(config)\n",
    "raw_test_data = json.load(open(hqa_args.test_data_path))\n",
    "retrieved_data = predict_table_retriever(hqa_args.data_path_root,hqa_args.collections_file,raw_dev_data)\n",
    "linked_data = predict_link_for_tables(lp_args,retrieved_data)\n",
    "test_data_processed = preprocess_data(hqa_args.data_path_root,hqa_args.dataset_name,linked_data,split=\"test\",test=test)\n",
    "print(\"Initial preprocessing done\")\n",
    "rr = RowRetriever(hqa_args,rr_args)\n",
    "qid_scores_dict = rr.predict(test_data_processed)\n",
    "print(\"Row retrieval predictions Done\")\n",
    "test_processed_data = preprocess_data_using_row_retrieval_scores(raw_test_data,qid_scores_dict,test)\n",
    "print(\"Row retrieval output processed\")\n",
    "answer_extraction_data = create_dataset_for_answer_extractor(test_processed_data,hqa_args.data_path_root,test)\n",
    "print(\"Answer extraction data generated\")\n",
    "ae_output_path,ae_output_path_nbest = run_answer_extractor(ae_args,answer_extraction_data)\n",
    "print(ae_output_path)\n",
    "print(ae_output_path_nbest)\n",
    "re_ranked_output = re_rank_ae_output(qid_scores_dict,ae_output_path_nbest,ae_args.pred_ans_file) \n",
    "print(\"re-ranked output saved at \",re_ranked_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f9f85f796d01129d0dd105a088854619f454435301f6ffec2fea96ecbd9be4ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
