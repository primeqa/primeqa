{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54e95342",
   "metadata": {},
   "source": [
    "# Dense IR using _Pipelines_ interface\n",
    "\n",
    "In this notebook, we show how to index data, and run search using the _Pipelines_ API.\n",
    "In orded to run (almost) instantaneously, we use trivial data sizes of training data and collection to search.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51992c4b",
   "metadata": {},
   "source": [
    "## Initial setup\n",
    "\n",
    "We start by defining variables specifying locations of data we will use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd0c7354",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "\n",
    "test_files_location = '../../../tests/resources/ir_dense'\n",
    "with tempfile.TemporaryDirectory() as working_dir:\n",
    "    output_dir=os.path.join(working_dir, 'output_dir')\n",
    "    \n",
    "index_root = os.path.join(output_dir, 'index_root')\n",
    "index_name = 'index_name'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e163b958",
   "metadata": {},
   "source": [
    "## Indexing\n",
    "\n",
    "To run indexing, we need an existing model (checkpoint).  In this example, we use the Dr.Decr model, downloaded to the test examples directory.  This can be done by running (on a command line):\n",
    "```\n",
    " wget https://huggingface.co/PrimeQA/DrDecr_XOR-TyDi_whitebox/resolve/main/DrDecr.dnn\n",
    "```\n",
    "Next, we will index a collection of documents, using model representaion from the previous step. The collection is a TSV file, containing each document's ID, title, and text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62caf219",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_fn = os.path.join(test_files_location, \"xorqa.train_ir_001pct_at_0_pct_collection_fornum.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441b0e3c",
   "metadata": {},
   "source": [
    "Here is an example document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bdb8743",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The Kangxi Emperor's reign of 61 years makes him the longest-reigning emperor in Chinese history (although his grandson, the Qianlong Emperor, had the longest period of \"de facto\" power) and one of the longest-reigning rulers in the world. However, since he ascended the throne at the age of seven, actual power was held for six years by four regents and his grandmother, the Grand Empress Dowager Xiaozhuang.</td>\n",
       "      <td>Kangxi Emperor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "data = pd.read_csv(collection_fn, sep='\\t', header=0, nrows=1)\n",
    "display(HTML(data.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db322714",
   "metadata": {},
   "source": [
    "Next we instantiate the indexer and index the collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90784083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No CUDA runtime is found, using CUDA_HOME='/opt/share/cuda-10.1/x86_64'\n",
      "\n",
      "\n",
      "[Oct 13, 15:27:25] #> Note: Output directory /u/franzm/git8/PrimeQA_services/primeqa/notebooks/ir/dense/experiments/default/indexes/index_name already exists\n",
      "\n",
      "\n",
      "[Oct 13, 15:27:25] #> Will delete 10 files already at /u/franzm/git8/PrimeQA_services/primeqa/notebooks/ir/dense/experiments/default/indexes/index_name in 20 seconds...\n",
      "#> Starting...\n",
      "No CUDA runtime is found, using CUDA_HOME='/opt/share/cuda-10.1/x86_64'\n",
      "{\n",
      "    \"ncells\": null,\n",
      "    \"centroid_score_threshold\": null,\n",
      "    \"ndocs\": null,\n",
      "    \"index_path\": null,\n",
      "    \"nbits\": 1,\n",
      "    \"kmeans_niters\": 4,\n",
      "    \"num_partitions_max\": 2,\n",
      "    \"similarity\": \"cosine\",\n",
      "    \"bsize\": 64,\n",
      "    \"accumsteps\": 1,\n",
      "    \"lr\": 3e-6,\n",
      "    \"maxsteps\": 500000,\n",
      "    \"save_every\": null,\n",
      "    \"resume\": false,\n",
      "    \"resume_optimizer\": false,\n",
      "    \"warmup\": null,\n",
      "    \"warmup_bert\": null,\n",
      "    \"relu\": false,\n",
      "    \"nway\": 2,\n",
      "    \"use_ib_negatives\": false,\n",
      "    \"reranker\": false,\n",
      "    \"distillation_alpha\": 1.0,\n",
      "    \"ignore_scores\": false,\n",
      "    \"shuffle_every_epoch\": false,\n",
      "    \"save_steps\": 2000,\n",
      "    \"save_epochs\": -1,\n",
      "    \"epochs\": 10,\n",
      "    \"input_arguments\": {},\n",
      "    \"model_type\": \"xlm-roberta-base\",\n",
      "    \"init_from_lm\": null,\n",
      "    \"local_models_repository\": null,\n",
      "    \"ranks_fn\": null,\n",
      "    \"topK\": 100,\n",
      "    \"student_teacher_temperature\": 1.0,\n",
      "    \"student_teacher_top_loss_weight\": 0.5,\n",
      "    \"teacher_model_type\": \"xlm-roberta-base\",\n",
      "    \"teacher_doc_maxlen\": 180,\n",
      "    \"distill_query_passage_separately\": false,\n",
      "    \"query_only\": false,\n",
      "    \"loss_function\": null,\n",
      "    \"query_weight\": 0.5,\n",
      "    \"rng_seed\": 12345,\n",
      "    \"query_maxlen\": 32,\n",
      "    \"attend_to_mask_tokens\": false,\n",
      "    \"interaction\": \"colbert\",\n",
      "    \"dim\": 128,\n",
      "    \"doc_maxlen\": 180,\n",
      "    \"mask_punctuation\": true,\n",
      "    \"checkpoint\": \"..\\/..\\/..\\/tests\\/resources\\/ir_dense\\/DrDecr.dnn\",\n",
      "    \"teacher_checkpoint\": null,\n",
      "    \"triples\": null,\n",
      "    \"teacher_triples\": null,\n",
      "    \"collection\": \"..\\/..\\/..\\/tests\\/resources\\/ir_dense\\/xorqa.train_ir_001pct_at_0_pct_collection_fornum.tsv\",\n",
      "    \"queries\": null,\n",
      "    \"index_name\": \"index_name\",\n",
      "    \"overwrite\": false,\n",
      "    \"root\": \"\\/u\\/franzm\\/git8\\/PrimeQA_services\\/primeqa\\/notebooks\\/ir\\/dense\\/experiments\",\n",
      "    \"experiment\": \"default\",\n",
      "    \"index_root\": null,\n",
      "    \"name\": \"2022-10\\/13\\/15.27.01\",\n",
      "    \"rank\": 0,\n",
      "    \"nranks\": 1,\n",
      "    \"amp\": true,\n",
      "    \"gpus\": 0\n",
      "}\n",
      "[Oct 13, 15:28:09] #> Loading collection...\n",
      "0M \n",
      "[Oct 13, 15:28:09] #>>>>> at ColBERT name (model type) : ../../../tests/resources/ir_dense/DrDecr.dnn\n",
      "[Oct 13, 15:28:09] #>>>>> at BaseColBERT name (model type) : ../../../tests/resources/ir_dense/DrDecr.dnn\n",
      "[Oct 13, 15:28:15] factory model type: xlm-roberta-base\n",
      "[Oct 13, 15:28:28] Loading segmented_maxsim_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "[Oct 13, 15:28:31] get query model type: xlm-roberta-base\n",
      "[Oct 13, 15:28:33] get doc model type: xlm-roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/franzm/packages/minconda3/envs/primeqa_oct2_4/lib/python3.9/site-packages/torch/cuda/amp/grad_scaler.py:115: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n",
      "/u/franzm/packages/minconda3/envs/primeqa_oct2_4/lib/python3.9/site-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 13, 15:28:34] [0] \t\t # of sampled PIDs = 7 \t sampled_pids[:3] = [3, 5, 0]\n",
      "[Oct 13, 15:28:34] [0] \t\t #> Encoding 7 passages..\n",
      "[Oct 13, 15:28:34] #> checkpoint, docFromText, Input: title | text, \t\t 64\n",
      "[Oct 13, 15:28:34] #> XLMR DocTokenizer.tensorize(batch_text[0], batch_background[0], bsize) ==\n",
      "[Oct 13, 15:28:34] #> Input: $ title | text, \t\t 64\n",
      "[Oct 13, 15:28:34] #> Output IDs: torch.Size([180]), tensor([    0,  9749, 44759,     6, 58745,  7986,     2,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1])\n",
      "[Oct 13, 15:28:34] #> Output Mask: torch.Size([180]), tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "[Oct 13, 15:28:34] #> checkpoint, docFromText, Output IDs: (tensor([[    0,  9749, 44759,  ...,     1,     1,     1],\n",
      "        [    0,  9749, 30267,  ...,     1,     1,     1],\n",
      "        [    0,  9749, 31678,  ...,     5,     2,     1],\n",
      "        ...,\n",
      "        [    0,  9749,  9098,  ...,     1,     1,     1],\n",
      "        [    0,  9749,   341,  ...,  4989,   525,     2],\n",
      "        [    0,  9749, 11617,  ...,     1,     1,     1]]), tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]))\n",
      "[Oct 13, 15:28:34] #>>>> colbert doc ==\n",
      "[Oct 13, 15:28:34] #>>>>> input_ids: torch.Size([180]), tensor([    0,  9749, 44759,     6, 58745,  7986,     2,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1])\n",
      "[Oct 13, 15:28:35] #>>>> before linear doc ==\n",
      "[Oct 13, 15:28:35] #>>>>> D: torch.Size([180, 768]), tensor([[-0.0206,  0.0178,  0.0109,  ..., -0.1482,  0.0424,  0.0743],\n",
      "        [-0.2106, -0.1617, -0.0293,  ..., -0.3439, -0.2542,  0.6386],\n",
      "        [-0.1392, -0.1904, -0.3955,  ..., -0.4584, -0.1321,  0.8035],\n",
      "        ...,\n",
      "        [-0.3337, -0.1872, -0.2755,  ..., -0.8305, -0.2524,  0.9564],\n",
      "        [-0.3337, -0.1872, -0.2755,  ..., -0.8305, -0.2524,  0.9564],\n",
      "        [-0.3337, -0.1872, -0.2755,  ..., -0.8305, -0.2524,  0.9564]])\n",
      "[Oct 13, 15:28:35] #>>>>> self.linear doc : Parameter containing:\n",
      "tensor([[-0.0167, -0.0032, -0.0119,  ..., -0.0170,  0.0129, -0.0077],\n",
      "        [-0.0066,  0.0045, -0.0204,  ..., -0.0280,  0.0008, -0.0067],\n",
      "        [-0.0126, -0.0006,  0.0135,  ..., -0.0071, -0.0104, -0.0233],\n",
      "        ...,\n",
      "        [-0.0122,  0.0324,  0.0043,  ...,  0.0306,  0.0014,  0.0217],\n",
      "        [ 0.0048,  0.0010,  0.0126,  ..., -0.0258,  0.0022,  0.0084],\n",
      "        [-0.0069,  0.0300, -0.0226,  ...,  0.0041,  0.0093, -0.0085]],\n",
      "       requires_grad=True)\n",
      "[Oct 13, 15:28:35] #>>>> colbert doc ==\n",
      "[Oct 13, 15:28:35] #>>>>> D: torch.Size([180, 128]), tensor([[-0.0700, -0.0414,  0.1927,  ...,  0.0555,  0.0578, -0.0755],\n",
      "        [ 0.0836,  0.0512,  1.4125,  ...,  0.0239,  0.3396, -0.8472],\n",
      "        [ 0.1549, -0.2601,  0.9531,  ...,  0.1276,  0.4468, -0.4947],\n",
      "        ...,\n",
      "        [ 0.1510, -0.0741,  1.3213,  ...,  0.1785,  0.4114, -0.5633],\n",
      "        [ 0.1510, -0.0741,  1.3213,  ...,  0.1785,  0.4114, -0.5633],\n",
      "        [ 0.1510, -0.0741,  1.3213,  ...,  0.1785,  0.4114, -0.5633]])\n",
      "[Oct 13, 15:28:35] [0] \t\t avg_doclen_est = 174.2857208251953 \t len(local_sample) = 7\n",
      "[Oct 13, 15:28:35] >> num_partitions_multiplier = 8, self.num_partitions = 256\n",
      "[Oct 13, 15:28:35] >> num_partitions limited to: self.num_partitions = 2\n",
      "[Oct 13, 15:28:35] [0] \t\t Creaing 2 partitions.\n",
      "[Oct 13, 15:28:35] [0] \t\t *Estimated* 1,220 embeddings.\n",
      "[Oct 13, 15:28:35] [0] \t\t #> Saving the indexing plan to /u/franzm/git8/PrimeQA_services/primeqa/notebooks/ir/dense/experiments/default/indexes/index_name/plan.json ..\n",
      "Sampling a subset of 512 / 1159 for training\n",
      "Clustering 512 points in 128D to 2 clusters, redo 1 times, 4 iterations\n",
      "  Preprocessing in 0.00 s\n",
      "[0.053, 0.07, 0.061, 0.067, 0.059, 0.051, 0.097, 0.077, 0.081, 0.063, 0.063, 0.046, 0.051, 0.08, 0.072, 0.044, 0.044, 0.062, 0.052, 0.068, 0.079, 0.062, 0.075, 0.061, 0.091, 0.045, 0.054, 0.06, 0.068, 0.055, 0.099, 0.054, 0.055, 0.055, 0.062, 0.073, 0.06, 0.061, 0.074, 0.049, 0.065, 0.059, 0.057, 0.077, 0.081, 0.08, 0.062, 0.052, 0.052, 0.057, 0.055, 0.058, 0.075, 0.065, 0.079, 0.064, 0.056, 0.045, 0.052, 0.065, 0.066, 0.044, 0.065, 0.068, 0.053, 0.053, 0.098, 0.068, 0.062, 0.074, 0.044, 0.049, 0.087, 0.077, 0.097, 0.06, 0.088, 0.072, 0.063, 0.068, 0.107, 0.053, 0.058, 0.05, 0.051, 0.074, 0.054, 0.069, 0.043, 0.067, 0.052, 0.078, 0.082, 0.057, 0.068, 0.046, 0.061, 0.087, 0.054, 0.049, 0.072, 0.06, 0.074, 0.063, 0.053, 0.052, 0.054, 0.072, 0.063, 0.078, 0.05, 0.076, 0.072, 0.072, 0.076, 0.063, 0.076, 0.07, 0.067, 0.052, 0.114, 0.084, 0.062, 0.112, 0.065, 0.055, 0.059, 0.061]\n",
      "[Oct 13, 15:28:35] #> Got bucket_cutoffs_quantiles = tensor([0.5000]) and bucket_weights_quantiles = tensor([0.2500, 0.7500])\n",
      "[Oct 13, 15:28:35] #> Got bucket_cutoffs = tensor([0.0026]) and bucket_weights = tensor([-0.0467,  0.0569])\n",
      "[Oct 13, 15:28:35] avg_residual = 0.06523443013429642\n",
      "[Oct 13, 15:28:35] #> base_config.py from_path /u/franzm/git8/PrimeQA_services/primeqa/notebooks/ir/dense/experiments/default/indexes/index_name/metadata.json\n",
      "[Oct 13, 15:28:35] #> base_config.py from_path /u/franzm/git8/PrimeQA_services/primeqa/notebooks/ir/dense/experiments/default/indexes/index_name/plan.json\n",
      "[Oct 13, 15:28:35] #> base_config.py from_path args loaded! \n",
      "[Oct 13, 15:28:35] #> base_config.py from_path args replaced ! \n",
      "[Oct 13, 15:28:35] [0] \t\t #> Encoding 7 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.01it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 13252.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 13, 15:28:36] [0] \t\t #> Saving chunk 0: \t 7 passages and 1,220 embeddings. From #0 onward.\n",
      "[Oct 13, 15:28:36] offset: 0\n",
      "[Oct 13, 15:28:36] chunk codes size(0): 1220\n",
      "[Oct 13, 15:28:36] codes size(0): 1220\n",
      "[Oct 13, 15:28:36] codes size(): torch.Size([1220])\n",
      "[Oct 13, 15:28:36] >>>>partition.size(0): 2\n",
      "[Oct 13, 15:28:36] >>>>num_partition: 2\n",
      "[Oct 13, 15:28:36] #> Optimizing IVF to store map from centroids to list of pids..\n",
      "[Oct 13, 15:28:36] #> Building the emb2pid mapping..\n",
      "[Oct 13, 15:28:36] len(emb2pid) = 1220\n",
      "[Oct 13, 15:28:36] #> Saved optimized IVF to /u/franzm/git8/PrimeQA_services/primeqa/notebooks/ir/dense/experiments/default/indexes/index_name/ivf.pid.pt\n",
      "[Oct 13, 15:28:36] [0] \t\t #> Saving the indexing metadata to /u/franzm/git8/PrimeQA_services/primeqa/notebooks/ir/dense/experiments/default/indexes/index_name/metadata.json ..\n",
      "\n",
      "#> Joined...\n"
     ]
    }
   ],
   "source": [
    "from primeqa.pipelines.components.indexer.dense import ColBERTIndexer\n",
    "os.makedirs(index_root, exist_ok = True)\n",
    "checkpoint_fn = os.path.join(test_files_location, \"DrDecr.dnn\")\n",
    "\n",
    "indexer = ColBERTIndexer(checkpoint = checkpoint_fn, index_root = index_root, index_name = index_name, num_partitions_max = 2)\n",
    "indexer.load()\n",
    "indexer.index(collection = collection_fn, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d4033c",
   "metadata": {},
   "source": [
    "### Search\n",
    "Next, we use the trained model and the index to search the collection, using queries in the form of a list of strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e919ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 13, 15:28:37] #> base_config.py from_path /u/franzm/git8/PrimeQA_services/primeqa/notebooks/ir/dense/experiments/default/indexes/index_name/metadata.json\n",
      "[Oct 13, 15:28:37] #> base_config.py from_path args loaded! \n",
      "[Oct 13, 15:28:37] #> base_config.py from_path args replaced ! \n",
      "[Oct 13, 15:28:40] #> Loading collection...\n",
      "0M \n",
      "[Oct 13, 15:28:40] #>>>>> at ColBERT name (model type) : ../../../tests/resources/ir_dense/DrDecr.dnn\n",
      "[Oct 13, 15:28:40] #>>>>> at BaseColBERT name (model type) : ../../../tests/resources/ir_dense/DrDecr.dnn\n",
      "[Oct 13, 15:28:46] factory model type: xlm-roberta-base\n",
      "[Oct 13, 15:28:59] Loading segmented_maxsim_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "[Oct 13, 15:29:03] get query model type: xlm-roberta-base\n",
      "[Oct 13, 15:29:04] get doc model type: xlm-roberta-base\n",
      "[Oct 13, 15:29:05] #> Loading codec...\n",
      "[Oct 13, 15:29:05] #> base_config.py from_path /u/franzm/git8/PrimeQA_services/primeqa/notebooks/ir/dense/experiments/default/indexes/index_name/metadata.json\n",
      "[Oct 13, 15:29:05] #> base_config.py from_path args loaded! \n",
      "[Oct 13, 15:29:05] #> base_config.py from_path args replaced ! \n",
      "[Oct 13, 15:29:05] #> Loading IVF...\n",
      "[Oct 13, 15:29:05] Loading segmented_lookup_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/franzm/packages/minconda3/envs/primeqa_oct2_4/lib/python3.9/site-packages/torch/cuda/amp/grad_scaler.py:115: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 13, 15:29:06] Loading filter_pids_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "[Oct 13, 15:29:06] Loading decompress_residuals_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "[Oct 13, 15:29:07] #> XMLR QueryTokenizer.tensorize(batch_text[0], batch_background[0], bsize) ==\n",
      "[Oct 13, 15:29:07] #> Input: $ Who is Michael Wigge, \t\t True, \t\t None\n",
      "[Oct 13, 15:29:07] #> Output IDs: torch.Size([32]), tensor([    0,  9748, 40469,    83, 11617,  5140, 23359,     2,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1])\n",
      "[Oct 13, 15:29:07] #> Output Mask: torch.Size([32]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "[Oct 13, 15:29:07] #>>>> colbert query ==\n",
      "[Oct 13, 15:29:07] #>>>>> input_ids: torch.Size([32]), tensor([    0,  9748, 40469,    83, 11617,  5140, 23359,     2,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/franzm/packages/minconda3/envs/primeqa_oct2_4/lib/python3.9/site-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Oct 13, 15:29:07] #>>>> before linear query ==\n",
      "[Oct 13, 15:29:07] #>>>>> Q: torch.Size([32, 768]), tensor([[-0.0133, -0.0071, -0.0338,  ..., -0.1157,  0.2084,  0.0775],\n",
      "        [-0.4863, -0.1339, -0.5917,  ..., -0.3788,  0.8931,  0.5833],\n",
      "        [-0.3398, -0.3887, -0.4318,  ..., -0.1584,  0.9153,  0.1573],\n",
      "        ...,\n",
      "        [-0.3348, -0.1802, -0.1922,  ..., -0.2447,  0.4070,  0.6802],\n",
      "        [-0.3348, -0.1802, -0.1922,  ..., -0.2447,  0.4070,  0.6802],\n",
      "        [-0.3348, -0.1802, -0.1922,  ..., -0.2447,  0.4070,  0.6802]])\n",
      "[Oct 13, 15:29:07] #>>>>> self.linear query : Parameter containing:\n",
      "tensor([[-0.0167, -0.0032, -0.0119,  ..., -0.0170,  0.0129, -0.0077],\n",
      "        [-0.0066,  0.0045, -0.0204,  ..., -0.0280,  0.0008, -0.0067],\n",
      "        [-0.0126, -0.0006,  0.0135,  ..., -0.0071, -0.0104, -0.0233],\n",
      "        ...,\n",
      "        [-0.0122,  0.0324,  0.0043,  ...,  0.0306,  0.0014,  0.0217],\n",
      "        [ 0.0048,  0.0010,  0.0126,  ..., -0.0258,  0.0022,  0.0084],\n",
      "        [-0.0069,  0.0300, -0.0226,  ...,  0.0041,  0.0093, -0.0085]],\n",
      "       requires_grad=True)\n",
      "[Oct 13, 15:29:07] #>>>> colbert query ==\n",
      "[Oct 13, 15:29:07] #>>>>> Q: torch.Size([32, 128]), tensor([[-0.0538, -0.0722, -0.0953,  ...,  0.0913, -0.0258, -0.0355],\n",
      "        [ 0.3742, -0.3261, -0.3451,  ...,  0.4478,  0.0581,  0.1226],\n",
      "        [ 0.5575, -0.5510, -0.2175,  ...,  0.7989,  0.0557,  0.1323],\n",
      "        ...,\n",
      "        [ 0.5475,  0.4454, -0.4406,  ...,  0.1876,  0.2905,  0.1305],\n",
      "        [ 0.5475,  0.4454, -0.4406,  ...,  0.1876,  0.2905,  0.1305],\n",
      "        [ 0.5475,  0.4454, -0.4406,  ...,  0.1876,  0.2905,  0.1305]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 85.83it/s]\n"
     ]
    }
   ],
   "source": [
    "from primeqa.pipelines.components.retriever.dense import ColBERTRetriever\n",
    "\n",
    "retriever = ColBERTRetriever(index_root = index_root, index_name = index_name, ndocs = 5, max_num_documents = 2)\n",
    "retriever.load()\n",
    "results = retriever.retrieve(input_texts = ['Who is Michael Wigge'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acc50fb",
   "metadata": {},
   "source": [
    "Here is the top search result for our query, containing document_id and score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d81d5cd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 12.305487632751465)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068c6478",
   "metadata": {},
   "source": [
    "Here is the top retrieved document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ff3e1dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\tMichael Wigge Michael Wigge is a travel writer and entertainment personality in Europe and in the United States. His work is characterized by a mixture of journalism and entertainment. His specialties are cultural issues which he examines in a very entertaining way. In 2002, Wigge drew attention to himself in Germany for the first time on TV broadcaster VIVA plus presenting comedy clips on the daily show “London Calling”. In this context he sets a record for the longest donkey ride in music television history and visits the Queen of England, dressed as King Henry VIII, on her 50th throne\tMichael Wigge\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(collection_fn, 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        if str(results[0][0][0]) == line.split()[0]:\n",
    "            print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859f31c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
