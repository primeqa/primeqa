{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "longNQ = pd.read_json(\"/dccstor/srosent2/generative/appen/NQ_formatted_answered_multiple-9.11.23.json\", orient='records', lines=True, dtype={\"id\":str})\n",
    "longNQ.sample(5)\n",
    "longNQTrain = pd.read_json(\"/dccstor/srosent2/generative/appen/NQ_formatted_answered_multiple-9.11.23_train.json\", orient='records', lines=True, dtype={\"id\":str})\n",
    "longNQDev = pd.read_json(\"/dccstor/srosent2/generative/appen/NQ_formatted_answered_multiple-9.11.23_dev.json\", orient='records', lines=True, dtype={\"id\":str})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "longNQ_first_word  = longNQ['input'].str.split().str.get(0)\n",
    "long_NQ_first_word_counts = longNQ_first_word.value_counts()[0:20] #/len(longNQ)\n",
    "print(long_NQ_first_word_counts.sum())\n",
    "long_NQ_first_word_counts['OTHER'] = 2379-long_NQ_first_word_counts.sum()\n",
    "print(long_NQ_first_word_counts.sum())\n",
    "print(long_NQ_first_word_counts['OTHER'])\n",
    "word_dist = []\n",
    "\n",
    "for word in long_NQ_first_word_counts.keys():\n",
    "    occurrence = long_NQ_first_word_counts[word]\n",
    "    word_dist.extend([word]*occurrence)\n",
    "\n",
    "print(long_NQ_first_word_counts)\n",
    "print(len(word_dist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import *\n",
    "\n",
    "print(word_dist)\n",
    "random_num = randint(0,len(word_dist))\n",
    "print(random_num)\n",
    "print(word_dist[random_num])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "originalNQfiles = glob.glob(\"/dccstor/srosent2/primeqa/data/train/nq-full/*\")\n",
    "\n",
    "originalNQ = pd.concat([pd.read_json(f,orient='records',lines=True, dtype={\"example_id\":str}) for f in originalNQfiles])\n",
    "\n",
    "originalNQ.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "originalNQdevfiles = glob.glob(\"/dccstor/srosent2/primeqa/data/dev/nq-full/*\")\n",
    "\n",
    "originalNQdev = pd.concat([pd.read_json(f,orient='records',lines=True, dtype={\"example_id\":str}) for f in originalNQdevfiles])\n",
    "\n",
    "originalNQdev.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter to only negatives\n",
    "negativeNQids1word = {}\n",
    "for i, row in originalNQ.iterrows():\n",
    "    if row['annotations'][0]['passage_answer']['candidate_index'] == -1:\n",
    "        first_word = row['question_text'].split()[0]\n",
    "\n",
    "        if first_word not in word_dist:\n",
    "            first_word = \"OTHER\"\n",
    "        if first_word not in negativeNQids1word:\n",
    "            negativeNQids1word[first_word] = set()\n",
    "        negativeNQids1word[first_word].add(row['example_id'])\n",
    "        \n",
    "print(len(negativeNQids1word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-existing negs\n",
    "import pandas as pd\n",
    "\n",
    "existing_data = pd.read_csv('/dccstor/srosent2/generative/hans-dev-questions.tsv',delimiter='\\t')\n",
    "\n",
    "existing_negs = existing_data[existing_data['relevant'] == '-1']\n",
    "existing_pos = existing_data[existing_data['relevant'] != '-1']\n",
    "print(existing_negs.sample(5))\n",
    "print(len(existing_negs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter to only negatives\n",
    "import pprint \n",
    "\n",
    "def filter_original(thisNQ, existing, fname, id_key, thisLongNQ):\n",
    "    existingids1worddev = {}\n",
    "    dev_ids = set()\n",
    "\n",
    "    for i, row in existing.iterrows():\n",
    "        first_word = row['text'].split()[0]\n",
    "        existing = False\n",
    "\n",
    "        # check if this question is in existing negs.\n",
    "        recordNQ = thisNQ[thisNQ['question_text'] == row['text']]\n",
    "        if len(recordNQ) > 0:\n",
    "            existing = True\n",
    "            dev_ids.add(recordNQ.iloc[0]['example_id'])\n",
    "\n",
    "            if first_word not in word_dist:\n",
    "                first_word = \"OTHER\"\n",
    "            if first_word not in existingids1worddev:\n",
    "                existingids1worddev[first_word] = 0\n",
    "            existingids1worddev[first_word] += 1\n",
    "    print(len(existingids1worddev))\n",
    "\n",
    "    pp = pprint.PrettyPrinter(indent=4)\n",
    "    pp.pprint(existingids1worddev)\n",
    "    print(len(dev_ids))\n",
    "    dataNQ= thisLongNQ[thisLongNQ[id_key].isin(list(dev_ids))]\n",
    "    dataNQ.to_json(f\"/dccstor/srosent2/generative/appen/final/{fname}.jsonl\", orient='records', lines=True)\n",
    "    return dev_ids\n",
    "\n",
    "used_ids = []\n",
    "used_ids.extend(filter_original(originalNQdev, existing_negs, 'longNQ_dev_unanswerable_tydi', 'example_id', originalNQdev))\n",
    "used_ids.extend(filter_original(originalNQ, existing_pos, 'longNQ_dev_answerable_tydi', 'example_id', originalNQ))\n",
    "used_ids.extend(filter_original(originalNQ, existing_pos, 'longNQ_dev_answerable_eli5', 'id', longNQ))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train: 1813\n",
    "# Dev: 300: 300 train (from hans)\n",
    "# Test: 67 dev + 233 more\n",
    "# NA train: 1813 from NQ train following distribution\n",
    "# NA dev: 300 from hans \n",
    "# NA test: 300 following distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NA train: 1813\n",
    "\n",
    "from random import *\n",
    "\n",
    "na_ids = set()\n",
    "na_train_words = {}\n",
    "\n",
    "while len(na_ids) < 20379:\n",
    "    random_num = randint(0,len(word_dist)-1)\n",
    "    random_id = list(negativeNQids1word[word_dist[random_num]])[randint(0,len(negativeNQids1word[word_dist[random_num]])-1)]\n",
    "    num_tries = 5\n",
    "    while random_id in na_ids and num_tries > 0:\n",
    "        random_id = list(negativeNQids1word[word_dist[random_num]])[randint(0,len(negativeNQids1word[word_dist[random_num]])-1)]\n",
    "        num_tries -= 1\n",
    "    if random_id in na_ids:\n",
    "        print(\"retry\")\n",
    "        continue\n",
    "    if word_dist[random_num] not in na_train_words:\n",
    "        na_train_words[word_dist[random_num]] = 0\n",
    "    na_train_words[word_dist[random_num]] += 1\n",
    "    na_ids.add(random_id)\n",
    "print(len(na_ids))\n",
    "print(na_train_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint \n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "pp.pprint(na_train_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NA train: 1791 from NQ train following distribution but check to make sure there are 5 sentences.\n",
    "import spacy \n",
    "\n",
    "# spacy.cli.download('en_core_web_sm')\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "trainNQNA = originalNQ[originalNQ['example_id'].isin(na_ids)] #[:1791]\n",
    "long_par_ids = []\n",
    "\n",
    "count = 0\n",
    "for i, row in trainNQNA.iterrows():\n",
    "    annotation = row['annotations'][0]\n",
    "    passage_offsets = row['passage_answer_candidates'][annotation['passage_answer']['candidate_index']]\n",
    "    passage_text = row['document_plaintext'].encode('utf-8')[passage_offsets['plaintext_start_byte']:passage_offsets['plaintext_end_byte']].decode('utf-8')\n",
    "    sentences = nlp(passage_text)\n",
    "        \n",
    "    num_sentences = 0\n",
    "    for sentence in sentences.sents:\n",
    "        num_sentences += 1\n",
    "    if num_sentences < 5:\n",
    "        continue\n",
    "    long_par_ids.append(row['example_id'])\n",
    "    count += 1\n",
    "    if count >= 2000:\n",
    "        break\n",
    "    \n",
    "\n",
    "trainNQNA = trainNQNA[trainNQNA['example_id'].isin(long_par_ids)][:1791]\n",
    "trainNQNA.to_json(\"/dccstor/srosent2/generative/appen/final/longNQ_train_unanswerable_tydi.jsonl\", orient='records', lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make test sets \n",
    "negativeNQids1worddev = {}\n",
    "existing_negsids1worddev = {}\n",
    "\n",
    "def allNA(annotations):\n",
    "    count = 0\n",
    "    for annotation in annotations:\n",
    "        if annotation['passage_answer']['candidate_index'] != -1:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "num_rows = 0\n",
    "for i, row in originalNQdev.iterrows():\n",
    "    if allNA(row['annotations']):\n",
    "        first_word = row['question_text'].split()[0]\n",
    "        existing = False\n",
    "\n",
    "        # check if this question is in existing negs.\n",
    "        if len(existing_negs[row['question_text'] == existing_negs['text']]) > 0:\n",
    "            existing = True\n",
    "\n",
    "        if first_word not in word_dist:\n",
    "            first_word = \"OTHER\"\n",
    "        if first_word not in negativeNQids1worddev:\n",
    "            negativeNQids1worddev[first_word] = set()\n",
    "        if existing and first_word not in existing_negsids1worddev:\n",
    "            existing_negsids1worddev[first_word] = set()\n",
    "        \n",
    "        if existing: \n",
    "            existing_negsids1worddev[first_word].add(row['example_id'])\n",
    "        else:\n",
    "            negativeNQids1worddev[first_word].add(row['example_id'])\n",
    "            num_rows += 1\n",
    "        \n",
    "print(num_rows)\n",
    "print(len(negativeNQids1worddev))\n",
    "print(len(existing_negsids1worddev))\n",
    "\n",
    "from random import *\n",
    "\n",
    "na_dev_ids = set()\n",
    "na_dev_words = {}\n",
    "\n",
    "while len(na_dev_ids) < 2139:\n",
    "    random_num = randint(0,len(word_dist)-1)\n",
    "    random_id = list(negativeNQids1worddev[word_dist[random_num]])[randint(0,len(negativeNQids1worddev[word_dist[random_num]])-1)]\n",
    "\n",
    "    if random_id in used_ids:\n",
    "        continue\n",
    "    num_tries = 5\n",
    "    while random_id in na_ids and num_tries > 0:\n",
    "        random_id = list(negativeNQids1worddev[word_dist[random_num]])[randint(0,len(negativeNQids1worddev[word_dist[random_num]])-1)]\n",
    "        num_tries -= 1\n",
    "    if random_id in na_dev_ids:\n",
    "        print(\"retry\")\n",
    "        continue\n",
    "    if word_dist[random_num] not in na_dev_words:\n",
    "        na_dev_words[word_dist[random_num]] = 0\n",
    "    na_dev_words[word_dist[random_num]] += 1\n",
    "    na_dev_ids.add(random_id)\n",
    "print(len(na_dev_ids))\n",
    "print(na_dev_words)\n",
    "\n",
    "testNQNA = originalNQdev[originalNQdev['example_id'].isin(na_dev_ids)]\n",
    "\n",
    "import spacy \n",
    "\n",
    "# spacy.cli.download('en_core_web_sm')\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "long_par_ids = []\n",
    "\n",
    "count = 0\n",
    "for i, row in testNQNA.iterrows():\n",
    "    annotation = row['annotations'][0]\n",
    "    passage_offsets = row['passage_answer_candidates'][annotation['passage_answer']['candidate_index']]\n",
    "    passage_text = row['document_plaintext'].encode('utf-8')[passage_offsets['plaintext_start_byte']:passage_offsets['plaintext_end_byte']].decode('utf-8')\n",
    "    sentences = nlp(passage_text)\n",
    "        \n",
    "    num_sentences = 0\n",
    "    for sentence in sentences.sents:\n",
    "        num_sentences += 1\n",
    "    if num_sentences < 5:\n",
    "        continue\n",
    "    long_par_ids.append(row['example_id'])\n",
    "    count += 1\n",
    "    if count >= 2139:\n",
    "        break\n",
    "\n",
    "testNQNA = testNQNA[testNQNA['example_id'].isin(long_par_ids)][:300]\n",
    "testNQNA.to_json(\"/dccstor/srosent2/generative/appen/final/longNQ_test_unanswerable_tydi.jsonl\", orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train: 1813\n",
    "# Test: 67 dev + 233 more\n",
    "\n",
    "longNQTrain = pd.read_json(\"/dccstor/srosent2/generative/appen/NQ_formatted_answered_multiple-9.15.23_train.json\", orient='records', lines=True, dtype={\"id\":str})\n",
    "longNQTrain = longNQTrain[~longNQTrain['id'].isin(used_ids)]\n",
    "longNQDev = pd.read_json(\"/dccstor/srosent2/generative/appen/NQ_formatted_answered_multiple-9.15.23_dev.json\", orient='records', lines=True, dtype={\"id\":str})\n",
    "longNQDev = longNQDev[~longNQDev['id'].isin(used_ids)]\n",
    "\n",
    "print(len(longNQTrain))\n",
    "print(len(longNQDev))\n",
    "\n",
    "longNQTrainShuffled = longNQTrain.sample(frac=1)\n",
    "\n",
    "longNQTrainNew = longNQTrainShuffled[234:]\n",
    "longNQDevNew = pd.concat([longNQTrainShuffled[0:234],longNQDev])\n",
    "\n",
    "longNQTrainNew.to_json(f\"/dccstor/srosent2/generative/appen/final/longNQ_train_answerable_eli5.jsonl\", orient='records', lines=True)\n",
    "longNQDevNew.to_json(f\"/dccstor/srosent2/generative/appen/final/longNQ_test_answerable_eli5.jsonl\", orient='records', lines=True)\n",
    "originalNQ[originalNQ['example_id'].isin(list(longNQTrainNew['id']))].to_json(f\"/dccstor/srosent2/generative/appen/final/longNQ_train_answerable_tydi.jsonl\", orient='records', lines=True)\n",
    "pd.concat([originalNQ[originalNQ['example_id'].isin(list(longNQDevNew['id']))],originalNQdev[originalNQdev['example_id'].isin(list(longNQDevNew['id']))]]).to_json(f\"/dccstor/srosent2/generative/appen/final/longNQ_test_answerable_tydi.jsonl\", orient='records', lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that splits are compatible with original NQ\n",
    "import pandas as pd\n",
    "\n",
    "final_files = glob.glob(\"/dccstor/srosent2/generative/appen/final/*/*/*\")\n",
    "\n",
    "unique_ids = set()\n",
    "\n",
    "for final_file in final_files:\n",
    "    id_key = 'id'\n",
    "    if 'tydi' in final_file:\n",
    "        id_key = 'example_id'\n",
    "    data = pd.read_json(final_file, orient='records', lines=True, dtype={id_key:str})\n",
    "\n",
    "    print(final_file)\n",
    "    print(f\"file: {len(data)}\")\n",
    "    print(f\"train: {len(originalNQ[originalNQ['example_id'].isin(list(data[id_key]))])}\")\n",
    "    print(f\"dev: {len(originalNQdev[originalNQdev['example_id'].isin(list(data[id_key]))])}\")\n",
    "    unique_ids.update(list(data[id_key]))\n",
    "print(len(unique_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that splits are compatible with original NQ\n",
    "import pandas as pd\n",
    "\n",
    "longNQTrain = pd.read_json(\"/dccstor/srosent2/generative/appen/NQ_formatted_answered_multiple-9.15.23_train.json\", orient='records', lines=True, dtype={\"id\":str})\n",
    "longNQDev = pd.read_json(\"/dccstor/srosent2/generative/appen/NQ_formatted_answered_multiple-9.15.23_dev.json\", orient='records', lines=True, dtype={\"id\":str})\n",
    "\n",
    "final_files = glob.glob(\"/dccstor/srosent2/generative/appen/final/*/*/*_answerable*\")\n",
    "\n",
    "for final_file in final_files:\n",
    "    id_key = 'id'\n",
    "    if 'tydi' in final_file:\n",
    "        id_key = 'example_id'\n",
    "    data = pd.read_json(final_file, orient='records', lines=True, dtype={id_key:str})\n",
    "\n",
    "    print(final_file)\n",
    "    print(f\"file: {len(data)}\")\n",
    "    print(f\"train: {len(longNQTrain[longNQTrain['id'].isin(list(data[id_key]))])}\")\n",
    "    print(f\"dev: {len(longNQDev[longNQDev['id'].isin(list(data[id_key]))])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check non-consecutive\n",
    "# check that IDs are all unique YES\n",
    "# check that TyDi and ELI5 splits have same IDs YES\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "final_files = glob.glob(\"/dccstor/srosent2/generative/appen/final/*/*/*\")\n",
    "\n",
    "seen_ids_tydi = set()\n",
    "seen_ids_eli5 = set()\n",
    "\n",
    "for final_file in final_files:\n",
    "    id_key = 'id'\n",
    "    if 'tydi' in final_file:\n",
    "        id_key = 'example_id'\n",
    "        data = pd.read_json(final_file, orient='records', lines=True, dtype={id_key:str})\n",
    "        seen_ids_tydi.update(data[id_key])\n",
    "    else:\n",
    "        data = pd.read_json(final_file, orient='records', lines=True, dtype={id_key:str})\n",
    "        seen_ids_eli5.update(data[id_key])\n",
    "\n",
    "        if \"_answerable\" in final_file:\n",
    "            num_false = 0\n",
    "            for i, row in data.iterrows():\n",
    "                if row['output'][0]['meta']['non_consecutive'] == False:\n",
    "                    num_false += 1\n",
    "            print(f\"{final_file}: {num_false}\")\n",
    "\n",
    "print(len(seen_ids_tydi))\n",
    "print(len(seen_ids_eli5))\n",
    "\n",
    "print(seen_ids_tydi == seen_ids_eli5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add to test set whether from original NQ dev or test\n",
    "\n",
    "# check that splits are compatible with original NQ\n",
    "import pandas as pd\n",
    "\n",
    "id_key = 'id'\n",
    "data = pd.read_json(\"/dccstor/srosent2/generative/appen/final/longNQ/test/longNQ_test_answerable.jsonl\", orient='records', lines=True, dtype={id_key:str})\n",
    "\n",
    "for i, row in data.iterrows():\n",
    "    if len(longNQDev[longNQDev['id'] == row[id_key]]) > 0:\n",
    "        data.at[i, 'is_dev'] = True\n",
    "    else:\n",
    "        data.at[i, 'is_dev'] = False\n",
    "\n",
    "data.to_json(\"/dccstor/srosent2/generative/appen/final/longNQ/test/longNQ_test_answerable_wdev.jsonl\", orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update data to include missing fields. for loading in HF\n",
    "import pandas as pd\n",
    "\n",
    "id_key = 'id'\n",
    "\n",
    "def convert_to_hf(data):\n",
    "    for i, row in data.iterrows():\n",
    "        for output in row['output']:\n",
    "            if output['answer'] == None:\n",
    "                output['answer'] = \"\"\n",
    "            if 'selected_sentences' not in output or output['selected_sentences'] == None:\n",
    "                output['selected_sentences'] = []\n",
    "            if 'meta' not in output or output['meta'] == None:\n",
    "                output['meta'] = {}\n",
    "            if 'skip' not in output['meta']:\n",
    "                output['meta']['skip'] = False\n",
    "            if 'non_consecutive' not in output['meta']:\n",
    "                output['meta']['non_consecutive'] = False # this may not be accurate\n",
    "            if 'round' not in output['meta']:\n",
    "                output['meta']['round'] = 0 # unanswerable\n",
    "            if 'annotator' not in output['meta']:\n",
    "                output['meta']['annotator'] = [] # unanswerable\n",
    "            if 'has_minimal_answer' not in output['meta']:\n",
    "                output['meta']['has_minimal_answer'] = False\n",
    "            if len(output['meta']) != 5:\n",
    "                print(output['meta'])\n",
    "        data.at[i,'output'] = row['output']\n",
    "    return data\n",
    "\n",
    "splits = [\"train\", \"dev\", \"test\"]\n",
    "\n",
    "for split in splits:\n",
    "    data = pd.read_json(f\"/dccstor/srosent2/generative/appen/final/longNQ/{split}/longNQ_{split}_answerable.jsonl\", orient='records', lines=True, dtype={id_key:str})\n",
    "    convert_to_hf(data).to_json(f\"/dccstor/srosent2/generative/appen/final/longNQ_hf/{split}/longNQ_{split}_answerable.jsonl\", orient='records', lines=True)\n",
    "    data = pd.read_json(f\"/dccstor/srosent2/generative/appen/final/longNQ/{split}/longNQ_{split}_unanswerable.jsonl\", orient='records', lines=True, dtype={id_key:str})\n",
    "    convert_to_hf(data).to_json(f\"/dccstor/srosent2/generative/appen/final/longNQ_hf/{split}/longNQ_{split}_unanswerable.jsonl\", orient='records', lines=True)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "primeqa_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
