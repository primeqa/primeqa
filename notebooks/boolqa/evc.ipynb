{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e139f661",
   "metadata": {},
   "source": [
    "# Answer classification for boolean questions\n",
    "\n",
    "In this notebook, we look at the answer (evidence) classifation, which is a component in the TyDiQA pipeline which decides whether a boolean question should be answered `yes` or `no`, based on a passage selected by the machine reading comprehension component.\n",
    "\n",
    "## Preliminaries\n",
    "We assume that the machine reading comprehension and the question type classifier components of the TyDiQA pipeline have already run, either through the integrated command line or the step-by-step process, both described [here](../../examples/boolqa/README.md) and that the output directory was `base`.\n",
    "\n",
    "First some setup.  The classifier will obtain its input from the `qtc/eval_predictions.json` file produced by the question type classifier.\n",
    "Most of this setup is very similar to the setup for [mrc](../mrc/mrc.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6778d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir=\"out\"\n",
    "input_file=f\"{base}/qtc/eval_predictions.json\"\n",
    "\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    HfArgumentParser,\n",
    "    Trainer,\n",
    "    TrainingArguments)\n",
    "from transformers.trainer_utils import set_seed\n",
    "from primeqa.boolqa.processors.postprocessors.boolqa_classifier import BoolQAClassifierPostProcessor\n",
    "from primeqa.boolqa.processors.preprocessors.boolqa_classifier import BoolQAClassifierPreProcessor\n",
    "from primeqa.boolqa.processors.dataset.mrc2dataset import create_dataset_from_run_mrc_output, create_dataset_from_json_str\n",
    "import pandas as pd\n",
    "\n",
    "seed = 42\n",
    "set_seed(seed)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    do_train=False,\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    gradient_accumulation_steps=1,\n",
    "    num_train_epochs=1,\n",
    "    evaluation_strategy='no',\n",
    "    learning_rate=4e-05,\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.1,\n",
    "    save_steps=50000,\n",
    "    seed=seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858358f1",
   "metadata": {},
   "source": [
    "## Setup the auxiliary classes\n",
    "\n",
    "These are the same type of classes that are used in the mrc system.  The `sentence1_key` and `sentence2_key` argument to the preprocessor specifies that the evidence classifier will predict `yes` or `no` based on the question and the (long) passage answer produced by the upstream MRC system.  In general the minimal (short) answers are too short to make reasonable predictions from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2071cb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained('PrimeQA/tydiqa-boolean-answer-classifier', num_labels=3, use_auth_token=True)\n",
    "\n",
    "tokenizer=AutoTokenizer.from_pretrained('PrimeQA/tydiqa-boolean-answer-classifier', use_fast=True, use_auth_token=True)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained('PrimeQA/tydiqa-boolean-answer-classifier', config=config, use_auth_token=True)\n",
    "\n",
    "label_list=['no', 'no_answer', 'yes']\n",
    "\n",
    "postprocessor_class = BoolQAClassifierPostProcessor\n",
    "postprocessor = postprocessor_class(\n",
    "    k=10,       \n",
    "    drop_label=\"no_answer\",\n",
    "    label_list = label_list,\n",
    "    id_key='example_id',\n",
    "    output_label_prefix='boolean_answer'\n",
    ")\n",
    "\n",
    "preprocessor_class = BoolQAClassifierPreProcessor\n",
    "preprocessor = preprocessor_class(\n",
    "    sentence1_key='question',\n",
    "    sentence2_key='passage_answer_text',\n",
    "    tokenizer=tokenizer,\n",
    "    load_from_cache_file=False,\n",
    "    max_seq_len=500,\n",
    "    padding=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023b1352",
   "metadata": {},
   "source": [
    "## Inputs\n",
    "Here we create a dataset from the input file.  The input file is the output file of the question type classifier.  For illustrative purposes, we filter it to focus on the english questions that have been predicted to be boolean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4bf62e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function <lambda> at 0x7f896dc10f80> of the transform datasets.arrow_dataset.Dataset.filter@2.0.1 couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf650d053e454b8b93de8a09c5a0f34e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec311f4bdb304bcc9d61cff77fc6cac0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['example_id', 'cls_score', 'start_logit', 'end_logit', 'span_answer', 'span_answer_score', 'start_index', 'end_index', 'passage_index', 'target_type_logits', 'span_answer_text', 'yes_no_answer', 'start_stdev', 'end_stdev', 'query_passage_similarity', 'normalized_span_answer_score', 'confidence_score', 'question', 'language', 'passage_answer_text', 'order', 'rank', 'question_type_pred', 'question_type_scores', 'question_type_conf'],\n",
       "    num_rows: 112\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples=create_dataset_from_run_mrc_output(input_file, unpack=False)\n",
    "examples=examples.filter(lambda x:x['language']=='english' and x['question_type_pred']=='boolean')\n",
    "eval_examples, eval_dataset = preprocessor.process_eval(examples)\n",
    "eval_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cd931f",
   "metadata": {},
   "source": [
    "## Do the predictions.\n",
    "As in mrc, the trainer class instance runs the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3607006",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: language, example_id, passage_answer_text, question. If language, example_id, passage_answer_text, question are not expected by `XLMRobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 112\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7/7 00:51]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = Trainer( \n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=None,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=None, #compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=None,\n",
    ")\n",
    "predictions = trainer.predict(eval_dataset, metric_key_prefix=\"predict\").predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d409461",
   "metadata": {},
   "source": [
    "## Predictions\n",
    "\n",
    "The pretrained model we provide is actually as ternary model - it predictions `no`, `no_answer`, or `yes`.  The `no_answer` is discarded for pipelines that end with an actual tydi evaluation, since the tydi evaluation script selects the score threshold that distinguishes answerable and unanswerable questions.  However, other applications may want to make use of this category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d74e335",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-6.061874</td>\n",
       "      <td>5.307685</td>\n",
       "      <td>1.121236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-5.060166</td>\n",
       "      <td>5.039388</td>\n",
       "      <td>0.649324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.695564</td>\n",
       "      <td>-1.510027</td>\n",
       "      <td>-3.645893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-3.895487</td>\n",
       "      <td>-1.826522</td>\n",
       "      <td>5.120609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-4.380426</td>\n",
       "      <td>6.555937</td>\n",
       "      <td>-1.952797</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2\n",
       "0 -6.061874  5.307685  1.121236\n",
       "1 -5.060166  5.039388  0.649324\n",
       "2  5.695564 -1.510027 -3.645893\n",
       "3 -3.895487 -1.826522  5.120609\n",
       "4 -4.380426  6.555937 -1.952797"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.from_records(predictions[0:5,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "236cebae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in process_references_and_predictions\n",
      "Dataset({\n",
      "    features: ['example_id', 'cls_score', 'start_logit', 'end_logit', 'span_answer', 'span_answer_score', 'start_index', 'end_index', 'passage_index', 'target_type_logits', 'span_answer_text', 'yes_no_answer', 'start_stdev', 'end_stdev', 'query_passage_similarity', 'normalized_span_answer_score', 'confidence_score', 'question', 'language', 'passage_answer_text', 'order', 'rank', 'question_type_pred', 'question_type_scores', 'question_type_conf', 'boolean_answer_pred', 'boolean_answer_scores', 'boolean_answer_conf'],\n",
      "    num_rows: 112\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "eval_preds = postprocessor.process_references_and_predictions(eval_examples, eval_dataset, predictions)\n",
    "eval_preds_ds = create_dataset_from_json_str(eval_preds.predictions, False)\n",
    "print(eval_preds_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c690369",
   "metadata": {},
   "source": [
    "## Questions and answers\n",
    "\n",
    "Here we display some questions that have been identified as boolean, and their predicted answers, based on the system output of the MRC system.  A weakness in the TydiQA dataset is that most (85%) of the boolean questions have an answer of `yes` - apparently the question writers wrote questions seeking confirmations of what they already knew or suspected.  We display the `passage_answer_text` that was automatically extracted by the upstream MRC system because that was used by the classifier to make the `yes`/`no`/`no_answer` prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f94694df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8887dbc131aa43fb8eb3d04a5e04e02b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>example_id</th>\n",
       "      <th>question</th>\n",
       "      <th>passage_answer_text</th>\n",
       "      <th>boolean_answer_pred</th>\n",
       "      <th>boolean_answer_scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>ec0e4b73-ec45-4ffb-b6d8-3fd23d00dddf</td>\n",
       "      <td>Does an animal with vertebrae have to be a chordate?</td>\n",
       "      <td>Craniates, one of the three subdivisions of chordates, all have distinct skulls. They include the hagfish, which have no vertebrae. Michael J. Benton commented that \"craniates are characterized by their heads, just as chordates, or possibly all deuterostomes, are by their tails\".[12]...</td>\n",
       "      <td>no</td>\n",
       "      <td>{'no': 6.118734836578369, 'no_answer': -1.7950204610824585, 'yes': -3.937854528427124}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0168b3e5-647f-485c-b3b0-58a6b47b9516</td>\n",
       "      <td>Does California get snow?</td>\n",
       "      <td>The high mountains, including the Sierra Nevada, the Cascade Range, and the Klamath Mountains, have a mountain climate with snow in winter and mild to moderate heat in summer. Ski resorts at Lake Tahoe, Mammoth Lakes, and Mount Shasta routinely receive over 10 feet (3.0m) of snow in a season, and so...</td>\n",
       "      <td>yes</td>\n",
       "      <td>{'no': -5.855449676513672, 'no_answer': -0.5776659846305847, 'yes': 5.964153289794922}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3f08a82a-48dd-4207-8e55-2431125c6c05</td>\n",
       "      <td>Does the Magellanic Cloud system have a super massive black hole?</td>\n",
       "      <td>The Magellanic Clouds (or Nubeculae Magellani[2]) are two irregular dwarf galaxies visible in the Southern Celestial Hemisphere; they are members of the Local Group and are orbiting the Milky Way galaxy. Because both show signs of a bar structure, they are often reclassified as Magellanic spiral gal...</td>\n",
       "      <td>yes</td>\n",
       "      <td>{'no': -4.380425930023193, 'no_answer': 6.55593729019165, 'yes': -1.9527971744537354}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>31b00909-a884-4eae-8a15-a388befb5eec</td>\n",
       "      <td>Is the great horned owl endangered?</td>\n",
       "      <td>The great horned owl is not considered a globally threatened species by the IUCN.[1] Including the Magellanic species, there are approximately 5.3 million wild horned owls in the Americas.[7] Most mortality in modern times is human-related, caused by owls flying into man-made objects, including buil...</td>\n",
       "      <td>no</td>\n",
       "      <td>{'no': 5.95291805267334, 'no_answer': -0.9690039753913879, 'yes': -4.356748580932617}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>e215f384-0afa-4477-b34a-d5f75ac8a467</td>\n",
       "      <td>Is the Mauser C96 produced today?</td>\n",
       "      <td>The Spanish gunmaker Astra-Unceta y Cia began producing a copy of the Mauser C.96 in 1927 that was externally similar to the C96 (including the presence of a detachable shoulder stock/holster) but with non-interlocking internal parts. It was produced until 1941, with a production hiatus in 1937 and ...</td>\n",
       "      <td>no</td>\n",
       "      <td>{'no': 3.602304458618164, 'no_answer': 0.8610267043113708, 'yes': -4.071805000305176}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>5b95654d-2cad-436e-bd42-18491a65c386</td>\n",
       "      <td>Can the central nervous system heal itself?</td>\n",
       "      <td>Nervous system injuries affect over 90,000 people every year.[2] It is estimated that spinal cord injuries alone affect 10,000 each year.[3] As a result of this high incidence of neurological injuries, nerve regeneration and repair, a subfield of neural tissue engineering, is becoming a rapidly grow...</td>\n",
       "      <td>no</td>\n",
       "      <td>{'no': 1.9971483945846558, 'no_answer': -1.7380565404891968, 'yes': -0.2094581127166748}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>8eb7e6eb-a03d-41aa-a9d4-dd6d46493b65</td>\n",
       "      <td>Is Daydream Software still an active company?</td>\n",
       "      <td>The Israeli company Waze Mobile developed the Waze software. Ehud Shabtai, Amir Shinar and Uri Levine founded the company. Two Israeli venture capital firms, Magma and Vertex, and an early-stage American venture capital firm, Bluerun Ventures, provided funding. Google acquired Waze Mobile in 2013....</td>\n",
       "      <td>yes</td>\n",
       "      <td>{'no': -4.1507720947265625, 'no_answer': 6.917901992797852, 'yes': -2.0596728324890137}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>07d20f1b-460f-48ce-ad38-feb956b25c49</td>\n",
       "      <td>Is Hungarian a romance language?</td>\n",
       "      <td>Hungarian (magyar nyelv) is a Finno-Ugric language spoken in Hungary and several neighbouring countries. It is the official language of  Hungary and one of the 24 official languages of the European Union. Outside Hungary it is also spoken by communities of Hungarians in the countries that today make...</td>\n",
       "      <td>yes</td>\n",
       "      <td>{'no': -3.6559829711914062, 'no_answer': 5.812913417816162, 'yes': -1.4828946590423584}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>bbb56373-e84e-4fde-8858-be612b8f5d2c</td>\n",
       "      <td>Is Cantonese written the same as Mandarin?</td>\n",
       "      <td>General estimates of vocabulary differences between Cantonese and Mandarin range from 30 to 50 percent. Donald B. Snow, the author of Cantonese as Written Language: The Growth of a Written Chinese Vernacular, wrote that \"It is difficult to quantify precisely how different\" the two vocabularies are.[...</td>\n",
       "      <td>no</td>\n",
       "      <td>{'no': 6.098764419555664, 'no_answer': -1.3992488384246826, 'yes': -4.08381462097168}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>2254035a-f43d-48e9-9eae-ef1c1e626f96</td>\n",
       "      <td>Can salt marsh die-off be fixed?</td>\n",
       "      <td>Research on the salt marsh snail Littoraria irrorata and its effects on marsh plant productivity, have provided strong evidence of consumer control in marshes triggered by overexploitation. This snail is capable of turning strands of cordgrass (Spartina alterniflora) (&gt;2.5m tall) into mudflats withi...</td>\n",
       "      <td>yes</td>\n",
       "      <td>{'no': -5.442584037780762, 'no_answer': 6.890753746032715, 'yes': -1.1657015085220337}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import ClassLabel, Sequence\n",
    "from numpy.random import permutation\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Based on https://github.com/huggingface/notebooks/blob/main/examples/question_answering.ipynb\n",
    "def show_balanced_examples(dataset, perm, groups, nrows, maxchars, cols):\n",
    "    df = pd.DataFrame(dataset)\n",
    "    dfp = df.iloc[perm] # shuffle\n",
    "    dfg = dfp.groupby(groups)\n",
    "    df_todisplay = dfg.head(nrows)[cols]\n",
    "    if 'passage_answer_text' in cols:\n",
    "        df_todisplay['passage_answer_text'] = df_todisplay['passage_answer_text'].str.slice(0,maxchars) + '...'\n",
    "    display(HTML(df_todisplay.to_html()))\n",
    "    \n",
    "    \n",
    "\n",
    "english_boolean_eval_examples = eval_preds_ds.filter(lambda x:x['language']=='english' and x['question_type_pred']=='boolean')\n",
    "random_idxs = permutation(len(english_boolean_eval_examples))\n",
    "cols=['example_id','question','passage_answer_text', 'boolean_answer_pred', 'boolean_answer_scores']\n",
    "show_balanced_examples(english_boolean_eval_examples, random_idxs, 'boolean_answer_pred', 5, 300, cols)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
