{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table Question Generation using TAPAS: How to Train\n",
    "\n",
    "In this notebook, we will show how to use ReaderComponent class to instantiate Tapas and use it to train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"time\":\"2022-12-06 09:12:04,280\", \"name\": \"apache_beam.typehints.native_type_compatibility\", \"level\": \"INFO\", \"message\": \"Using Any for unsupported type: typing.Sequence[~T]\"}\n",
      "{\"time\":\"2022-12-06 09:12:06,906\", \"name\": \"apache_beam.io.gcp.bigquery\", \"level\": \"INFO\", \"message\": \"No module named google.cloud.bigquery_storage_v1. As a result, the ReadFromBigQuery transform *CANNOT* be used with `method=DIRECT_READ`.\"}\n",
      "{\"time\":\"2022-12-06 09:12:10,260\", \"name\": \"nlp.utils.file_utils\", \"level\": \"INFO\", \"message\": \"PyTorch version 1.11.0+cu102 available.\"}\n",
      "{\"time\":\"2022-12-06 09:12:10,572\", \"name\": \"faiss.loader\", \"level\": \"INFO\", \"message\": \"Loading faiss.\"}\n",
      "{\"time\":\"2022-12-06 09:12:10,645\", \"name\": \"faiss.loader\", \"level\": \"INFO\", \"message\": \"Successfully loaded faiss.\"}\n"
     ]
    }
   ],
   "source": [
    "from primeqa.components.reader_component import ReaderComponent\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config is read from a default location when no explicit path is provided for config.json\n",
    "reader = ReaderComponent('TapasModel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading from config at  ../../primeqa/tableqa/tapas/configs/tapas_config.json\n",
      "Preprocessing wikisql dataset\n",
      "{\"time\":\"2022-12-06 09:12:27,227\", \"name\": \"nlp.load\", \"level\": \"INFO\", \"message\": \"Checking /u/jaydesen/.cache/huggingface/datasets/e6305d5981c5107101189ff917073a7286c4e71db220340fec67e0f69bb6bbdd.e1797f2c1ffe28c0d770867818c0e7df49f220a07f4feb608ec8939227b2dbdc.py for additional imports.\"}\n",
      "{\"time\":\"2022-12-06 09:12:27,265\", \"name\": \"nlp.load\", \"level\": \"INFO\", \"message\": \"Found main folder for dataset https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/wikisql/wikisql.py at /dccstor/irl-tableqa/jaydeep/tu4.24.0/lib/python3.7/site-packages/nlp/datasets/wikisql\"}\n",
      "{\"time\":\"2022-12-06 09:12:27,274\", \"name\": \"nlp.load\", \"level\": \"INFO\", \"message\": \"Found specific version folder for dataset https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/wikisql/wikisql.py at /dccstor/irl-tableqa/jaydeep/tu4.24.0/lib/python3.7/site-packages/nlp/datasets/wikisql/6c75e26bb08db6bca2dbc201e89b574f053fc25492ef8bbf2614463c14436618\"}\n",
      "{\"time\":\"2022-12-06 09:12:27,289\", \"name\": \"nlp.load\", \"level\": \"INFO\", \"message\": \"Found script file from https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/wikisql/wikisql.py to /dccstor/irl-tableqa/jaydeep/tu4.24.0/lib/python3.7/site-packages/nlp/datasets/wikisql/6c75e26bb08db6bca2dbc201e89b574f053fc25492ef8bbf2614463c14436618/wikisql.py\"}\n",
      "{\"time\":\"2022-12-06 09:12:27,314\", \"name\": \"nlp.load\", \"level\": \"INFO\", \"message\": \"Found dataset infos file from https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/wikisql/dataset_infos.json to /dccstor/irl-tableqa/jaydeep/tu4.24.0/lib/python3.7/site-packages/nlp/datasets/wikisql/6c75e26bb08db6bca2dbc201e89b574f053fc25492ef8bbf2614463c14436618/dataset_infos.json\"}\n",
      "{\"time\":\"2022-12-06 09:12:27,322\", \"name\": \"nlp.load\", \"level\": \"INFO\", \"message\": \"Found metadata file for dataset https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/wikisql/wikisql.py at /dccstor/irl-tableqa/jaydeep/tu4.24.0/lib/python3.7/site-packages/nlp/datasets/wikisql/6c75e26bb08db6bca2dbc201e89b574f053fc25492ef8bbf2614463c14436618/wikisql.json\"}\n",
      "{\"time\":\"2022-12-06 09:12:27,402\", \"name\": \"nlp.builder\", \"level\": \"WARNING\", \"message\": \"Using custom data configuration default\"}\n",
      "{\"time\":\"2022-12-06 09:12:27,436\", \"name\": \"nlp.info\", \"level\": \"INFO\", \"message\": \"Loading Dataset Infos from /dccstor/irl-tableqa/jaydeep/tu4.24.0/lib/python3.7/site-packages/nlp/datasets/wikisql/6c75e26bb08db6bca2dbc201e89b574f053fc25492ef8bbf2614463c14436618\"}\n",
      "{\"time\":\"2022-12-06 09:12:27,477\", \"name\": \"nlp.builder\", \"level\": \"INFO\", \"message\": \"Overwrite dataset info from restored data version.\"}\n",
      "{\"time\":\"2022-12-06 09:12:27,477\", \"name\": \"nlp.info\", \"level\": \"INFO\", \"message\": \"Loading Dataset info from /u/jaydesen/.cache/huggingface/datasets/wiki_sql/default/0.1.0/6c75e26bb08db6bca2dbc201e89b574f053fc25492ef8bbf2614463c14436618\"}\n",
      "{\"time\":\"2022-12-06 09:12:27,493\", \"name\": \"nlp.builder\", \"level\": \"INFO\", \"message\": \"Reusing dataset wiki_sql (/u/jaydesen/.cache/huggingface/datasets/wiki_sql/default/0.1.0/6c75e26bb08db6bca2dbc201e89b574f053fc25492ef8bbf2614463c14436618)\"}\n",
      "{\"time\":\"2022-12-06 09:12:27,494\", \"name\": \"nlp.builder\", \"level\": \"INFO\", \"message\": \"Constructing Dataset for split validation, from /u/jaydesen/.cache/huggingface/datasets/wiki_sql/default/0.1.0/6c75e26bb08db6bca2dbc201e89b574f053fc25492ef8bbf2614463c14436618\"}\n",
      "{\"time\":\"2022-12-06 09:12:27,508\", \"name\": \"nlp.utils.info_utils\", \"level\": \"INFO\", \"message\": \"All the checksums matched successfully for post processing resources\"}\n",
      "{\"time\":\"2022-12-06 09:12:27,911\", \"name\": \"nlp.load\", \"level\": \"INFO\", \"message\": \"Checking /u/jaydesen/.cache/huggingface/datasets/e6305d5981c5107101189ff917073a7286c4e71db220340fec67e0f69bb6bbdd.e1797f2c1ffe28c0d770867818c0e7df49f220a07f4feb608ec8939227b2dbdc.py for additional imports.\"}\n",
      "{\"time\":\"2022-12-06 09:12:27,928\", \"name\": \"nlp.load\", \"level\": \"INFO\", \"message\": \"Found main folder for dataset https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/wikisql/wikisql.py at /dccstor/irl-tableqa/jaydeep/tu4.24.0/lib/python3.7/site-packages/nlp/datasets/wikisql\"}\n",
      "{\"time\":\"2022-12-06 09:12:27,939\", \"name\": \"nlp.load\", \"level\": \"INFO\", \"message\": \"Found specific version folder for dataset https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/wikisql/wikisql.py at /dccstor/irl-tableqa/jaydeep/tu4.24.0/lib/python3.7/site-packages/nlp/datasets/wikisql/6c75e26bb08db6bca2dbc201e89b574f053fc25492ef8bbf2614463c14436618\"}\n",
      "{\"time\":\"2022-12-06 09:12:27,956\", \"name\": \"nlp.load\", \"level\": \"INFO\", \"message\": \"Found script file from https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/wikisql/wikisql.py to /dccstor/irl-tableqa/jaydeep/tu4.24.0/lib/python3.7/site-packages/nlp/datasets/wikisql/6c75e26bb08db6bca2dbc201e89b574f053fc25492ef8bbf2614463c14436618/wikisql.py\"}\n",
      "{\"time\":\"2022-12-06 09:12:27,965\", \"name\": \"nlp.load\", \"level\": \"INFO\", \"message\": \"Found dataset infos file from https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/wikisql/dataset_infos.json to /dccstor/irl-tableqa/jaydeep/tu4.24.0/lib/python3.7/site-packages/nlp/datasets/wikisql/6c75e26bb08db6bca2dbc201e89b574f053fc25492ef8bbf2614463c14436618/dataset_infos.json\"}\n",
      "{\"time\":\"2022-12-06 09:12:27,970\", \"name\": \"nlp.load\", \"level\": \"INFO\", \"message\": \"Found metadata file for dataset https://s3.amazonaws.com/datasets.huggingface.co/nlp/datasets/wikisql/wikisql.py at /dccstor/irl-tableqa/jaydeep/tu4.24.0/lib/python3.7/site-packages/nlp/datasets/wikisql/6c75e26bb08db6bca2dbc201e89b574f053fc25492ef8bbf2614463c14436618/wikisql.json\"}\n",
      "{\"time\":\"2022-12-06 09:12:27,973\", \"name\": \"nlp.builder\", \"level\": \"WARNING\", \"message\": \"Using custom data configuration default\"}\n",
      "{\"time\":\"2022-12-06 09:12:27,974\", \"name\": \"nlp.info\", \"level\": \"INFO\", \"message\": \"Loading Dataset Infos from /dccstor/irl-tableqa/jaydeep/tu4.24.0/lib/python3.7/site-packages/nlp/datasets/wikisql/6c75e26bb08db6bca2dbc201e89b574f053fc25492ef8bbf2614463c14436618\"}\n",
      "{\"time\":\"2022-12-06 09:12:27,986\", \"name\": \"nlp.builder\", \"level\": \"INFO\", \"message\": \"Overwrite dataset info from restored data version.\"}\n",
      "{\"time\":\"2022-12-06 09:12:27,987\", \"name\": \"nlp.info\", \"level\": \"INFO\", \"message\": \"Loading Dataset info from /u/jaydesen/.cache/huggingface/datasets/wiki_sql/default/0.1.0/6c75e26bb08db6bca2dbc201e89b574f053fc25492ef8bbf2614463c14436618\"}\n",
      "{\"time\":\"2022-12-06 09:12:28,004\", \"name\": \"nlp.builder\", \"level\": \"INFO\", \"message\": \"Reusing dataset wiki_sql (/u/jaydesen/.cache/huggingface/datasets/wiki_sql/default/0.1.0/6c75e26bb08db6bca2dbc201e89b574f053fc25492ef8bbf2614463c14436618)\"}\n",
      "{\"time\":\"2022-12-06 09:12:28,005\", \"name\": \"nlp.builder\", \"level\": \"INFO\", \"message\": \"Constructing Dataset for split train, from /u/jaydesen/.cache/huggingface/datasets/wiki_sql/default/0.1.0/6c75e26bb08db6bca2dbc201e89b574f053fc25492ef8bbf2614463c14436618\"}\n",
      "{\"time\":\"2022-12-06 09:12:28,029\", \"name\": \"nlp.utils.info_utils\", \"level\": \"INFO\", \"message\": \"All the checksums matched successfully for post processing resources\"}\n",
      "Preprocessing done\n",
      "Preprocessing done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dccstor/irl-tableqa/jaydeep/tu4.24.0/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 16142\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2018\n",
      "  Number of trainable parameters = 110673410\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16' max='2018' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  16/2018 04:41 < 11:10:22, 0.05 it/s, Epoch 0.01/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reader.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f9f85f796d01129d0dd105a088854619f454435301f6ffec2fea96ecbd9be4ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
