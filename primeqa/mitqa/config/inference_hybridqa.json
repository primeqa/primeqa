{
    "per_device_train_batch_size_rr":24,
    "per_device_eval_batch_size_rr":24,
    "rr_model_name":"bert-large-uncased",
    "row_retriever_model_name_path":"data/hybridqa/models/row_retriever/row_retriever.bin",
    "pos_frac_per_epoch":[0.51, 0.5, 0.3, 0.1, 0.09],
    "group_frac_per_epoch":[0.0, 0.0, 0.0, 0.0, 0.0],
    "max_seq_length":512,
    "per_gpu_train_batch_size":8,
    "train_batch_size":8,
    "per_gpu_eval_batch_size":8,
    "eval_batch_size":8,
    "max_query_length":64,
    "threads":1,
    "null_score_diff_threshold":0.0,
    "n_best_size":20,
    "do_predict_ae":true,
    "n_gpu":1,
    "max_answer_length":30,
    "model_name_or_path_ae":"PrimeQA/MITQA_hybridqa_multi_answer_answer_extractor",
    "model_type":"bert",
    "doc_stride":128,
    "pred_ans_file":"data/hybridqa/predictions/answer_extractor_output_test.json",
    "eval_file":"data/hybridqa/ae_input_test.json",
    "output_dir":"data/hybridqa/models/answer_extractor/",
    "model":"gpt2",
    "top_k":0,
    "top_p":0.9,
    "seed_lg":42,
    "batch_size_lg":2,
    "linker_model":"data/ottqa/models/link_generator/model-ep9.pt",
    "max_source_len":32,
    "max_target_len":16,
    "do_all_lg":true,
    "data_path_root":"data/hybridqa/",
    "dataset_name":"hybridqa",
    "test_data_path":"data/hybridqa/toy.json",
    "collections_file":"linearized_tables.tsv",
    "test":true
}